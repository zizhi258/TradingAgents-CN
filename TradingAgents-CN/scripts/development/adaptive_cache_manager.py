#!/usr/bin/env python3
"""
è‡ªé€‚åº”ç¼“å­˜ç®¡ç†å™¨ - æ ¹æ®å¯ç”¨æœåŠ¡è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç¼“å­˜ç­–ç•¥
æ”¯æŒæ–‡ä»¶ç¼“å­˜ã€Redisç¼“å­˜ã€MongoDBç¼“å­˜çš„æ™ºèƒ½åˆ‡æ¢
"""

import os
import json
import pickle
import hashlib
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, Optional, Union
import pandas as pd

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('scripts')

# å¯¼å…¥æ™ºèƒ½é…ç½®
try:
    from smart_config import get_smart_config, get_config
    SMART_CONFIG_AVAILABLE = True
except ImportError:
    SMART_CONFIG_AVAILABLE = False

class AdaptiveCacheManager:
    """è‡ªé€‚åº”ç¼“å­˜ç®¡ç†å™¨ - æ™ºèƒ½é€‰æ‹©ç¼“å­˜åç«¯"""
    
    def __init__(self, cache_dir: str = "data_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # è·å–æ™ºèƒ½é…ç½®
        self._load_smart_config()
        
        # åˆå§‹åŒ–ç¼“å­˜åç«¯
        self._init_backends()
        
        self.logger.info(f"ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œä¸»è¦åç«¯: {self.primary_backend}")
    
    def _load_smart_config(self):
        """åŠ è½½æ™ºèƒ½é…ç½®"""
        if SMART_CONFIG_AVAILABLE:
            try:
                config_manager = get_smart_config()
                self.config = config_manager.get_config()
                self.primary_backend = self.config["cache"]["primary_backend"]
                self.mongodb_enabled = self.config["database"]["mongodb"]["enabled"]
                self.redis_enabled = self.config["database"]["redis"]["enabled"]
                self.fallback_enabled = self.config["cache"]["fallback_enabled"]
                self.ttl_settings = self.config["cache"]["ttl_settings"]
                
                self.logger.info("âœ… æ™ºèƒ½é…ç½®åŠ è½½æˆåŠŸ")
                return
            except Exception as e:
                self.logger.warning(f"æ™ºèƒ½é…ç½®åŠ è½½å¤±è´¥: {e}")
        
        # é»˜è®¤é…ç½®ï¼ˆçº¯æ–‡ä»¶ç¼“å­˜ï¼‰
        self.config = {
            "cache": {
                "primary_backend": "file",
                "fallback_enabled": True,
                "ttl_settings": {
                    "us_stock_data": 7200,
                    "china_stock_data": 3600,
                    "us_news": 21600,
                    "china_news": 14400,
                    "us_fundamentals": 86400,
                    "china_fundamentals": 43200,
                }
            }
        }
        self.primary_backend = "file"
        self.mongodb_enabled = False
        self.redis_enabled = False
        self.fallback_enabled = True
        self.ttl_settings = self.config["cache"]["ttl_settings"]
        
        self.logger.info("ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆçº¯æ–‡ä»¶ç¼“å­˜ï¼‰")
    
    def _init_backends(self):
        """åˆå§‹åŒ–ç¼“å­˜åç«¯"""
        self.mongodb_client = None
        self.redis_client = None
        
        # åˆå§‹åŒ–MongoDB
        if self.mongodb_enabled:
            try:
                import pymongo
                self.mongodb_client = pymongo.MongoClient(
                    'localhost', 27017, 
                    serverSelectionTimeoutMS=2000
                )
                # æµ‹è¯•è¿æ¥
                self.mongodb_client.server_info()
                self.mongodb_db = self.mongodb_client.tradingagents
                self.logger.info("âœ… MongoDBåç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                self.logger.warning(f"MongoDBåˆå§‹åŒ–å¤±è´¥: {e}")
                self.mongodb_enabled = False
                self.mongodb_client = None
        
        # åˆå§‹åŒ–Redis
        if self.redis_enabled:
            try:
                import redis

                self.redis_client = redis.Redis(
                    host='localhost', port=6379, 
                    socket_timeout=2
                )
                # æµ‹è¯•è¿æ¥
                self.redis_client.ping()
                self.logger.info("âœ… Redisåç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                self.logger.warning(f"Redisåˆå§‹åŒ–å¤±è´¥: {e}")
                self.redis_enabled = False
                self.redis_client = None
        
        # å¦‚æœä¸»è¦åç«¯ä¸å¯ç”¨ï¼Œè‡ªåŠ¨é™çº§
        if self.primary_backend == "redis" and not self.redis_enabled:
            if self.mongodb_enabled:
                self.primary_backend = "mongodb"
                self.logger.info("Redisä¸å¯ç”¨ï¼Œé™çº§åˆ°MongoDB")
            else:
                self.primary_backend = "file"
                self.logger.info("Redisä¸å¯ç”¨ï¼Œé™çº§åˆ°æ–‡ä»¶ç¼“å­˜")
        
        elif self.primary_backend == "mongodb" and not self.mongodb_enabled:
            if self.redis_enabled:
                self.primary_backend = "redis"
                self.logger.info("MongoDBä¸å¯ç”¨ï¼Œé™çº§åˆ°Redis")
            else:
                self.primary_backend = "file"
                self.logger.info("MongoDBä¸å¯ç”¨ï¼Œé™çº§åˆ°æ–‡ä»¶ç¼“å­˜")
    
    def _get_cache_key(self, symbol: str, start_date: str, end_date: str, 
                      data_source: str = "default") -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = f"{symbol}_{start_date}_{end_date}_{data_source}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def _get_ttl_seconds(self, symbol: str, data_type: str = "stock_data") -> int:
        """è·å–TTLç§’æ•°"""
        # åˆ¤æ–­å¸‚åœºç±»å‹
        if len(symbol) == 6 and symbol.isdigit():
            market = "china"
        else:
            market = "us"
        
        # è·å–TTLé…ç½®
        ttl_key = f"{market}_{data_type}"
        ttl_hours = self.ttl_settings.get(ttl_key, 7200)  # é»˜è®¤2å°æ—¶
        return ttl_hours
    
    def _is_cache_valid(self, cache_time: datetime, ttl_seconds: int) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if cache_time is None:
            return False
        
        expiry_time = cache_time + timedelta(seconds=ttl_seconds)
        return datetime.now() < expiry_time
    
    def _save_to_file(self, cache_key: str, data: Any, metadata: Dict) -> bool:
        """ä¿å­˜åˆ°æ–‡ä»¶ç¼“å­˜"""
        try:
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            cache_data = {
                'data': data,
                'metadata': metadata,
                'timestamp': datetime.now()
            }
            
            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            
            return True
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def _load_from_file(self, cache_key: str) -> Optional[Dict]:
        """ä»æ–‡ä»¶ç¼“å­˜åŠ è½½"""
        try:
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            if not cache_file.exists():
                return None
            
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            return cache_data
        except Exception as e:
            self.logger.error(f"æ–‡ä»¶ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            return None
    
    def _save_to_redis(self, cache_key: str, data: Any, metadata: Dict, ttl_seconds: int) -> bool:
        """ä¿å­˜åˆ°Redisç¼“å­˜"""
        if not self.redis_client:
            return False
        
        try:
            cache_data = {
                'data': data,
                'metadata': metadata,
                'timestamp': datetime.now().isoformat()
            }
            
            serialized_data = pickle.dumps(cache_data)
            self.redis_client.setex(cache_key, ttl_seconds, serialized_data)
            return True
        except Exception as e:
            self.logger.error(f"Redisç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def _load_from_redis(self, cache_key: str) -> Optional[Dict]:
        """ä»Redisç¼“å­˜åŠ è½½"""
        if not self.redis_client:
            return None
        
        try:
            serialized_data = self.redis_client.get(cache_key)
            if not serialized_data:
                return None
            
            cache_data = pickle.loads(serialized_data)
            # è½¬æ¢æ—¶é—´æˆ³
            if isinstance(cache_data['timestamp'], str):
                cache_data['timestamp'] = datetime.fromisoformat(cache_data['timestamp'])
            
            return cache_data
        except Exception as e:
            self.logger.error(f"Redisç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            return None
    
    def save_stock_data(self, symbol: str, data: Any, start_date: str = None, 
                       end_date: str = None, data_source: str = "default") -> str:
        """ä¿å­˜è‚¡ç¥¨æ•°æ®åˆ°ç¼“å­˜"""
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._get_cache_key(symbol, start_date or "", end_date or "", data_source)
        
        # å‡†å¤‡å…ƒæ•°æ®
        metadata = {
            'symbol': symbol,
            'start_date': start_date,
            'end_date': end_date,
            'data_source': data_source,
            'data_type': 'stock_data'
        }
        
        # è·å–TTL
        ttl_seconds = self._get_ttl_seconds(symbol, 'stock_data')
        
        # æ ¹æ®ä¸»è¦åç«¯ä¿å­˜
        success = False
        
        if self.primary_backend == "redis":
            success = self._save_to_redis(cache_key, data, metadata, ttl_seconds)
        elif self.primary_backend == "mongodb":
            # MongoDBä¿å­˜é€»è¾‘ï¼ˆç®€åŒ–ç‰ˆï¼‰
            success = self._save_to_file(cache_key, data, metadata)
        
        # å¦‚æœä¸»è¦åç«¯å¤±è´¥ï¼Œä½¿ç”¨æ–‡ä»¶ç¼“å­˜ä½œä¸ºå¤‡ç”¨
        if not success and self.fallback_enabled:
            success = self._save_to_file(cache_key, data, metadata)
            if success:
                self.logger.info(f"ä½¿ç”¨æ–‡ä»¶ç¼“å­˜å¤‡ç”¨ä¿å­˜: {cache_key}")
        
        if success:
            self.logger.info(f"æ•°æ®ä¿å­˜æˆåŠŸ: {symbol} -> {cache_key}")
        else:
            self.logger.error(f"æ•°æ®ä¿å­˜å¤±è´¥: {symbol}")
        
        return cache_key
    
    def load_stock_data(self, cache_key: str) -> Optional[Any]:
        """ä»ç¼“å­˜åŠ è½½è‚¡ç¥¨æ•°æ®"""
        cache_data = None
        
        # æ ¹æ®ä¸»è¦åç«¯åŠ è½½
        if self.primary_backend == "redis":
            cache_data = self._load_from_redis(cache_key)
        elif self.primary_backend == "mongodb":
            # MongoDBåŠ è½½é€»è¾‘ï¼ˆç®€åŒ–ç‰ˆï¼‰
            cache_data = self._load_from_file(cache_key)
        
        # å¦‚æœä¸»è¦åç«¯å¤±è´¥ï¼Œå°è¯•æ–‡ä»¶ç¼“å­˜
        if not cache_data and self.fallback_enabled:
            cache_data = self._load_from_file(cache_key)
            if cache_data:
                self.logger.info(f"ä½¿ç”¨æ–‡ä»¶ç¼“å­˜å¤‡ç”¨åŠ è½½: {cache_key}")
        
        if not cache_data:
            return None
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        symbol = cache_data['metadata'].get('symbol', '')
        data_type = cache_data['metadata'].get('data_type', 'stock_data')
        ttl_seconds = self._get_ttl_seconds(symbol, data_type)
        
        if not self._is_cache_valid(cache_data['timestamp'], ttl_seconds):
            self.logger.info(f"ç¼“å­˜å·²è¿‡æœŸ: {cache_key}")
            return None
        
        return cache_data['data']
    
    def find_cached_stock_data(self, symbol: str, start_date: str = None, 
                              end_date: str = None, data_source: str = "default") -> Optional[str]:
        """æŸ¥æ‰¾ç¼“å­˜çš„è‚¡ç¥¨æ•°æ®"""
        cache_key = self._get_cache_key(symbol, start_date or "", end_date or "", data_source)
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
        if self.load_stock_data(cache_key) is not None:
            return cache_key
        
        return None
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'primary_backend': self.primary_backend,
            'mongodb_enabled': self.mongodb_enabled,
            'redis_enabled': self.redis_enabled,
            'fallback_enabled': self.fallback_enabled,
            'cache_directory': str(self.cache_dir),
            'file_cache_count': len(list(self.cache_dir.glob("*.pkl"))),
        }
        
        # Redisç»Ÿè®¡
        if self.redis_client:
            try:
                redis_info = self.redis_client.info()
                stats['redis_memory_used'] = redis_info.get('used_memory_human', 'N/A')
                stats['redis_keys'] = self.redis_client.dbsize()
            except:
                stats['redis_status'] = 'Error'
        
        return stats


# å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
_cache_manager = None

def get_cache() -> AdaptiveCacheManager:
    """è·å–å…¨å±€è‡ªé€‚åº”ç¼“å­˜ç®¡ç†å™¨"""
    global _cache_manager
    if _cache_manager is None:
        _cache_manager = AdaptiveCacheManager()
    return _cache_manager


def main():
    """æµ‹è¯•è‡ªé€‚åº”ç¼“å­˜ç®¡ç†å™¨"""
    logger.info(f"ğŸ”§ æµ‹è¯•è‡ªé€‚åº”ç¼“å­˜ç®¡ç†å™¨")
    logger.info(f"=")
    
    # åˆ›å»ºç¼“å­˜ç®¡ç†å™¨
    cache = get_cache()
    
    # æ˜¾ç¤ºçŠ¶æ€
    stats = cache.get_cache_stats()
    logger.info(f"\nğŸ“Š ç¼“å­˜çŠ¶æ€:")
    for key, value in stats.items():
        logger.info(f"  {key}: {value}")
    
    # æµ‹è¯•ç¼“å­˜åŠŸèƒ½
    logger.info(f"\nğŸ’¾ æµ‹è¯•ç¼“å­˜åŠŸèƒ½...")
    
    test_data = "æµ‹è¯•è‚¡ç¥¨æ•°æ® - AAPL"
    cache_key = cache.save_stock_data(
        symbol="AAPL",
        data=test_data,
        start_date="2024-01-01",
        end_date="2024-12-31",
        data_source="test"
    )
    logger.info(f"âœ… æ•°æ®ä¿å­˜: {cache_key}")
    
    # åŠ è½½æ•°æ®
    loaded_data = cache.load_stock_data(cache_key)
    if loaded_data == test_data:
        logger.info(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
    else:
        logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥")
    
    # æŸ¥æ‰¾ç¼“å­˜
    found_key = cache.find_cached_stock_data(
        symbol="AAPL",
        start_date="2024-01-01",
        end_date="2024-12-31",
        data_source="test"
    )
    
    if found_key:
        logger.info(f"âœ… ç¼“å­˜æŸ¥æ‰¾æˆåŠŸ: {found_key}")
    else:
        logger.error(f"âŒ ç¼“å­˜æŸ¥æ‰¾å¤±è´¥")
    
    logger.info(f"\nğŸ‰ è‡ªé€‚åº”ç¼“å­˜ç®¡ç†å™¨æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    main()
