#!/usr/bin/env python3
"""
æ—¥å¿—åˆ†æå·¥å…·
åˆ†æTradingAgents-CNçš„æ—¥å¿—æ–‡ä»¶ï¼Œæä¾›ç»Ÿè®¡å’Œæ´å¯Ÿ
"""

import json
import re
import sys
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from collections import defaultdict, Counter
import argparse

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('scripts')



class LogAnalyzer:
    """æ—¥å¿—åˆ†æå™¨"""
    
    def __init__(self, log_file: Path):
        self.log_file = log_file
        self.entries = []
        self.structured_entries = []
        
    def parse_logs(self):
        """è§£ææ—¥å¿—æ–‡ä»¶"""
        if not self.log_file.exists():
            logger.error(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {self.log_file}")
            return
            
        logger.info(f"ğŸ“– è§£ææ—¥å¿—æ–‡ä»¶: {self.log_file}")
        
        with open(self.log_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue
                    
                # å°è¯•è§£æç»“æ„åŒ–æ—¥å¿—ï¼ˆJSONï¼‰
                if line.startswith('{'):
                    try:
                        entry = json.loads(line)
                        entry['line_number'] = line_num
                        self.structured_entries.append(entry)
                        continue
                    except json.JSONDecodeError:
                        pass
                
                # è§£ææ™®é€šæ—¥å¿—
                entry = self._parse_regular_log(line, line_num)
                if entry:
                    self.entries.append(entry)
        
        logger.info(f"âœ… è§£æå®Œæˆ: {len(self.entries)} æ¡æ™®é€šæ—¥å¿—, {len(self.structured_entries)} æ¡ç»“æ„åŒ–æ—¥å¿—")
    
    def _parse_regular_log(self, line: str, line_num: int) -> Optional[Dict[str, Any]]:
        """è§£ææ™®é€šæ—¥å¿—è¡Œ"""
        # åŒ¹é…æ ¼å¼: 2025-01-15 10:30:45,123 | module_name | INFO | message
        pattern = r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3}) \| ([^|]+) \| ([^|]+) \| (.+)'
        match = re.match(pattern, line)
        
        if match:
            timestamp_str, logger_name, level, message = match.groups()
            try:
                timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S,%f')
            except ValueError:
                timestamp = None
                
            return {
                'timestamp': timestamp,
                'logger': logger_name.strip(),
                'level': level.strip(),
                'message': message.strip(),
                'line_number': line_num,
                'raw_line': line
            }
        
        return None
    
    def analyze_performance(self) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½ç›¸å…³æ—¥å¿—"""
        logger.info(f"\nğŸ“Š æ€§èƒ½åˆ†æ")
        logger.info(f"=")
        
        analysis = {
            'slow_operations': [],
            'analysis_times': [],
            'token_usage': [],
            'cost_summary': {'total_cost': 0, 'by_provider': defaultdict(float)}
        }
        
        # åˆ†ææ‰€æœ‰æ—¥å¿—æ¡ç›®
        all_entries = self.entries + self.structured_entries
        
        for entry in all_entries:
            message = entry.get('message', '')
            
            # æ£€æµ‹æ…¢æ“ä½œ
            if 'è€—æ—¶' in message or 'duration' in entry:
                duration = self._extract_duration(message, entry)
                if duration and duration > 5.0:  # è¶…è¿‡5ç§’
                    analysis['slow_operations'].append({
                        'timestamp': entry.get('timestamp'),
                        'duration': duration,
                        'message': message,
                        'logger': entry.get('logger', '')
                    })
            
            # åˆ†æå®Œæˆæ—¶é—´
            if 'åˆ†æå®Œæˆ' in message or 'analysis_complete' in entry.get('event_type', ''):
                duration = self._extract_duration(message, entry)
                if duration:
                    analysis['analysis_times'].append(duration)
            
            # Tokenä½¿ç”¨ç»Ÿè®¡
            if 'Tokenä½¿ç”¨' in message or 'token_usage' in entry.get('event_type', ''):
                cost = self._extract_cost(message, entry)
                provider = self._extract_provider(message, entry)
                if cost:
                    analysis['cost_summary']['total_cost'] += cost
                    if provider:
                        analysis['cost_summary']['by_provider'][provider] += cost
        
        # è¾“å‡ºåˆ†æç»“æœ
        if analysis['slow_operations']:
            logger.info(f"ğŸŒ æ…¢æ“ä½œ ({len(analysis['slow_operations'])} ä¸ª):")
            for op in analysis['slow_operations'][:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                logger.info(f"  - {op['duration']:.2f}s: {op['message'][:80]}...")
        
        if analysis['analysis_times']:
            avg_time = sum(analysis['analysis_times']) / len(analysis['analysis_times'])
            logger.info(f"â±ï¸  å¹³å‡åˆ†ææ—¶é—´: {avg_time:.2f}s")
            logger.info(f"ğŸ“ˆ åˆ†ææ¬¡æ•°: {len(analysis['analysis_times'])}")
        
        if analysis['cost_summary']['total_cost'] > 0:
            logger.info(f"ğŸ’° æ€»æˆæœ¬: Â¥{analysis['cost_summary']['total_cost']:.4f}")
            for provider, cost in analysis['cost_summary']['by_provider'].items():
                logger.info(f"  - {provider}: Â¥{cost:.4f}")
        
        return analysis
    
    def analyze_errors(self) -> Dict[str, Any]:
        """åˆ†æé”™è¯¯æ—¥å¿—"""
        logger.error(f"\nâŒ é”™è¯¯åˆ†æ")
        logger.info(f"=")
        
        error_entries = []
        warning_entries = []
        
        all_entries = self.entries + self.structured_entries
        
        for entry in all_entries:
            level = entry.get('level', '').upper()
            if level == 'ERROR':
                error_entries.append(entry)
            elif level == 'WARNING':
                warning_entries.append(entry)
        
        logger.error(f"ğŸ”´ é”™è¯¯æ•°é‡: {len(error_entries)}")
        logger.warning(f"ğŸŸ¡ è­¦å‘Šæ•°é‡: {len(warning_entries)}")
        
        # é”™è¯¯åˆ†ç±»
        error_patterns = defaultdict(int)
        for entry in error_entries:
            message = entry.get('message', '')
            # ç®€å•çš„é”™è¯¯åˆ†ç±»
            if 'API' in message or 'api' in message:
                error_patterns['APIé”™è¯¯'] += 1
            elif 'ç½‘ç»œ' in message or 'network' in message or 'connection' in message:
                error_patterns['ç½‘ç»œé”™è¯¯'] += 1
            elif 'æ•°æ®åº“' in message or 'database' in message or 'mongodb' in message:
                error_patterns['æ•°æ®åº“é”™è¯¯'] += 1
            elif 'PDF' in message or 'pdf' in message:
                error_patterns['PDFå¯¼å‡ºé”™è¯¯'] += 1
            else:
                error_patterns['å…¶ä»–é”™è¯¯'] += 1
        
        if error_patterns:
            logger.error(f"\né”™è¯¯åˆ†ç±»:")
            for pattern, count in error_patterns.most_common():
                logger.info(f"  - {pattern}: {count}")
        
        # æ˜¾ç¤ºæœ€è¿‘çš„é”™è¯¯
        if error_entries:
            logger.error(f"\næœ€è¿‘çš„é”™è¯¯:")
            recent_errors = sorted(error_entries, key=lambda x: x.get('timestamp', datetime.min))[-3:]
            for error in recent_errors:
                timestamp = error.get('timestamp', 'Unknown')
                message = error.get('message', '')[:100]
                logger.info(f"  - {timestamp}: {message}...")
        
        return {
            'error_count': len(error_entries),
            'warning_count': len(warning_entries),
            'error_patterns': dict(error_patterns),
            'recent_errors': error_entries[-5:] if error_entries else []
        }
    
    def analyze_usage(self) -> Dict[str, Any]:
        """åˆ†æä½¿ç”¨æƒ…å†µ"""
        logger.info(f"\nğŸ“ˆ ä½¿ç”¨æƒ…å†µåˆ†æ")
        logger.info(f"=")
        
        analysis = {
            'daily_usage': defaultdict(int),
            'hourly_usage': defaultdict(int),
            'module_usage': defaultdict(int),
            'analysis_types': defaultdict(int)
        }
        
        all_entries = self.entries + self.structured_entries
        
        for entry in all_entries:
            timestamp = entry.get('timestamp')
            if timestamp:
                # æŒ‰æ—¥ç»Ÿè®¡
                date_str = timestamp.strftime('%Y-%m-%d')
                analysis['daily_usage'][date_str] += 1
                
                # æŒ‰å°æ—¶ç»Ÿè®¡
                hour = timestamp.hour
                analysis['hourly_usage'][hour] += 1
            
            # æ¨¡å—ä½¿ç”¨ç»Ÿè®¡
            logger = entry.get('logger', '')
            if logger:
                analysis['module_usage'][logger] += 1
            
            # åˆ†æç±»å‹ç»Ÿè®¡
            message = entry.get('message', '')
            if 'å¼€å§‹åˆ†æ' in message or 'analysis_start' in entry.get('event_type', ''):
                analysis_type = entry.get('analysis_type', 'æœªçŸ¥')
                analysis['analysis_types'][analysis_type] += 1
        
        # è¾“å‡ºç»“æœ
        if analysis['daily_usage']:
            logger.info(f"ğŸ“… æ¯æ—¥ä½¿ç”¨é‡:")
            for date, count in sorted(analysis['daily_usage'].items())[-7:]:  # æœ€è¿‘7å¤©
                logger.info(f"  - {date}: {count}")
        
        if analysis['module_usage']:
            logger.info(f"\nğŸ“¦ æ¨¡å—ä½¿ç”¨æƒ…å†µ:")
            for module, count in Counter(analysis['module_usage']).most_common(5):
                logger.info(f"  - {module}: {count}")
        
        if analysis['analysis_types']:
            logger.debug(f"\nğŸ” åˆ†æç±»å‹:")
            for analysis_type, count in Counter(analysis['analysis_types']).most_common():
                logger.info(f"  - {analysis_type}: {count}")
        
        return analysis
    
    def _extract_duration(self, message: str, entry: Dict[str, Any]) -> Optional[float]:
        """ä»æ¶ˆæ¯ä¸­æå–è€—æ—¶"""
        # ä»ç»“æ„åŒ–æ—¥å¿—ä¸­æå–
        if 'duration' in entry:
            return entry['duration']
        
        # ä»æ¶ˆæ¯ä¸­æå–
        match = re.search(r'è€—æ—¶[ï¼š:]\s*(\d+\.?\d*)s', message)
        if match:
            return float(match.group(1))
        
        return None
    
    def _extract_cost(self, message: str, entry: Dict[str, Any]) -> Optional[float]:
        """ä»æ¶ˆæ¯ä¸­æå–æˆæœ¬"""
        # ä»ç»“æ„åŒ–æ—¥å¿—ä¸­æå–
        if 'cost' in entry:
            return entry['cost']
        
        # ä»æ¶ˆæ¯ä¸­æå–
        match = re.search(r'æˆæœ¬[ï¼š:]\s*Â¥(\d+\.?\d*)', message)
        if match:
            return float(match.group(1))
        
        return None
    
    def _extract_provider(self, message: str, entry: Dict[str, Any]) -> Optional[str]:
        """ä»æ¶ˆæ¯ä¸­æå–æä¾›å•†"""
        # ä»ç»“æ„åŒ–æ—¥å¿—ä¸­æå–
        if 'provider' in entry:
            return entry['provider']
        
        # ä»æ¶ˆæ¯ä¸­æå–
        providers = ['DeepSeek', 'OpenAI', 'Tongyi', 'Gemini']
        for provider in providers:
            if provider in message:
                return provider
        
        return None
    
    def generate_report(self) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        logger.info(f"\nğŸ“‹ ç”Ÿæˆåˆ†ææŠ¥å‘Š")
        logger.info(f"=")
        
        performance = self.analyze_performance()
        errors = self.analyze_errors()
        usage = self.analyze_usage()
        
        report = f"""
# TradingAgents-CN æ—¥å¿—åˆ†ææŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
æ—¥å¿—æ–‡ä»¶: {self.log_file}

## æ¦‚è§ˆ
- æ™®é€šæ—¥å¿—æ¡ç›®: {len(self.entries)}
- ç»“æ„åŒ–æ—¥å¿—æ¡ç›®: {len(self.structured_entries)}
- é”™è¯¯æ•°é‡: {errors['error_count']}
- è­¦å‘Šæ•°é‡: {errors['warning_count']}

## æ€§èƒ½åˆ†æ
- æ…¢æ“ä½œæ•°é‡: {len(performance['slow_operations'])}
- å¹³å‡åˆ†ææ—¶é—´: {sum(performance['analysis_times']) / len(performance['analysis_times']):.2f}s (å¦‚æœæœ‰æ•°æ®)
- æ€»æˆæœ¬: Â¥{performance['cost_summary']['total_cost']:.4f}

## ä½¿ç”¨æƒ…å†µ
- æ´»è·ƒæ¨¡å—: {len(usage['module_usage'])}
- åˆ†æç±»å‹: {len(usage['analysis_types'])}

## å»ºè®®
"""
        
        # æ·»åŠ å»ºè®®
        if len(performance['slow_operations']) > 10:
            report += "- âš ï¸ æ£€æµ‹åˆ°è¾ƒå¤šæ…¢æ“ä½œï¼Œå»ºè®®ä¼˜åŒ–æ€§èƒ½\n"
        
        if errors['error_count'] > 0:
            report += f"- âŒ å‘ç° {errors['error_count']} ä¸ªé”™è¯¯ï¼Œå»ºè®®æ£€æŸ¥æ—¥å¿—\n"
        
        if performance['cost_summary']['total_cost'] > 10:
            report += "- ğŸ’° APIæˆæœ¬è¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–è°ƒç”¨ç­–ç•¥\n"
        
        return report


def main():
    parser = argparse.ArgumentParser(description='TradingAgents-CN æ—¥å¿—åˆ†æå·¥å…·')
    parser.add_argument('log_file', help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', help='è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--format', choices=['text', 'json'], default='text', help='è¾“å‡ºæ ¼å¼')
    
    args = parser.parse_args()
    
    log_file = Path(args.log_file)
    analyzer = LogAnalyzer(log_file)
    
    try:
        analyzer.parse_logs()
        report = analyzer.generate_report()
        
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
        else:
            print(report)
            
    except Exception as e:
        logger.error(f"âŒ åˆ†æå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
