"""
Smart Routing Engine
AIé©±åŠ¨çš„æ™ºèƒ½è·¯ç”±ç³»ç»Ÿï¼Œè´Ÿè´£æ ¹æ®ä»»åŠ¡ç‰¹æ€§é€‰æ‹©æœ€ä¼˜æ¨¡å‹
"""

import json
import sqlite3
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta
import numpy as np
from pathlib import Path

from .base_multi_model_adapter import (
    ModelSpec, TaskSpec, ModelSelection, TaskComplexity, ModelProvider
)

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import get_logger
logger = get_logger('smart_routing_engine')


@dataclass
class RoutingDecision:
    """è·¯ç”±å†³ç­–ç»“æœ"""
    selected_model: str
    provider: str
    confidence_score: float
    reasoning: str
    estimated_cost: float
    estimated_time: int
    alternative_models: List[str] = None
    routing_strategy: str = "intelligent"


@dataclass
class ModelPerformanceData:
    """æ¨¡å‹æ€§èƒ½æ•°æ®"""
    model_name: str
    provider: str
    task_type: str
    avg_response_time: float
    success_rate: float
    cost_efficiency: float  # æ€§ä»·æ¯”åˆ†æ•°
    quality_score: float
    last_updated: datetime


class SmartRoutingEngine:
    """AIé©±åŠ¨çš„æ™ºèƒ½è·¯ç”±ç³»ç»Ÿ"""
    
    def __init__(self, database_path: str = None, config: Dict[str, Any] = None):
        self.config = config or {}
        self.database_path = database_path or self._get_default_db_path()
        
        # ä¸‰æ± æ——èˆ°æ¨¡å‹è·¯ç”±ç­–ç•¥æƒé‡é…ç½®ï¼ˆè´¨é‡ä¼˜å…ˆï¼‰
        self.routing_weights = self.config.get('weights', {
            'quality': 0.6,         # è´¨é‡æƒé‡æœ€é«˜
            'performance': 0.3,     # æ€§èƒ½æƒé‡
            'cost': 0.1,           # æˆæœ¬æƒé‡æœ€ä½ï¼ˆè´¨é‡ä¼˜å…ˆï¼‰
            'availability': 0.0     # å¯ç”¨æ€§é€šè¿‡fallbackä¿è¯
        })
        
        # å¼ºåˆ¶å¤šæ ·åŒ–é…ç½®
        self.force_diversity = self.config.get('force_diversity', True)  # é»˜è®¤å¯ç”¨
        self.model_usage_tracker = {}  # è·Ÿè¸ªæ¨¡å‹ä½¿ç”¨æƒ…å†µ
        self.diversity_threshold = self.config.get('diversity_threshold', 0.4)  # é™ä½é˜ˆå€¼åˆ°40%
        self.diversity_weight = self.config.get('diversity_weight', 0.8)  # æé«˜å¤šæ ·åŒ–æƒé‡
        
        # ä¸‰æ± æ˜ å°„é…ç½® - è°ƒæ•´ä¸ºä¸¤æ± æ¶æ„
        self.pool_mapping = self.config.get('pool_mapping', {
            'deep_reasoning': {
                'flagship_model': 'gemini-2.5-pro',
                'target_agents': ['fundamental_expert', 'chief_decision_officer', 'risk_manager', 'policy_researcher', 'compliance_officer'],
                'task_types': ['financial_report', 'risk_assessment', 'decision_making', 'policy_analysis', 'compliance_check'],
                'capabilities': ['multimodal', 'long_context', 'financial_analysis', 'reasoning_chains']
            },
            'technical_longseq': {
                'flagship_model': 'deepseek-ai/DeepSeek-V3',
                'target_agents': ['technical_analyst', 'news_hunter', 'sentiment_analyst', 'tool_engineer'],
                'task_types': ['technical_analysis', 'news_analysis', 'sentiment_analysis', 'tool_development', 'code_generation', 'backtesting'],
                'capabilities': ['long_context', 'chinese_optimized', 'technical_analysis', 'time_series', 'code_generation']
            }
        })
        
        # ä¸‰æ± æ——èˆ°æ¨¡å‹èƒ½åŠ›æ˜ å°„
        self.flagship_model_capabilities = {
            # ğŸ§  é€šç”¨æ·±åº¦æ¨ç†æ± æ——èˆ°
            'gemini-2.5-pro': {
                'reasoning': 0.95,      # æœ€å¼ºæ¨ç†èƒ½åŠ›
                'multimodal': 0.95,     # åŸç”Ÿå¤šæ¨¡æ€
                'long_context': 0.9,    # 30K+ä¸Šä¸‹æ–‡
                'chinese': 0.8,         # ä¸­æ–‡èƒ½åŠ›è‰¯å¥½
                'financial_analysis': 0.92,  # é‡‘èåˆ†ææœ€ä½³
                'cost_efficiency': 0.6,  # è´¨é‡ä¼˜å…ˆï¼Œæˆæœ¬æ¬¡è¦
                'speed': 0.5,           # æ·±åº¦æ¨ç†éœ€æ—¶
                'reliability': 0.95     # æé«˜å¯é æ€§
            },
            # ğŸ”„ æŠ€æœ¯é¢&é•¿åºåˆ—æ± æ——èˆ°
            'deepseek-ai/DeepSeek-V3': {
                'reasoning': 0.9,       # å¼ºæ¨ç†èƒ½åŠ›
                'multimodal': 0.7,      # æœ‰é™å¤šæ¨¡æ€
                'long_context': 0.95,   # 100K+ä¸Šä¸‹æ–‡æœ€ä½³
                'chinese': 0.95,        # ä¸­æ–‡èƒ½åŠ›æœ€å¼º
                'technical_analysis': 0.92,  # æŠ€æœ¯åˆ†æä¸“é•¿
                'time_series': 0.9,     # é•¿åºåˆ—æ•°æ®å¤„ç†
                'cost_efficiency': 0.85, # é«˜æ€§ä»·æ¯”
                'speed': 0.7,           # è¾ƒå¿«é€Ÿåº¦
                'reliability': 0.9      # é«˜å¯é æ€§
            }
        }
        
        # æ——èˆ°æ¨¡å‹ä¸“ç”¨tokené™åˆ¶ï¼ˆä¸è®¡æˆæœ¬ï¼‰- åŸºäºå®˜ç½‘å®é™…é™åˆ¶
        self.flagship_token_limits = self.config.get('flagship_token_limits', {
            'gemini-2.5-pro': 1000000,      # Googleå®˜ç½‘ - 1M tokens
            'deepseek-ai/DeepSeek-V3': 128000,  # SiliconFlowå¹³å° - 128K tokens (æ¯”DeepSeekå®˜ç½‘64Kæ›´é«˜)
        })
        
        # ä»»åŠ¡ç±»å‹åˆ°æ± çš„æ˜ å°„æƒé‡ - è°ƒæ•´ä¸ºä¸¤æ± æ¶æ„
        self.task_pool_affinity = {
            'financial_report': {'deep_reasoning': 0.9, 'technical_longseq': 0.1},
            'technical_analysis': {'deep_reasoning': 0.3, 'technical_longseq': 0.9},
            'news_analysis': {'deep_reasoning': 0.2, 'technical_longseq': 0.9},
            'sentiment_analysis': {'deep_reasoning': 0.3, 'technical_longseq': 0.9},
            'risk_assessment': {'deep_reasoning': 0.9, 'technical_longseq': 0.2},
            'decision_making': {'deep_reasoning': 0.95, 'technical_longseq': 0.4},
            'policy_analysis': {'deep_reasoning': 0.9, 'technical_longseq': 0.1},
            'code_generation': {'deep_reasoning': 0.1, 'technical_longseq': 0.95},  # ä»£ç ä»»åŠ¡è½¬ç»™technical_longseq
            'tool_development': {'deep_reasoning': 0.2, 'technical_longseq': 0.9},
            'backtesting': {'deep_reasoning': 0.3, 'technical_longseq': 0.9},       # æ–°å¢
            'compliance_check': {'deep_reasoning': 0.9, 'technical_longseq': 0.0},
            'fundamental_analysis': {'deep_reasoning': 0.9, 'technical_longseq': 0.1},
            'general': {'deep_reasoning': 0.6, 'technical_longseq': 0.4}
        }
        
        # æ¨¡å‹ç‰¹æ€§é…ç½®
        self.model_capabilities = {
            # SiliconFlowæ¨¡å‹ - ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹åç§°
            'deepseek-ai/DeepSeek-R1': {
                'reasoning': 0.95, 'speed': 0.5, 'chinese': 0.8, 'cost': 0.6, 'reliability': 0.9
            },
            'deepseek-ai/DeepSeek-V3': {
                'reasoning': 0.9, 'speed': 0.7, 'chinese': 0.85, 'cost': 0.8, 'reliability': 0.9
            },
            'moonshotai/Kimi-K2-Instruct': {
                'reasoning': 0.8, 'speed': 0.6, 'chinese': 0.8, 'long_context': 0.95, 'cost': 0.6, 'reliability': 0.8
            },
            'Qwen/Qwen2.5-Coder-32B-Instruct': {
                'reasoning': 0.8, 'speed': 0.7, 'chinese': 0.9, 'code_generation': 0.95, 'cost': 0.8, 'reliability': 0.85
            },
            'Pro/Qwen/Qwen2.5-72B-Instruct': {
                'reasoning': 0.87, 'speed': 0.6, 'chinese': 0.9, 'cost': 0.65, 'reliability': 0.9
            },
            'deepseek-ai/deepseek-llm-67b-chat': {
                'reasoning': 0.8, 'speed': 0.6, 'chinese': 0.8, 'cost': 0.7, 'reliability': 0.85
            },
            'meta-llama/Meta-Llama-3.1-405B-Instruct': {
                'reasoning': 0.9, 'speed': 0.4, 'chinese': 0.6, 'cost': 0.5, 'reliability': 0.85
            },
            'meta-llama/Meta-Llama-3.1-70B-Instruct': {
                'reasoning': 0.85, 'speed': 0.6, 'chinese': 0.6, 'cost': 0.7, 'reliability': 0.85
            },
            
            # Googleæ¨¡å‹ - ä¿æŒæ‰€æœ‰æ¨¡å‹é…ç½®ï¼Œä½†gemini-2.5-proä¼˜å…ˆçº§æœ€é«˜
            'gemini-2.5-pro': {
                'reasoning': 0.95, 'multimodal': 0.95, 'long_context': 0.9, 'chinese': 0.8, 'financial_analysis': 0.92, 'speed': 0.5, 'cost': 0.3, 'reliability': 0.95
            },
            'Qwen/Qwen3-235B-A22B-Instruct-2507': {
                'reasoning': 0.92, 'speed': 0.5, 'chinese': 0.95, 'long_context': 0.9, 'cost': 0.4, 'reliability': 0.9
            },
            'gemini-1.5-pro': {
                'reasoning': 0.85, 'speed': 0.6, 'chinese': 0.7, 'cost': 0.6, 'reliability': 0.9
            },
            'gemini-2.0-flash': {
                'reasoning': 0.8, 'speed': 0.8, 'chinese': 0.7, 'cost': 0.75, 'reliability': 0.85
            },
            'gemini-2.5-flash': {
                'reasoning': 0.85, 'speed': 0.9, 'chinese': 0.8, 'cost': 0.8, 'reliability': 0.9
            },
            
            # DeepSeekç›´è¿
            'deepseek-chat': {
                'reasoning': 0.85, 'speed': 0.7, 'chinese': 0.85, 'cost': 0.8, 'reliability': 0.85
            }
        }
        
        # åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„ï¼ˆå¦‚æœéœ€è¦ï¼‰
        self._initialize_database_tables()
        
        # éªŒè¯è·¯ç”±å¼•æ“å¯ç”¨æ€§
        usability_status = self._validate_engine_usability()
        if usability_status['usable']:
            logger.info(f"æ™ºèƒ½è·¯ç”±å¼•æ“åˆå§‹åŒ–å®Œæˆ - {usability_status['message']}")
        else:
            logger.warning(f"æ™ºèƒ½è·¯ç”±å¼•æ“åˆå§‹åŒ–å®Œæˆä½†å­˜åœ¨é—®é¢˜ - {usability_status['message']}")
    
    def route_task(self,
                  task_description: str,
                  agent_role: str,
                  task_spec: TaskSpec,
                  available_models: Dict[str, ModelSpec],
                  context: Dict[str, Any] = None) -> RoutingDecision:
        """
        åŸºäºä¸‰æ± æ——èˆ°æ¨¡å‹æ¶æ„è¿›è¡Œä»»åŠ¡è·¯ç”±ï¼ˆæ”¯æŒå¼ºåˆ¶å¤šæ ·åŒ–ï¼‰
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            agent_role: æ™ºèƒ½ä½“è§’è‰²
            task_spec: ä»»åŠ¡è§„æ ¼
            available_models: å¯ç”¨æ¨¡å‹åˆ—è¡¨
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            RoutingDecision: è·¯ç”±å†³ç­–ç»“æœ
        """
        context = context or {}
        
        try:
            # å¦‚æœå¯ç”¨å¼ºåˆ¶å¤šæ ·åŒ–ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦éœ€è¦å¤šæ ·åŒ–è·¯ç”±
            if self.force_diversity and self._should_diversify_routing(agent_role, available_models):
                diverse_decision = self._route_with_diversity(
                    task_description, agent_role, task_spec, available_models, context
                )
                if diverse_decision:
                    return diverse_decision
            
            # ä¼˜å…ˆå°è¯•ä¸‰æ± æ——èˆ°æ¨¡å‹è·¯ç”±
            pool_decision = self._route_with_flagship_pools(
                task_description, agent_role, task_spec, available_models, context
            )
            if pool_decision:
                return pool_decision
            
            # å›é€€åˆ°ä¼ ç»Ÿè·¯ç”±é€»è¾‘
            return self._route_with_traditional_logic(
                task_description, agent_role, task_spec, available_models, context
            )
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½è·¯ç”±å¤±è´¥: {e}", exc_info=True)
            # æœ€ç»ˆå›é€€åˆ°é»˜è®¤é€‰æ‹©
            default_model = self._get_default_model(available_models, task_spec)
            return RoutingDecision(
                selected_model=default_model.name,
                provider=default_model.provider.value,
                confidence_score=0.3,
                reasoning=f"è·¯ç”±å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹: {str(e)}",
                estimated_cost=0.01,
                estimated_time=5000,
                routing_strategy="fallback"
            )
    
    def _route_with_flagship_pools(self,
                                 task_description: str,
                                 agent_role: str,
                                 task_spec: TaskSpec,
                                 available_models: Dict[str, ModelSpec],
                                 context: Dict[str, Any]) -> Optional[RoutingDecision]:
        """ä½¿ç”¨ä¸‰æ± æ——èˆ°æ¨¡å‹æ¶æ„è¿›è¡Œè·¯ç”±"""
        try:
            # 1. ç¡®å®šç›®æ ‡æ± 
            target_pool = self._determine_target_pool(agent_role, task_spec, context)
            if not target_pool:
                return None
            
            # 2. è·å–ç›®æ ‡æ± çš„æ——èˆ°æ¨¡å‹
            flagship_model = self.pool_mapping[target_pool]['flagship_model']
            
            # 3. æ£€æŸ¥æ——èˆ°æ¨¡å‹æ˜¯å¦å¯ç”¨
            if flagship_model in available_models:
                model_spec = available_models[flagship_model]
                
                # 4. è®¡ç®—ç½®ä¿¡åº¦å’Œæˆæœ¬
                confidence_score = self._calculate_pool_confidence(target_pool, agent_role, task_spec)
                estimated_cost = self._estimate_task_cost(model_spec, task_spec)
                estimated_time = self._estimate_flagship_execution_time(flagship_model, task_spec)
                
                # 5. ç”Ÿæˆæ¨ç†è¯´æ˜
                reasoning = self._generate_pool_reasoning(target_pool, flagship_model, agent_role, task_spec)
                
                # 6. è·å–å¤‡é€‰æ¨¡å‹
                alternative_models = self._get_pool_alternatives(target_pool, available_models)
                
                decision = RoutingDecision(
                    selected_model=flagship_model,
                    provider=model_spec.provider.value,
                    confidence_score=confidence_score,
                    reasoning=reasoning,
                    estimated_cost=estimated_cost,
                    estimated_time=estimated_time,
                    alternative_models=alternative_models,
                    routing_strategy="flagship_pool"
                )
                
                # 7. è®°å½•è·¯ç”±å†³ç­–å’Œæ›´æ–°ä½¿ç”¨ç»Ÿè®¡
                self._log_routing_decision(decision, agent_role, task_spec, context)
                if self.force_diversity:
                    self._update_model_usage(flagship_model)
                
                logger.info(f"ä¸‰æ± è·¯ç”±å†³ç­–å®Œæˆ: {target_pool} æ±  -> {flagship_model} (ç½®ä¿¡åº¦: {confidence_score:.3f})")
                return decision
            
            return None
            
        except Exception as e:
            logger.warning(f"ä¸‰æ± è·¯ç”±å¤±è´¥: {e}")
            return None
    
    def _determine_target_pool(self, agent_role: str, task_spec: TaskSpec, context: Dict[str, Any]) -> Optional[str]:
        """ç¡®å®šç›®æ ‡æ± """
        # 1. æ ¹æ®æ™ºèƒ½ä½“è§’è‰²ç¡®å®šæ± 
        for pool_name, pool_config in self.pool_mapping.items():
            if agent_role in pool_config['target_agents']:
                logger.debug(f"æ ¹æ®æ™ºèƒ½ä½“è§’è‰² {agent_role} é€‰æ‹©æ± : {pool_name}")
                return pool_name
        
        # 2. æ ¹æ®ä»»åŠ¡ç±»å‹ç¡®å®šæ± 
        if task_spec.task_type in self.task_pool_affinity:
            affinities = self.task_pool_affinity[task_spec.task_type]
            best_pool = max(affinities.items(), key=lambda x: x[1])[0]
            logger.debug(f"æ ¹æ®ä»»åŠ¡ç±»å‹ {task_spec.task_type} é€‰æ‹©æ± : {best_pool}")
            return best_pool
        
        # 3. æ ¹æ®ç‰¹æ®Šéœ€æ±‚ç¡®å®šæ± 
        if context.get('code_generation_required') or task_spec.task_type in ['code_generation', 'tool_development', 'backtesting']:
            return 'technical_longseq'  # ä»£ç ä»»åŠ¡åˆ†é…ç»™technical_longseqæ± 
        
        if context.get('long_context') or task_spec.estimated_tokens > 20000:
            return 'technical_longseq'
        
        if task_spec.requires_reasoning or task_spec.complexity == TaskComplexity.HIGH:
            return 'deep_reasoning'
        
        # 4. é»˜è®¤é€‰æ‹©æ·±åº¦æ¨ç†æ± 
        logger.debug(f"æ— ç‰¹å®šåŒ¹é…ï¼Œé»˜è®¤é€‰æ‹©æ·±åº¦æ¨ç†æ± ")
        return 'deep_reasoning'
    
    def _calculate_pool_confidence(self, pool_name: str, agent_role: str, task_spec: TaskSpec) -> float:
        """è®¡ç®—æ± é€‰æ‹©ç½®ä¿¡åº¦"""
        confidence = 0.7  # åŸºç¡€ç½®ä¿¡åº¦
        
        # æ™ºèƒ½ä½“åŒ¹é…åŠ åˆ†
        if agent_role in self.pool_mapping[pool_name]['target_agents']:
            confidence += 0.15
        
        # ä»»åŠ¡ç±»å‹åŒ¹é…åŠ åˆ†
        if task_spec.task_type in self.task_pool_affinity:
            affinity_score = self.task_pool_affinity[task_spec.task_type].get(pool_name, 0)
            confidence += affinity_score * 0.15
        
        # å¤æ‚åº¦åŒ¹é…åŠ åˆ†
        if pool_name == 'deep_reasoning' and task_spec.complexity == TaskComplexity.HIGH:
            confidence += 0.1
        elif pool_name == 'technical_longseq' and task_spec.estimated_tokens > 10000:
            confidence += 0.1
        elif pool_name == 'technical_longseq' and task_spec.task_type in ['code_generation', 'tool_development', 'backtesting']:
            confidence += 0.15
        
        return min(confidence, 0.95)
    
    def _generate_pool_reasoning(self, pool_name: str, model_name: str, agent_role: str, task_spec: TaskSpec) -> str:
        """ç”Ÿæˆæ± é€‰æ‹©æ¨ç†è¯´æ˜"""
        pool_descriptions = {
            'deep_reasoning': f"ğŸ§  é€šç”¨æ·±åº¦æ¨ç†æ± æ——èˆ°æ¨¡å‹ {model_name}ï¼Œæ“…é•¿å¤šæ¨¡æ€é€»è¾‘æ¨ç†å’Œé‡‘èåˆ†æ",
            'technical_longseq': f"ğŸ”„ æŠ€æœ¯é¢&é•¿åºåˆ—&ä»£ç æ± æ——èˆ°æ¨¡å‹ {model_name}ï¼Œæ”¯æŒ100K+ä¸Šä¸‹æ–‡ã€ä¸­æ–‡ä¼˜åŒ–å’Œä»£ç ç”Ÿæˆ"
        }
        
        base_reason = pool_descriptions.get(pool_name, f"ä½¿ç”¨{pool_name}æ± çš„{model_name}æ¨¡å‹")
        
        # æ·»åŠ å…·ä½“é€‰æ‹©ç†ç”±
        reasons = []
        if agent_role in self.pool_mapping[pool_name]['target_agents']:
            reasons.append(f"é’ˆå¯¹{agent_role}æ™ºèƒ½ä½“ä¼˜åŒ–")
        
        if task_spec.complexity == TaskComplexity.HIGH and pool_name == 'deep_reasoning':
            reasons.append("é«˜å¤æ‚åº¦ä»»åŠ¡éœ€æ±‚æœ€å¼ºæ¨ç†èƒ½åŠ›")
        
        if task_spec.estimated_tokens > 20000 and pool_name == 'technical_longseq':
            reasons.append(f"é•¿ä¸Šä¸‹æ–‡({task_spec.estimated_tokens} tokens)ä»»åŠ¡")
        
        if task_spec.task_type in ['code_generation', 'tool_development', 'backtesting'] and pool_name == 'technical_longseq':
            reasons.append("ä»£ç ç”Ÿæˆå’Œå¼€å‘ä»»åŠ¡")
        
        if reasons:
            return f"{base_reason}ï¼Œé€‰æ‹©ç†ç”±ï¼š{'ã€'.join(reasons)}"
        else:
            return base_reason
    
    def _get_pool_alternatives(self, pool_name: str, available_models: Dict[str, ModelSpec]) -> List[str]:
        """è·å–æ± å†…å¤‡é€‰æ¨¡å‹"""
        alternatives = []
        
        # è·å–æ± å†…æ‰€æœ‰æ¨¡å‹ï¼ˆé™¤äº†æ——èˆ°æ¨¡å‹ï¼‰
        flagship = self.pool_mapping[pool_name]['flagship_model']
        
        # æ·»åŠ æ± å†…å…¶ä»–æ¨¡å‹ - è°ƒæ•´ä¸ºä¸¤æ± æ¶æ„
        pool_models = {
            'deep_reasoning': ['Qwen/Qwen3-235B-A22B-Instruct-2507', 'gemini-2.5-pro'],
            'technical_longseq': ['deepseek-ai/DeepSeek-R1', 'moonshotai/Kimi-K2-Instruct', 'Pro/Qwen/Qwen2.5-72B-Instruct', 'deepseek-chat']
        }
        
        for model in pool_models.get(pool_name, []):
            if model in available_models and model != flagship:
                alternatives.append(model)
        
        return alternatives[:3]  # æœ€å¤š3ä¸ªå¤‡é€‰
    
    def _estimate_flagship_execution_time(self, model_name: str, task_spec: TaskSpec) -> int:
        """ä¼°ç®—æ——èˆ°æ¨¡å‹æ‰§è¡Œæ—¶é—´"""
        base_times = {
            'gemini-2.5-pro': 8000,      # æ·±åº¦æ¨ç†éœ€è¦æ›´å¤šæ—¶é—´
            'deepseek-ai/DeepSeek-V3': 5000,  # ä¸­ç­‰é€Ÿåº¦ï¼ŒåŒ…å«ä»£ç èƒ½åŠ›
        }
        
        base_time = base_times.get(model_name, 5000)
        
        # æ ¹æ®ä»»åŠ¡å¤æ‚åº¦è°ƒæ•´
        if task_spec.complexity == TaskComplexity.HIGH:
            base_time = int(base_time * 1.5)
        elif task_spec.complexity == TaskComplexity.LOW:
            base_time = int(base_time * 0.8)
        
        # æ ¹æ®tokenæ•°è°ƒæ•´
        if task_spec.estimated_tokens > 20000:
            base_time = int(base_time * 1.3)
        
        return base_time
    
    def _get_flagship_max_tokens(self, model_name: str) -> int:
        """è·å–æ——èˆ°æ¨¡å‹çš„æœ€å¤§tokené™åˆ¶ï¼ˆä¸è®¡æˆæœ¬ï¼‰"""
        return self.flagship_token_limits.get(model_name, 100000)  # é»˜è®¤10ä¸‡tokens
    
    def _should_diversify_routing(self, agent_role: str, available_models: Dict[str, ModelSpec]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›è¡Œå¤šæ ·åŒ–è·¯ç”±"""
        if not self.model_usage_tracker:
            return False
        
        # è®¡ç®—å½“å‰ä¸»å¯¼æ¨¡å‹çš„ä½¿ç”¨ç‡
        total_usage = sum(self.model_usage_tracker.values())
        if total_usage < 2:  # è‡³å°‘éœ€è¦2æ¬¡è¯·æ±‚æ‰å¼€å§‹åˆ¤æ–­
            return False
        
        # æ‰¾åˆ°ä½¿ç”¨æœ€å¤šçš„æ¨¡å‹
        most_used_model = max(self.model_usage_tracker.items(), key=lambda x: x[1])
        dominant_usage_rate = most_used_model[1] / total_usage
        
        # è®¡ç®—ç†æƒ³çš„å¹³å‡ä½¿ç”¨ç‡ï¼ˆå¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½å‡åŒ€ä½¿ç”¨ï¼‰
        num_available_models = len([m for m in available_models.keys() if m in self.model_capabilities])
        ideal_usage_rate = 1.0 / num_available_models if num_available_models > 0 else 1.0
        
        # åŠ¨æ€é˜ˆå€¼ï¼šåŸºäºç†æƒ³å€¼ + å®¹å¿å€¼
        dynamic_threshold = ideal_usage_rate + 0.15  # ç†æƒ³å€¼ + 15%å®¹å¿åº¦
        effective_threshold = min(self.diversity_threshold, dynamic_threshold)
        
        # å¦‚æœæŸä¸ªæ¨¡å‹çš„ä½¿ç”¨ç‡è¶…è¿‡åŠ¨æ€é˜ˆå€¼ï¼Œåˆ™éœ€è¦å¤šæ ·åŒ–
        needs_diversification = dominant_usage_rate > effective_threshold
        
        if needs_diversification:
            logger.info(f"è§¦å‘å¤šæ ·åŒ–è·¯ç”±ï¼š{most_used_model[0]} ä½¿ç”¨ç‡ {dominant_usage_rate:.2%} è¶…è¿‡åŠ¨æ€é˜ˆå€¼ {effective_threshold:.0%} (ç†æƒ³å€¼:{ideal_usage_rate:.0%})")
        
        return needs_diversification
    
    def _route_with_diversity(self,
                            task_description: str,
                            agent_role: str,
                            task_spec: TaskSpec,
                            available_models: Dict[str, ModelSpec],
                            context: Dict[str, Any]) -> Optional[RoutingDecision]:
        """åŸºäºå¤šæ ·åŒ–ç­–ç•¥è¿›è¡Œè·¯ç”±"""
        try:
            # æ‰¾åˆ°ä½¿ç”¨æœ€å°‘çš„å¯ç”¨æ¨¡å‹
            model_usage_scores = {}
            
            for model_name, model_spec in available_models.items():
                if model_name in self.model_capabilities:
                    # ä½¿ç”¨æƒ…å†µå¾—åˆ†ï¼ˆä½¿ç”¨è¶Šå°‘å¾—åˆ†è¶Šé«˜ï¼‰
                    usage_count = self.model_usage_tracker.get(model_name, 0)
                    total_usage = sum(self.model_usage_tracker.values()) or 1
                    usage_score = 1.0 - (usage_count / total_usage)
                    
                    # åŸºç¡€èƒ½åŠ›å¾—åˆ†
                    task_analysis = self._analyze_task_characteristics(
                        task_description, task_spec, context
                    )
                    capability_score = self._calculate_model_score(
                        model_name, model_spec, task_analysis, task_spec
                    )
                    
                    # ç»¼åˆå¾—åˆ†ï¼šæé«˜å¤šæ ·åŒ–æƒé‡åˆ°0.8ï¼Œèƒ½åŠ›æƒé‡ä¸º0.2
                    diversity_weight = self.diversity_weight
                    capability_weight = 1.0 - diversity_weight
                    final_score = usage_score * diversity_weight + capability_score * capability_weight
                    model_usage_scores[model_name] = {
                        'score': final_score,
                        'usage_score': usage_score,
                        'capability_score': capability_score,
                        'usage_count': usage_count
                    }
            
            if not model_usage_scores:
                return None
            
            # é€‰æ‹©ç»¼åˆå¾—åˆ†æœ€é«˜çš„æ¨¡å‹
            best_model_name = max(model_usage_scores.items(), key=lambda x: x[1]['score'])[0]
            best_model_spec = available_models[best_model_name]
            best_scores = model_usage_scores[best_model_name]
            
            # è®¡ç®—ç½®ä¿¡åº¦å’Œæˆæœ¬
            confidence_score = min(best_scores['score'], 0.9)  # å¤šæ ·åŒ–è·¯ç”±ç½®ä¿¡åº¦ç¨ä½
            estimated_cost = self._estimate_task_cost(best_model_spec, task_spec)
            estimated_time = self._estimate_execution_time(best_model_name, task_spec)
            
            # ç”Ÿæˆæ¨ç†è¯´æ˜
            reasoning = self._generate_diversity_reasoning(
                best_model_name, best_scores, model_usage_scores
            )
            
            # è·å–å¤‡é€‰æ¨¡å‹
            sorted_models = sorted(
                model_usage_scores.items(), 
                key=lambda x: x[1]['score'], 
                reverse=True
            )
            alternative_models = [model[0] for model in sorted_models[1:4]]
            
            decision = RoutingDecision(
                selected_model=best_model_name,
                provider=best_model_spec.provider.value,
                confidence_score=confidence_score,
                reasoning=reasoning,
                estimated_cost=estimated_cost,
                estimated_time=estimated_time,
                alternative_models=alternative_models,
                routing_strategy="diversity_forced"
            )
            
            # æ›´æ–°ä½¿ç”¨ç»Ÿè®¡
            self._update_model_usage(best_model_name)
            
            # è®°å½•è·¯ç”±å†³ç­–
            self._log_routing_decision(decision, agent_role, task_spec, context)
            
            logger.info(f"å¤šæ ·åŒ–è·¯ç”±å†³ç­–å®Œæˆ: {best_model_name} (ç»¼åˆå¾—åˆ†: {best_scores['score']:.3f}, ä½¿ç”¨æ¬¡æ•°: {best_scores['usage_count']})")
            return decision
            
        except Exception as e:
            logger.warning(f"å¤šæ ·åŒ–è·¯ç”±å¤±è´¥: {e}")
            return None
    
    def _generate_diversity_reasoning(self, selected_model: str, 
                                    selected_scores: Dict[str, float],
                                    all_scores: Dict[str, Dict]) -> str:
        """ç”Ÿæˆå¤šæ ·åŒ–è·¯ç”±æ¨ç†è¯´æ˜"""
        usage_count = selected_scores['usage_count']
        usage_score = selected_scores['usage_score']
        capability_score = selected_scores['capability_score']
        
        base_reason = f"å¤šæ ·åŒ–è·¯ç”±é€‰æ‹© {selected_model}"
        
        reasons = []
        reasons.append(f"ä½¿ç”¨æ¬¡æ•°è¾ƒå°‘({usage_count}æ¬¡)")
        reasons.append(f"å¤šæ ·åŒ–å¾—åˆ†{usage_score:.2f}")
        reasons.append(f"èƒ½åŠ›åŒ¹é…åº¦{capability_score:.2f}")
        
        # æ·»åŠ å¯¹æ¯”ä¿¡æ¯
        most_used_model = max(all_scores.items(), key=lambda x: x[1]['usage_count'])
        if most_used_model[0] != selected_model:
            reasons.append(f"é¿å…è¿‡åº¦ä½¿ç”¨{most_used_model[0]}({most_used_model[1]['usage_count']}æ¬¡)")
        
        return f"{base_reason}ï¼ŒåŸå› ï¼š{','.join(reasons)}"
    
    def _update_model_usage(self, model_name: str) -> None:
        """æ›´æ–°æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡"""
        self.model_usage_tracker[model_name] = self.model_usage_tracker.get(model_name, 0) + 1
        
        # å®šæœŸé‡ç½®ç»Ÿè®¡ï¼Œé¿å…ç´¯ç§¯æ•ˆåº”
        total_usage = sum(self.model_usage_tracker.values())
        if total_usage > 50:  # æ¯50æ¬¡ä½¿ç”¨åé‡ç½®
            # ä¿æŒç›¸å¯¹æ¯”ä¾‹ï¼Œä½†å‡å°‘ç»å¯¹æ•°é‡
            for model in self.model_usage_tracker:
                self.model_usage_tracker[model] = max(1, self.model_usage_tracker[model] // 2)
            logger.debug("é‡ç½®æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡ä»¥ä¿æŒå¤šæ ·åŒ–æ•ˆæœ")
    def _route_with_traditional_logic(self,
                                    task_description: str,
                                    agent_role: str,
                                    task_spec: TaskSpec,
                                    available_models: Dict[str, ModelSpec],
                                    context: Dict[str, Any]) -> RoutingDecision:
        """ä½¿ç”¨ä¼ ç»Ÿé€»è¾‘è¿›è¡Œè·¯ç”±ï¼ˆå‘åå…¼å®¹ï¼‰"""
        try:
            # 1. åˆ†æä»»åŠ¡ç‰¹æ€§
            task_analysis = self._analyze_task_characteristics(
                task_description, task_spec, context
            )
            
            # 2. è®¡ç®—æ¨¡å‹é€‚é…åº¦åˆ†æ•°
            model_scores = {}
            for model_name, model_spec in available_models.items():
                if model_name in self.model_capabilities:
                    score = self._calculate_model_score(
                        model_name, model_spec, task_analysis, task_spec
                    )
                    model_scores[model_name] = score
            
            # 3. é€‰æ‹©æœ€ä¼˜æ¨¡å‹
            if not model_scores:
                # å›é€€åˆ°é»˜è®¤é€‰æ‹©
                default_model = self._get_default_model(available_models, task_spec)
                return RoutingDecision(
                    selected_model=default_model.name,
                    provider=default_model.provider.value,
                    confidence_score=0.5,
                    reasoning="ä½¿ç”¨é»˜è®¤æ¨¡å‹é€‰æ‹©",
                    estimated_cost=0.01,
                    estimated_time=5000,
                    routing_strategy="traditional_fallback"
                )
            
            # æŒ‰åˆ†æ•°æ’åº
            sorted_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)
            best_model_name, best_score = sorted_models[0]
            best_model_spec = available_models[best_model_name]
            
            # 4. ç”Ÿæˆå†³ç­–æ¨ç†
            reasoning = self._generate_routing_reasoning(
                best_model_name, task_analysis, best_score, sorted_models[:3]
            )
            
            # 5. ä¼°ç®—æˆæœ¬å’Œæ—¶é—´
            estimated_cost = self._estimate_task_cost(best_model_spec, task_spec)
            estimated_time = self._estimate_execution_time(best_model_name, task_spec)
            
            # 6. è·å–å¤‡é€‰æ¨¡å‹
            alternative_models = [model[0] for model in sorted_models[1:4]]
            
            decision = RoutingDecision(
                selected_model=best_model_name,
                provider=best_model_spec.provider.value,
                confidence_score=min(best_score, 0.95),
                reasoning=f"ä¼ ç»Ÿè·¯ç”±: {reasoning}",
                estimated_cost=estimated_cost,
                estimated_time=estimated_time,
                alternative_models=alternative_models,
                routing_strategy="traditional"
            )
            
            # 7. è®°å½•è·¯ç”±å†³ç­–å’Œæ›´æ–°ä½¿ç”¨ç»Ÿè®¡
            self._log_routing_decision(decision, agent_role, task_spec, context)
            if self.force_diversity:
                self._update_model_usage(best_model_name)
            
            logger.info(f"ä¼ ç»Ÿè·¯ç”±å†³ç­–å®Œæˆ: {best_model_name} (ç½®ä¿¡åº¦: {best_score:.3f})")
            
            return decision
            
        except Exception as e:
            logger.error(f"ä¼ ç»Ÿè·¯ç”±å¤±è´¥: {e}", exc_info=True)
            # å›é€€åˆ°é»˜è®¤é€‰æ‹©
            default_model = self._get_default_model(available_models, task_spec)
            return RoutingDecision(
                selected_model=default_model.name,
                provider=default_model.provider.value,
                confidence_score=0.3,
                reasoning=f"ä¼ ç»Ÿè·¯ç”±å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹: {str(e)}",
                estimated_cost=0.01,
                estimated_time=5000,
                routing_strategy="fallback"
            )
    
    def _analyze_task_characteristics(self,
                                    task_description: str,
                                    task_spec: TaskSpec,
                                    context: Dict[str, Any]) -> Dict[str, float]:
        """åˆ†æä»»åŠ¡ç‰¹æ€§ï¼ˆé€‚é…ä¸‰æ± æ¶æ„ï¼‰"""
        characteristics = {
            'reasoning_required': 0.5,
            'speed_priority': 0.5,
            'chinese_content': 0.5,
            'cost_sensitivity': 0.5,
            'complexity_level': 0.5,
            'multimodal_required': 0.0,
            'long_context_required': 0.0,
            'code_generation_required': 0.0
        }
        
        # åŸºäºä»»åŠ¡ç±»å‹çš„åŸºç¡€ç‰¹æ€§
        task_type_characteristics = {
            'financial_report': {'reasoning_required': 0.9, 'multimodal_required': 0.8, 'complexity_level': 0.8},
            'technical_analysis': {'reasoning_required': 0.8, 'long_context_required': 0.7, 'chinese_content': 0.8},
            'news_analysis': {'speed_priority': 0.8, 'long_context_required': 0.6, 'chinese_content': 0.9},
            'sentiment_analysis': {'chinese_content': 0.9, 'speed_priority': 0.7},
            'risk_assessment': {'reasoning_required': 0.9, 'complexity_level': 0.8},
            'decision_making': {'reasoning_required': 0.95, 'multimodal_required': 0.7, 'complexity_level': 0.9},
            'code_generation': {'code_generation_required': 0.95, 'reasoning_required': 0.7},
            'tool_development': {'code_generation_required': 0.9, 'reasoning_required': 0.8},
            'policy_analysis': {'reasoning_required': 0.8, 'chinese_content': 0.95, 'multimodal_required': 0.6}
        }
        
        if task_spec.task_type in task_type_characteristics:
            task_chars = task_type_characteristics[task_spec.task_type]
            for key, value in task_chars.items():
                characteristics[key] = max(characteristics[key], value)
        
        # åŸºäºä»»åŠ¡å¤æ‚åº¦è°ƒæ•´
        if task_spec.complexity == TaskComplexity.HIGH:
            characteristics['reasoning_required'] *= 1.2
            characteristics['cost_sensitivity'] *= 0.8
            characteristics['complexity_level'] = 0.9
        elif task_spec.complexity == TaskComplexity.LOW:
            characteristics['speed_priority'] *= 1.2
            characteristics['cost_sensitivity'] *= 1.2
            characteristics['complexity_level'] = 0.3
        
        # åŸºäºä»»åŠ¡æè¿°çš„æ–‡æœ¬åˆ†æ
        description_lower = task_description.lower()
        
        # æ£€æµ‹ä¸­æ–‡å†…å®¹
        chinese_chars = sum(1 for char in task_description if '\u4e00' <= char <= '\u9fff')
        if chinese_chars > len(task_description) * 0.3:
            characteristics['chinese_content'] = min(characteristics['chinese_content'] * 1.3, 1.0)
        
        # æ£€æµ‹æ¨ç†éœ€æ±‚å…³é”®è¯
        reasoning_keywords = ['åˆ†æ', 'æ¨ç†', 'åˆ¤æ–­', 'è¯„ä¼°', 'å†³ç­–', 'analysis', 'reasoning', 'evaluate']
        if any(keyword in description_lower for keyword in reasoning_keywords):
            characteristics['reasoning_required'] = min(characteristics['reasoning_required'] * 1.2, 1.0)
        
        # æ£€æµ‹é€Ÿåº¦éœ€æ±‚å…³é”®è¯
        speed_keywords = ['å¿«é€Ÿ', 'æ€¥', 'ç«‹å³', 'quick', 'fast', 'urgent']
        if any(keyword in description_lower for keyword in speed_keywords):
            characteristics['speed_priority'] = min(characteristics['speed_priority'] * 1.3, 1.0)
        
        # æ£€æµ‹ä»£ç ç”Ÿæˆéœ€æ±‚
        code_keywords = ['ä»£ç ', 'ç¼–ç¨‹', 'è„šæœ¬', 'code', 'script', 'python', 'function']
        if any(keyword in description_lower for keyword in code_keywords):
            characteristics['code_generation_required'] = min(characteristics['code_generation_required'] * 1.5, 1.0)
        
        # æ£€æµ‹å¤šæ¨¡æ€éœ€æ±‚
        multimodal_keywords = ['å›¾ç‰‡', 'å›¾è¡¨', 'è¡¨æ ¼', 'pdf', 'image', 'chart', 'table']
        if any(keyword in description_lower for keyword in multimodal_keywords):
            characteristics['multimodal_required'] = min(characteristics['multimodal_required'] * 1.5, 1.0)
        
        # æ£€æµ‹é•¿ä¸Šä¸‹æ–‡éœ€æ±‚
        if task_spec.estimated_tokens > 20000:
            characteristics['long_context_required'] = 0.8
        elif task_spec.estimated_tokens > 10000:
            characteristics['long_context_required'] = 0.6
        
        # åŸºäºä¸Šä¸‹æ–‡è°ƒæ•´
        if context.get('time_limit'):
            characteristics['speed_priority'] = min(characteristics['speed_priority'] * 1.2, 1.0)
        
        if context.get('budget_limit'):
            characteristics['cost_sensitivity'] = min(characteristics['cost_sensitivity'] * 1.3, 1.0)
            
        if context.get('code_generation_required'):
            characteristics['code_generation_required'] = 0.9
            
        if context.get('multimodal_required'):
            characteristics['multimodal_required'] = 0.8
        
        return characteristics
    
    def _calculate_model_score(self,
                              model_name: str,
                              model_spec: ModelSpec,
                              task_characteristics: Dict[str, float],
                              task_spec: TaskSpec) -> float:
        """è®¡ç®—æ¨¡å‹é€‚é…åº¦åˆ†æ•°"""
        if model_name not in self.model_capabilities:
            return 0.0
        
        capabilities = self.model_capabilities[model_name]
        
        # è®¡ç®—å„ç»´åº¦å¾—åˆ†
        reasoning_score = capabilities.get('reasoning', 0) * task_characteristics['reasoning_required']
        speed_score = capabilities.get('speed', 0.5) * task_characteristics['speed_priority']
        chinese_score = capabilities.get('chinese', 0.5) * task_characteristics['chinese_content']
        cost_score = capabilities.get('cost', 0.5) * task_characteristics['cost_sensitivity']
        reliability_score = capabilities.get('reliability', 0.8) * 0.8  # ç¨³å®šæ€§åŸºç¡€æƒé‡
        
        # ä¸“ä¸šèƒ½åŠ›åŠ åˆ†
        code_score = capabilities.get('code_generation', 0) * task_characteristics.get('code_generation_required', 0)
        multimodal_score = capabilities.get('multimodal', 0) * task_characteristics.get('multimodal_required', 0)
        financial_score = capabilities.get('financial_analysis', 0) * (1.0 if task_spec.task_type in ['financial_report', 'fundamental_analysis'] else 0)
        
        # åŠ æƒå¹³å‡ï¼ˆé€‚é…æ–°çš„æƒé‡é…ç½®ï¼‰
        total_score = (
            (reasoning_score + code_score + multimodal_score + financial_score) * self.routing_weights.get('quality', 0.6) +
            (speed_score + reliability_score) * self.routing_weights.get('performance', 0.3) +
            chinese_score * self.routing_weights.get('performance', 0.3) * 0.5 +  # ä¸­æ–‡èƒ½åŠ›ä½œä¸ºæ€§èƒ½çš„ä¸€éƒ¨åˆ†
            cost_score * self.routing_weights.get('cost', 0.1)
        )
        
        # æ ¹æ®å†å²æ€§èƒ½è°ƒæ•´
        historical_performance = self._get_historical_performance(model_name, task_spec.task_type)
        if historical_performance:
            performance_factor = (
                historical_performance['success_rate'] * 0.6 +
                min(historical_performance['avg_response_time'] / 10000, 1.0) * 0.4
            )
            total_score *= performance_factor
        
        # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
        return max(0.0, min(total_score, 1.0))
    
    def _get_historical_performance(self, model_name: str, task_type: str) -> Optional[Dict[str, float]]:
        """è·å–æ¨¡å‹å†å²æ€§èƒ½æ•°æ®"""
        try:
            with sqlite3.connect(self.database_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    SELECT avg_response_time, success_rate, last_updated
                    FROM model_performance
                    WHERE model_name = ? AND task_type = ?
                    ORDER BY last_updated DESC
                    LIMIT 1
                """, (model_name, task_type))
                
                result = cursor.fetchone()
                if result:
                    return {
                        'avg_response_time': result[0],
                        'success_rate': result[1],
                        'last_updated': result[2]
                    }
        except Exception as e:
            logger.warning(f"è·å–å†å²æ€§èƒ½æ•°æ®å¤±è´¥: {e}")
        
        return None
    
    def _generate_routing_reasoning(self,
                                  selected_model: str,
                                  task_characteristics: Dict[str, float],
                                  score: float,
                                  top_models: List[Tuple[str, float]]) -> str:
        """ç”Ÿæˆè·¯ç”±å†³ç­–æ¨ç†è¯´æ˜"""
        reasoning_parts = []
        
        # é€‰æ‹©åŸå› 
        model_caps = self.model_capabilities.get(selected_model, {})
        
        if task_characteristics['reasoning_required'] > 0.7 and model_caps.get('reasoning', 0) > 0.8:
            reasoning_parts.append(f"{selected_model}å…·æœ‰å¼ºå¤§çš„æ¨ç†èƒ½åŠ›")
        
        if task_characteristics['speed_priority'] > 0.7 and model_caps.get('speed', 0) > 0.8:
            reasoning_parts.append("ä¼˜å…ˆè€ƒè™‘å“åº”é€Ÿåº¦")
        
        if task_characteristics['chinese_content'] > 0.7 and model_caps.get('chinese', 0) > 0.8:
            reasoning_parts.append("é’ˆå¯¹ä¸­æ–‡å†…å®¹ä¼˜åŒ–")
        
        if task_characteristics['cost_sensitivity'] > 0.7 and model_caps.get('cost', 0) > 0.8:
            reasoning_parts.append("å…·å¤‡è‰¯å¥½çš„æˆæœ¬æ•ˆç›Š")
        
        # æ„å»ºæ¨ç†è¯´æ˜
        base_reasoning = f"é€‰æ‹©{selected_model}ï¼ˆç½®ä¿¡åº¦: {score:.2f}ï¼‰"
        
        if reasoning_parts:
            reasons = "ï¼Œ".join(reasoning_parts)
            base_reasoning += f"ï¼Œä¸»è¦åŸå› : {reasons}"
        
        # æ·»åŠ å¤‡é€‰æ–¹æ¡ˆ
        if len(top_models) > 1:
            alternatives = [f"{model}({score:.2f})" for model, score in top_models[1:]]
            base_reasoning += f"ã€‚å¤‡é€‰æ–¹æ¡ˆ: {', '.join(alternatives)}"
        
        return base_reasoning
    
    def _estimate_task_cost(self, model_spec: ModelSpec, task_spec: TaskSpec) -> float:
        """ä¼°ç®—ä»»åŠ¡æˆæœ¬"""
        estimated_tokens = max(task_spec.estimated_tokens, 1000)  # æœ€å°‘1000 tokens
        return (estimated_tokens / 1000) * model_spec.cost_per_1k_tokens
    
    def _estimate_execution_time(self, model_name: str, task_spec: TaskSpec) -> int:
        """ä¼°ç®—æ‰§è¡Œæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰"""
        base_time = 3000  # åŸºç¡€3ç§’
        
        # æ ¹æ®æ¨¡å‹é€Ÿåº¦ç‰¹æ€§è°ƒæ•´
        if model_name in self.model_capabilities:
            speed_factor = self.model_capabilities[model_name].get('speed', 0.5)
            base_time = int(base_time * (1.5 - speed_factor))
        
        # æ ¹æ®ä»»åŠ¡å¤æ‚åº¦è°ƒæ•´
        if task_spec.complexity == TaskComplexity.HIGH:
            base_time *= 2
        elif task_spec.complexity == TaskComplexity.LOW:
            base_time = int(base_time * 0.6)
        
        # æ ¹æ®ä¼°ç®—tokenæ•°è°ƒæ•´
        if task_spec.estimated_tokens > 4000:
            token_factor = min(task_spec.estimated_tokens / 4000, 3.0)
            base_time = int(base_time * token_factor)
        
        return max(base_time, 1000)  # æœ€å°‘1ç§’
    
    def _initialize_database_tables(self) -> None:
        """åˆå§‹åŒ–å¿…è¦çš„æ•°æ®åº“è¡¨"""
        try:
            with sqlite3.connect(self.database_path) as conn:
                cursor = conn.cursor()
                
                # åˆ›å»ºæ¨¡å‹æ€§èƒ½è¡¨
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS model_performance (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        model_name TEXT NOT NULL,
                        provider TEXT NOT NULL,
                        task_type TEXT NOT NULL,
                        avg_response_time REAL NOT NULL,
                        success_rate REAL NOT NULL,
                        cost_efficiency REAL DEFAULT 1.0,
                        quality_score REAL DEFAULT 1.0,
                        last_updated TEXT NOT NULL,
                        created_at TEXT DEFAULT (datetime('now')),
                        UNIQUE(model_name, provider, task_type)
                    )
                """)
                
                # åˆ›å»ºè·¯ç”±å†³ç­–è¡¨
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS routing_decisions (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        session_id TEXT NOT NULL,
                        agent_role TEXT NOT NULL,
                        task_type TEXT NOT NULL,
                        selected_model TEXT NOT NULL,
                        selected_provider TEXT NOT NULL,
                        routing_reason TEXT,
                        confidence_score REAL,
                        execution_time INTEGER,
                        cost_estimate REAL,
                        created_at TEXT DEFAULT (datetime('now'))
                    )
                """)
                
                conn.commit()
                logger.debug("æ•°æ®åº“è¡¨ç»“æ„åˆå§‹åŒ–å®Œæˆ")
                
        except Exception as e:
            logger.warning(f"æ•°æ®åº“è¡¨åˆå§‹åŒ–å¤±è´¥ï¼ˆå°†ä½¿ç”¨å†…å­˜æ¨¡å¼ï¼‰: {e}")
    
    def _validate_engine_usability(self) -> Dict[str, Any]:
        """éªŒè¯è·¯ç”±å¼•æ“çš„å¯ç”¨æ€§"""
        issues = []
        
        # æ£€æŸ¥æ¨¡å‹èƒ½åŠ›é…ç½®
        if not self.model_capabilities:
            issues.append("ç¼ºå°‘æ¨¡å‹èƒ½åŠ›é…ç½®")
        
        # æ£€æŸ¥ä»»åŠ¡-æ± äº²å’Œåº¦é…ç½®
        if not self.task_pool_affinity:
            issues.append("ç¼ºå°‘ä»»åŠ¡-æ± äº²å’Œåº¦é…ç½®")
        
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        try:
            with sqlite3.connect(self.database_path) as conn:
                conn.execute("SELECT 1").fetchone()
        except Exception as e:
            issues.append(f"æ•°æ®åº“è¿æ¥é—®é¢˜: {str(e)[:50]}")
        
        # æ£€æŸ¥æƒé‡é…ç½®
        total_weight = sum(self.routing_weights.values())
        if abs(total_weight - 1.0) > 0.01:
            issues.append(f"è·¯ç”±æƒé‡é…ç½®å¼‚å¸¸ (æ€»å’Œ: {total_weight:.3f})")
        
        if issues:
            return {
                'usable': False,
                'message': f"å‘ç°{len(issues)}ä¸ªé—®é¢˜: {'; '.join(issues[:2])}",
                'issues': issues
            }
        else:
            model_count = len(self.model_capabilities)
            task_count = len(self.task_pool_affinity)
            return {
                'usable': True,
                'message': f"æ”¯æŒ{model_count}ä¸ªæ¨¡å‹ï¼Œ{task_count}ç§ä»»åŠ¡ç±»å‹",
                'issues': []
            }
    
    def _get_default_model(self, available_models: Dict[str, ModelSpec], task_spec: TaskSpec) -> ModelSpec:
        """è·å–é»˜è®¤æ¨¡å‹"""
        # ä¼˜å…ˆé€‰æ‹©æˆæœ¬æ•ˆç›Šè¾ƒå¥½çš„æ¨¡å‹ - åªä¿ç•™å¯ç”¨çš„æ¨¡å‹
        cost_efficient_models = ['gemini-2.5-pro', 'deepseek-ai/DeepSeek-V3', 'gemini-2.0-flash', 'deepseek-chat', 'moonshotai/Kimi-K2-Instruct']
        
        for model_name in cost_efficient_models:
            if model_name in available_models:
                return available_models[model_name]
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
        if available_models:
            return next(iter(available_models.values()))
        
        # è¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä½œä¸ºä¿é™©
        return ModelSpec(
            name="fallback",
            provider=ModelProvider.SILICONFLOW,
            model_type="general",
            cost_per_1k_tokens=0.001,
            max_tokens=2048,
            context_window=4096
        )
    
    def _log_routing_decision(self,
                            decision: RoutingDecision,
                            agent_role: str,
                            task_spec: TaskSpec,
                            context: Dict[str, Any]) -> None:
        """è®°å½•è·¯ç”±å†³ç­–åˆ°æ•°æ®åº“"""
        try:
            with sqlite3.connect(self.database_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT INTO routing_decisions 
                    (session_id, agent_role, task_type, selected_model, selected_provider,
                     routing_reason, confidence_score, execution_time, cost_estimate)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    context.get('session_id', 'unknown'),
                    agent_role,
                    task_spec.task_type,
                    decision.selected_model,
                    decision.provider,
                    decision.reasoning,
                    decision.confidence_score,
                    decision.estimated_time,
                    decision.estimated_cost
                ))
                conn.commit()
        except Exception as e:
            logger.warning(f"è·¯ç”±å†³ç­–è®°å½•å¤±è´¥: {e}")
    
    def reset_diversity_tracker(self) -> None:
        """é‡ç½®å¤šæ ·åŒ–è·Ÿè¸ªå™¨"""
        self.model_usage_tracker.clear()
        logger.info("å¤šæ ·åŒ–è·Ÿè¸ªå™¨å·²é‡ç½®")
    
    def _get_default_db_path(self) -> str:
        """è·å–é»˜è®¤æ•°æ®åº“è·¯å¾„"""
        # å°è¯•ä½¿ç”¨é¡¹ç›®æ•°æ®åº“è·¯å¾„
        project_root = Path(__file__).parent.parent.parent
        db_path = project_root / "data" / "trading_agents.db"
        
        if db_path.parent.exists():
            return str(db_path)
        else:
            # å›é€€åˆ°ä¸´æ—¶æ•°æ®åº“
            return ":memory:"
    
    def update_model_performance(self,
                               model_name: str,
                               provider: str,
                               task_type: str,
                               execution_time: int,
                               success: bool,
                               cost: float = None) -> None:
        """æ›´æ–°æ¨¡å‹æ€§èƒ½æ•°æ®"""
        try:
            with sqlite3.connect(self.database_path) as conn:
                cursor = conn.cursor()
                
                # è·å–ç°æœ‰æ•°æ®
                cursor.execute("""
                    SELECT avg_response_time, success_rate, last_updated
                    FROM model_performance
                    WHERE model_name = ? AND provider = ? AND task_type = ?
                """, (model_name, provider, task_type))
                
                existing = cursor.fetchone()
                
                if existing:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    old_time, old_success_rate, _ = existing
                    # ç®€å•çš„ç§»åŠ¨å¹³å‡
                    new_avg_time = (old_time + execution_time) / 2
                    new_success_rate = (old_success_rate + (1.0 if success else 0.0)) / 2
                    
                    cursor.execute("""
                        UPDATE model_performance 
                        SET avg_response_time = ?, success_rate = ?, last_updated = ?
                        WHERE model_name = ? AND provider = ? AND task_type = ?
                    """, (new_avg_time, new_success_rate, datetime.now().isoformat(),
                          model_name, provider, task_type))
                else:
                    # æ’å…¥æ–°è®°å½•
                    cursor.execute("""
                        INSERT INTO model_performance 
                        (model_name, provider, task_type, avg_response_time, success_rate, last_updated)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (model_name, provider, task_type, execution_time,
                          1.0 if success else 0.0, datetime.now().isoformat()))
                
                conn.commit()
                
        except Exception as e:
            logger.warning(f"æ€§èƒ½æ•°æ®æ›´æ–°å¤±è´¥: {e}")
    
    def get_routing_statistics(self, time_window_hours: int = 24) -> Dict[str, Any]:
        """è·å–è·¯ç”±ç»Ÿè®¡ä¿¡æ¯"""
        try:
            with sqlite3.connect(self.database_path) as conn:
                cursor = conn.cursor()
                
                since_time = datetime.now() - timedelta(hours=time_window_hours)
                
                # æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡
                cursor.execute("""
                    SELECT selected_model, COUNT(*) as usage_count,
                           AVG(confidence_score) as avg_confidence,
                           AVG(cost_estimate) as avg_cost
                    FROM routing_decisions 
                    WHERE created_at > ?
                    GROUP BY selected_model
                    ORDER BY usage_count DESC
                """, (since_time.isoformat(),))
                
                model_stats = cursor.fetchall()
                
                return {
                    'time_window_hours': time_window_hours,
                    'total_decisions': sum(stat[1] for stat in model_stats),
                    'model_usage': [
                        {
                            'model': stat[0],
                            'usage_count': stat[1],
                            'avg_confidence': round(stat[2], 3),
                            'avg_cost': round(stat[3], 6)
                        } for stat in model_stats
                    ]
                }
        except Exception as e:
            logger.error(f"è·å–è·¯ç”±ç»Ÿè®¡å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def get_diversity_statistics(self) -> Dict[str, Any]:
        """è·å–å¤šæ ·åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.model_usage_tracker:
            return {
                'diversity_enabled': self.force_diversity,
                'total_usage': 0,
                'model_usage': {},
                'diversity_score': 1.0
            }
        
        total_usage = sum(self.model_usage_tracker.values())
        model_usage = dict(sorted(self.model_usage_tracker.items(), key=lambda x: x[1], reverse=True))
        
        # è®¡ç®—å¤šæ ·åŒ–å¾—åˆ†ï¼ˆåŸºå°¼ç³»æ•°çš„ç®€åŒ–ç‰ˆæœ¬ï¼‰
        diversity_score = 1.0
        if total_usage > 0:
            usage_rates = [count / total_usage for count in self.model_usage_tracker.values()]
            # è®¡ç®—åˆ†å¸ƒçš„å‡åŒ€ç¨‹åº¦
            n = len(usage_rates)
            if n > 1:
                max_rate = max(usage_rates)
                diversity_score = 1.0 - (max_rate - 1/n) / (1.0 - 1/n)
        
        return {
            'diversity_enabled': self.force_diversity,
            'diversity_threshold': self.diversity_threshold,
            'total_usage': total_usage,
            'model_usage': model_usage,
            'diversity_score': round(diversity_score, 3),
            'needs_diversification': self._should_diversify_routing('', {}) if self.model_usage_tracker else False
        }