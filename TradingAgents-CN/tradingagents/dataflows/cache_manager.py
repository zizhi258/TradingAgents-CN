#!/usr/bin/env python3
"""
è‚¡ç¥¨æ•°æ®ç¼“å­˜ç®¡ç†å™¨
æ”¯æŒæœ¬åœ°ç¼“å­˜è‚¡ç¥¨æ•°æ®ï¼Œå‡å°‘APIè°ƒç”¨ï¼Œæé«˜å“åº”é€Ÿåº¦
"""

import os
import json
import pickle
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, Any, Union
import hashlib

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('agents')


class StockDataCache:
    """è‚¡ç¥¨æ•°æ®ç¼“å­˜ç®¡ç†å™¨ - æ”¯æŒç¾è‚¡å’ŒAè‚¡æ•°æ®ç¼“å­˜ä¼˜åŒ–"""

    def __init__(self, cache_dir: str = None):
        """
        åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨

        Args:
            cache_dir: ç¼“å­˜ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸º tradingagents/dataflows/data_cache
        """
        if cache_dir is None:
            # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
            current_dir = Path(__file__).parent
            cache_dir = current_dir / "data_cache"

        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

        # åˆ›å»ºå­ç›®å½• - æŒ‰å¸‚åœºåˆ†ç±»
        self.us_stock_dir = self.cache_dir / "us_stocks"
        self.china_stock_dir = self.cache_dir / "china_stocks"
        self.us_news_dir = self.cache_dir / "us_news"
        self.china_news_dir = self.cache_dir / "china_news"
        self.us_fundamentals_dir = self.cache_dir / "us_fundamentals"
        self.china_fundamentals_dir = self.cache_dir / "china_fundamentals"
        self.metadata_dir = self.cache_dir / "metadata"

        # åˆ›å»ºæ‰€æœ‰ç›®å½•
        for dir_path in [self.us_stock_dir, self.china_stock_dir, self.us_news_dir,
                        self.china_news_dir, self.us_fundamentals_dir,
                        self.china_fundamentals_dir, self.metadata_dir]:
            dir_path.mkdir(exist_ok=True)

        # ç¼“å­˜é…ç½® - é’ˆå¯¹ä¸åŒå¸‚åœºè®¾ç½®ä¸åŒçš„TTL
        self.cache_config = {
            'us_stock_data': {
                'ttl_hours': 2,  # ç¾è‚¡æ•°æ®ç¼“å­˜2å°æ—¶ï¼ˆè€ƒè™‘åˆ°APIé™åˆ¶ï¼‰
                'max_files': 1000,
                'description': 'ç¾è‚¡å†å²æ•°æ®'
            },
            'china_stock_data': {
                'ttl_hours': 1,  # Aè‚¡æ•°æ®ç¼“å­˜1å°æ—¶ï¼ˆå®æ—¶æ€§è¦æ±‚é«˜ï¼‰
                'max_files': 1000,
                'description': 'Aè‚¡å†å²æ•°æ®'
            },
            'us_news': {
                'ttl_hours': 6,  # ç¾è‚¡æ–°é—»ç¼“å­˜6å°æ—¶
                'max_files': 500,
                'description': 'ç¾è‚¡æ–°é—»æ•°æ®'
            },
            'china_news': {
                'ttl_hours': 4,  # Aè‚¡æ–°é—»ç¼“å­˜4å°æ—¶
                'max_files': 500,
                'description': 'Aè‚¡æ–°é—»æ•°æ®'
            },
            'us_fundamentals': {
                'ttl_hours': 24,  # ç¾è‚¡åŸºæœ¬é¢æ•°æ®ç¼“å­˜24å°æ—¶
                'max_files': 200,
                'description': 'ç¾è‚¡åŸºæœ¬é¢æ•°æ®'
            },
            'china_fundamentals': {
                'ttl_hours': 12,  # Aè‚¡åŸºæœ¬é¢æ•°æ®ç¼“å­˜12å°æ—¶
                'max_files': 200,
                'description': 'Aè‚¡åŸºæœ¬é¢æ•°æ®'
            }
        }

        logger.info(f"ğŸ“ ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œç¼“å­˜ç›®å½•: {self.cache_dir}")
        logger.info(f"ğŸ—„ï¸ æ•°æ®åº“ç¼“å­˜ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   ç¾è‚¡æ•°æ®: âœ… å·²é…ç½®")
        logger.info(f"   Aè‚¡æ•°æ®: âœ… å·²é…ç½®")

    def _determine_market_type(self, symbol: str) -> str:
        """æ ¹æ®è‚¡ç¥¨ä»£ç ç¡®å®šå¸‚åœºç±»å‹"""
        import re


        # åˆ¤æ–­æ˜¯å¦ä¸ºä¸­å›½Aè‚¡ï¼ˆ6ä½æ•°å­—ï¼‰
        if re.match(r'^\d{6}$', str(symbol)):
            return 'china'
        else:
            return 'us'
    
    def _generate_cache_key(self, data_type: str, symbol: str, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰å‚æ•°çš„å­—ç¬¦ä¸²
        params_str = f"{data_type}_{symbol}"
        for key, value in sorted(kwargs.items()):
            params_str += f"_{key}_{value}"
        
        # ä½¿ç”¨MD5ç”ŸæˆçŸ­çš„å”¯ä¸€æ ‡è¯†
        cache_key = hashlib.md5(params_str.encode()).hexdigest()[:12]
        return f"{symbol}_{data_type}_{cache_key}"
    
    def _get_cache_path(self, data_type: str, cache_key: str, file_format: str = "json", symbol: str = None) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„ - æ”¯æŒå¸‚åœºåˆ†ç±»"""
        if symbol:
            market_type = self._determine_market_type(symbol)
        else:
            # ä»ç¼“å­˜é”®ä¸­å°è¯•æå–å¸‚åœºç±»å‹
            market_type = 'us' if not cache_key.startswith(('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')) else 'china'

        # æ ¹æ®æ•°æ®ç±»å‹å’Œå¸‚åœºç±»å‹é€‰æ‹©ç›®å½•
        if data_type == "stock_data":
            base_dir = self.china_stock_dir if market_type == 'china' else self.us_stock_dir
        elif data_type == "news":
            base_dir = self.china_news_dir if market_type == 'china' else self.us_news_dir
        elif data_type == "fundamentals":
            base_dir = self.china_fundamentals_dir if market_type == 'china' else self.us_fundamentals_dir
        else:
            base_dir = self.cache_dir

        return base_dir / f"{cache_key}.{file_format}"
    
    def _get_metadata_path(self, cache_key: str) -> Path:
        """è·å–å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„"""
        return self.metadata_dir / f"{cache_key}_meta.json"
    
    def _save_metadata(self, cache_key: str, metadata: Dict[str, Any]):
        """ä¿å­˜å…ƒæ•°æ®"""
        metadata_path = self._get_metadata_path(cache_key)
        metadata['cached_at'] = datetime.now().isoformat()
        
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    def _load_metadata(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """åŠ è½½å…ƒæ•°æ®"""
        metadata_path = self._get_metadata_path(cache_key)
        if not metadata_path.exists():
            return None
        
        try:
            with open(metadata_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"âš ï¸ åŠ è½½å…ƒæ•°æ®å¤±è´¥: {e}")
            return None
    
    def is_cache_valid(self, cache_key: str, max_age_hours: int = None, symbol: str = None, data_type: str = None) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ - æ”¯æŒæ™ºèƒ½TTLé…ç½®"""
        metadata = self._load_metadata(cache_key)
        if not metadata:
            return False

        # å¦‚æœæ²¡æœ‰æŒ‡å®šTTLï¼Œæ ¹æ®æ•°æ®ç±»å‹å’Œå¸‚åœºè‡ªåŠ¨ç¡®å®š
        if max_age_hours is None:
            if symbol and data_type:
                market_type = self._determine_market_type(symbol)
                cache_type = f"{market_type}_{data_type}"
                max_age_hours = self.cache_config.get(cache_type, {}).get('ttl_hours', 24)
            else:
                # ä»å…ƒæ•°æ®ä¸­è·å–ä¿¡æ¯
                symbol = metadata.get('symbol', '')
                data_type = metadata.get('data_type', 'stock_data')
                market_type = self._determine_market_type(symbol)
                cache_type = f"{market_type}_{data_type}"
                max_age_hours = self.cache_config.get(cache_type, {}).get('ttl_hours', 24)

        cached_at = datetime.fromisoformat(metadata['cached_at'])
        age = datetime.now() - cached_at

        is_valid = age.total_seconds() < max_age_hours * 3600

        if is_valid:
            market_type = self._determine_market_type(metadata.get('symbol', ''))
            cache_type = f"{market_type}_{metadata.get('data_type', 'stock_data')}"
            desc = self.cache_config.get(cache_type, {}).get('description', 'æ•°æ®')
            logger.info(f"âœ… ç¼“å­˜æœ‰æ•ˆ: {desc} - {metadata.get('symbol')} (å‰©ä½™ {max_age_hours - age.total_seconds()/3600:.1f}h)")

        return is_valid
    
    def save_stock_data(self, symbol: str, data: Union[pd.DataFrame, str],
                       start_date: str = None, end_date: str = None,
                       data_source: str = "unknown") -> str:
        """
        ä¿å­˜è‚¡ç¥¨æ•°æ®åˆ°ç¼“å­˜ - æ”¯æŒç¾è‚¡å’ŒAè‚¡åˆ†ç±»å­˜å‚¨

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            data: è‚¡ç¥¨æ•°æ®ï¼ˆDataFrameæˆ–å­—ç¬¦ä¸²ï¼‰
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            data_source: æ•°æ®æºï¼ˆå¦‚ "tdx", "yfinance", "finnhub"ï¼‰

        Returns:
            cache_key: ç¼“å­˜é”®
        """
        market_type = self._determine_market_type(symbol)
        cache_key = self._generate_cache_key("stock_data", symbol,
                                           start_date=start_date,
                                           end_date=end_date,
                                           source=data_source,
                                           market=market_type)

        # ä¿å­˜æ•°æ®
        if isinstance(data, pd.DataFrame):
            cache_path = self._get_cache_path("stock_data", cache_key, "csv", symbol)
            data.to_csv(cache_path, index=True)
        else:
            cache_path = self._get_cache_path("stock_data", cache_key, "txt", symbol)
            with open(cache_path, 'w', encoding='utf-8') as f:
                f.write(str(data))

        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'symbol': symbol,
            'data_type': 'stock_data',
            'market_type': market_type,
            'start_date': start_date,
            'end_date': end_date,
            'data_source': data_source,
            'file_path': str(cache_path),
            'file_format': 'csv' if isinstance(data, pd.DataFrame) else 'txt'
        }
        self._save_metadata(cache_key, metadata)

        # è·å–æè¿°ä¿¡æ¯
        cache_type = f"{market_type}_stock_data"
        desc = self.cache_config.get(cache_type, {}).get('description', 'è‚¡ç¥¨æ•°æ®')
        logger.info(f"ğŸ’¾ {desc}å·²ç¼“å­˜: {symbol} ({data_source}) -> {cache_key}")
        return cache_key
    
    def load_stock_data(self, cache_key: str) -> Optional[Union[pd.DataFrame, str]]:
        """ä»ç¼“å­˜åŠ è½½è‚¡ç¥¨æ•°æ®"""
        metadata = self._load_metadata(cache_key)
        if not metadata:
            return None
        
        cache_path = Path(metadata['file_path'])
        if not cache_path.exists():
            return None
        
        try:
            if metadata['file_format'] == 'csv':
                return pd.read_csv(cache_path, index_col=0)
            else:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    return f.read()
        except Exception as e:
            logger.error(f"âš ï¸ åŠ è½½ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
            return None
    
    def find_cached_stock_data(self, symbol: str, start_date: str = None,
                              end_date: str = None, data_source: str = None,
                              max_age_hours: int = None) -> Optional[str]:
        """
        æŸ¥æ‰¾åŒ¹é…çš„ç¼“å­˜æ•°æ® - æ”¯æŒæ™ºèƒ½å¸‚åœºåˆ†ç±»æŸ¥æ‰¾

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            data_source: æ•°æ®æº
            max_age_hours: æœ€å¤§ç¼“å­˜æ—¶é—´ï¼ˆå°æ—¶ï¼‰ï¼ŒNoneæ—¶ä½¿ç”¨æ™ºèƒ½é…ç½®

        Returns:
            cache_key: å¦‚æœæ‰¾åˆ°æœ‰æ•ˆç¼“å­˜åˆ™è¿”å›ç¼“å­˜é”®ï¼Œå¦åˆ™è¿”å›None
        """
        market_type = self._determine_market_type(symbol)

        # å¦‚æœæ²¡æœ‰æŒ‡å®šTTLï¼Œä½¿ç”¨æ™ºèƒ½é…ç½®
        if max_age_hours is None:
            cache_type = f"{market_type}_stock_data"
            max_age_hours = self.cache_config.get(cache_type, {}).get('ttl_hours', 24)

        # ç”ŸæˆæŸ¥æ‰¾é”®
        search_key = self._generate_cache_key("stock_data", symbol,
                                            start_date=start_date,
                                            end_date=end_date,
                                            source=data_source,
                                            market=market_type)

        # æ£€æŸ¥ç²¾ç¡®åŒ¹é…
        if self.is_cache_valid(search_key, max_age_hours, symbol, 'stock_data'):
            desc = self.cache_config.get(f"{market_type}_stock_data", {}).get('description', 'æ•°æ®')
            logger.info(f"ğŸ¯ æ‰¾åˆ°ç²¾ç¡®åŒ¹é…çš„{desc}: {symbol} -> {search_key}")
            return search_key

        # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼ŒæŸ¥æ‰¾éƒ¨åˆ†åŒ¹é…ï¼ˆç›¸åŒè‚¡ç¥¨ä»£ç çš„å…¶ä»–ç¼“å­˜ï¼‰
        for metadata_file in self.metadata_dir.glob(f"*_meta.json"):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)

                if (metadata.get('symbol') == symbol and
                    metadata.get('data_type') == 'stock_data' and
                    metadata.get('market_type') == market_type and
                    (data_source is None or metadata.get('data_source') == data_source)):

                    cache_key = metadata_file.stem.replace('_meta', '')
                    if self.is_cache_valid(cache_key, max_age_hours, symbol, 'stock_data'):
                        desc = self.cache_config.get(f"{market_type}_stock_data", {}).get('description', 'æ•°æ®')
                        logger.info(f"ğŸ“‹ æ‰¾åˆ°éƒ¨åˆ†åŒ¹é…çš„{desc}: {symbol} -> {cache_key}")
                        return cache_key
            except Exception:
                continue

        desc = self.cache_config.get(f"{market_type}_stock_data", {}).get('description', 'æ•°æ®')
        logger.error(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„{desc}ç¼“å­˜: {symbol}")
        return None
    
    def save_news_data(self, symbol: str, news_data: str, 
                      start_date: str = None, end_date: str = None,
                      data_source: str = "unknown") -> str:
        """ä¿å­˜æ–°é—»æ•°æ®åˆ°ç¼“å­˜"""
        cache_key = self._generate_cache_key("news", symbol,
                                           start_date=start_date,
                                           end_date=end_date,
                                           source=data_source)
        
        cache_path = self._get_cache_path("news", cache_key, "txt")
        with open(cache_path, 'w', encoding='utf-8') as f:
            f.write(news_data)
        
        metadata = {
            'symbol': symbol,
            'data_type': 'news',
            'start_date': start_date,
            'end_date': end_date,
            'data_source': data_source,
            'file_path': str(cache_path),
            'file_format': 'txt'
        }
        self._save_metadata(cache_key, metadata)
        
        logger.info(f"ğŸ“° æ–°é—»æ•°æ®å·²ç¼“å­˜: {symbol} ({data_source}) -> {cache_key}")
        return cache_key
    
    def save_fundamentals_data(self, symbol: str, fundamentals_data: str,
                              data_source: str = "unknown") -> str:
        """ä¿å­˜åŸºæœ¬é¢æ•°æ®åˆ°ç¼“å­˜"""
        market_type = self._determine_market_type(symbol)
        cache_key = self._generate_cache_key("fundamentals", symbol,
                                           source=data_source,
                                           market=market_type,
                                           date=datetime.now().strftime("%Y-%m-%d"))
        
        cache_path = self._get_cache_path("fundamentals", cache_key, "txt", symbol)
        with open(cache_path, 'w', encoding='utf-8') as f:
            f.write(fundamentals_data)
        
        metadata = {
            'symbol': symbol,
            'data_type': 'fundamentals',
            'data_source': data_source,
            'market_type': market_type,
            'file_path': str(cache_path),
            'file_format': 'txt'
        }
        self._save_metadata(cache_key, metadata)
        
        desc = self.cache_config.get(f"{market_type}_fundamentals", {}).get('description', 'åŸºæœ¬é¢æ•°æ®')
        logger.info(f"ğŸ’¼ {desc}å·²ç¼“å­˜: {symbol} ({data_source}) -> {cache_key}")
        return cache_key
    
    def load_fundamentals_data(self, cache_key: str) -> Optional[str]:
        """ä»ç¼“å­˜åŠ è½½åŸºæœ¬é¢æ•°æ®"""
        metadata = self._load_metadata(cache_key)
        if not metadata:
            return None
        
        cache_path = Path(metadata['file_path'])
        if not cache_path.exists():
            return None
        
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            logger.error(f"âš ï¸ åŠ è½½åŸºæœ¬é¢ç¼“å­˜æ•°æ®å¤±è´¥: {e}")
            return None
    
    def find_cached_fundamentals_data(self, symbol: str, data_source: str = None,
                                    max_age_hours: int = None) -> Optional[str]:
        """
        æŸ¥æ‰¾åŒ¹é…çš„åŸºæœ¬é¢ç¼“å­˜æ•°æ®
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            data_source: æ•°æ®æºï¼ˆå¦‚ "openai", "finnhub"ï¼‰
            max_age_hours: æœ€å¤§ç¼“å­˜æ—¶é—´ï¼ˆå°æ—¶ï¼‰ï¼ŒNoneæ—¶ä½¿ç”¨æ™ºèƒ½é…ç½®
        
        Returns:
            cache_key: å¦‚æœæ‰¾åˆ°æœ‰æ•ˆç¼“å­˜åˆ™è¿”å›ç¼“å­˜é”®ï¼Œå¦åˆ™è¿”å›None
        """
        market_type = self._determine_market_type(symbol)
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šTTLï¼Œä½¿ç”¨æ™ºèƒ½é…ç½®
        if max_age_hours is None:
            cache_type = f"{market_type}_fundamentals"
            max_age_hours = self.cache_config.get(cache_type, {}).get('ttl_hours', 24)
        
        # æŸ¥æ‰¾åŒ¹é…çš„ç¼“å­˜
        for metadata_file in self.metadata_dir.glob(f"*_meta.json"):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                if (metadata.get('symbol') == symbol and
                    metadata.get('data_type') == 'fundamentals' and
                    metadata.get('market_type') == market_type and
                    (data_source is None or metadata.get('data_source') == data_source)):
                    
                    cache_key = metadata_file.stem.replace('_meta', '')
                    if self.is_cache_valid(cache_key, max_age_hours, symbol, 'fundamentals'):
                        desc = self.cache_config.get(f"{market_type}_fundamentals", {}).get('description', 'åŸºæœ¬é¢æ•°æ®')
                        logger.info(f"ğŸ¯ æ‰¾åˆ°åŒ¹é…çš„{desc}ç¼“å­˜: {symbol} ({data_source}) -> {cache_key}")
                        return cache_key
            except Exception:
                continue
        
        desc = self.cache_config.get(f"{market_type}_fundamentals", {}).get('description', 'åŸºæœ¬é¢æ•°æ®')
        logger.error(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„{desc}ç¼“å­˜: {symbol} ({data_source})")
        return None
    
    def clear_old_cache(self, max_age_days: int = 7):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        cutoff_time = datetime.now() - timedelta(days=max_age_days)
        cleared_count = 0
        
        for metadata_file in self.metadata_dir.glob("*_meta.json"):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                cached_at = datetime.fromisoformat(metadata['cached_at'])
                if cached_at < cutoff_time:
                    # åˆ é™¤æ•°æ®æ–‡ä»¶
                    data_file = Path(metadata['file_path'])
                    if data_file.exists():
                        data_file.unlink()
                    
                    # åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶
                    metadata_file.unlink()
                    cleared_count += 1
                    
            except Exception as e:
                logger.warning(f"âš ï¸ æ¸…ç†ç¼“å­˜æ—¶å‡ºé”™: {e}")
        
        logger.info(f"ğŸ§¹ å·²æ¸…ç† {cleared_count} ä¸ªè¿‡æœŸç¼“å­˜æ–‡ä»¶")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_files': 0,
            'stock_data_count': 0,
            'news_count': 0,
            'fundamentals_count': 0,
            'total_size_mb': 0
        }
        
        for metadata_file in self.metadata_dir.glob("*_meta.json"):
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                data_type = metadata.get('data_type', 'unknown')
                if data_type == 'stock_data':
                    stats['stock_data_count'] += 1
                elif data_type == 'news':
                    stats['news_count'] += 1
                elif data_type == 'fundamentals':
                    stats['fundamentals_count'] += 1
                
                # è®¡ç®—æ–‡ä»¶å¤§å°
                data_file = Path(metadata['file_path'])
                if data_file.exists():
                    stats['total_size_mb'] += data_file.stat().st_size / (1024 * 1024)
                
                stats['total_files'] += 1
                
            except Exception:
                continue
        
        stats['total_size_mb'] = round(stats['total_size_mb'], 2)
        return stats


# å…¨å±€ç¼“å­˜å®ä¾‹
_cache_instance = None

def get_cache() -> StockDataCache:
    """è·å–å…¨å±€ç¼“å­˜å®ä¾‹"""
    global _cache_instance
    if _cache_instance is None:
        _cache_instance = StockDataCache()
    return _cache_instance
