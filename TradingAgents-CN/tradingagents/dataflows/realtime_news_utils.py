#!/usr/bin/env python3
"""
å®æ—¶æ–°é—»æ•°æ®è·å–å·¥å…·
è§£å†³æ–°é—»æ»åæ€§é—®é¢˜
"""

import requests
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import time
import os
from dataclasses import dataclass

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('agents')



@dataclass
class NewsItem:
    """æ–°é—»é¡¹ç›®æ•°æ®ç»“æ„"""
    title: str
    content: str
    source: str
    publish_time: datetime
    url: str
    urgency: str  # high, medium, low
    relevance_score: float


class RealtimeNewsAggregator:
    """å®æ—¶æ–°é—»èšåˆå™¨"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'TradingAgents-CN/1.0'
        }
        
        # APIå¯†é’¥é…ç½®
        self.finnhub_key = os.getenv('FINNHUB_API_KEY')
        self.alpha_vantage_key = os.getenv('ALPHA_VANTAGE_API_KEY')
        self.newsapi_key = os.getenv('NEWSAPI_KEY')
        
    def get_realtime_stock_news(self, ticker: str, hours_back: int = 6) -> List[NewsItem]:
        """
        è·å–å®æ—¶è‚¡ç¥¨æ–°é—»
        ä¼˜å…ˆçº§ï¼šä¸“ä¸šAPI > æ–°é—»API > æœç´¢å¼•æ“
        """
        logger.info(f"[æ–°é—»èšåˆå™¨] å¼€å§‹è·å– {ticker} çš„å®æ—¶æ–°é—»ï¼Œå›æº¯æ—¶é—´: {hours_back}å°æ—¶")
        start_time = datetime.now()
        all_news = []
        
        # 1. FinnHubå®æ—¶æ–°é—» (æœ€é«˜ä¼˜å…ˆçº§)
        logger.info(f"[æ–°é—»èšåˆå™¨] å°è¯•ä» FinnHub è·å– {ticker} çš„æ–°é—»")
        finnhub_start = datetime.now()
        finnhub_news = self._get_finnhub_realtime_news(ticker, hours_back)
        finnhub_time = (datetime.now() - finnhub_start).total_seconds()
        
        if finnhub_news:
            logger.info(f"[æ–°é—»èšåˆå™¨] æˆåŠŸä» FinnHub è·å– {len(finnhub_news)} æ¡æ–°é—»ï¼Œè€—æ—¶: {finnhub_time:.2f}ç§’")
        else:
            logger.info(f"[æ–°é—»èšåˆå™¨] FinnHub æœªè¿”å›æ–°é—»ï¼Œè€—æ—¶: {finnhub_time:.2f}ç§’")
            
        all_news.extend(finnhub_news)
        
        # 2. Alpha Vantageæ–°é—»
        logger.info(f"[æ–°é—»èšåˆå™¨] å°è¯•ä» Alpha Vantage è·å– {ticker} çš„æ–°é—»")
        av_start = datetime.now()
        av_news = self._get_alpha_vantage_news(ticker, hours_back)
        av_time = (datetime.now() - av_start).total_seconds()
        
        if av_news:
            logger.info(f"[æ–°é—»èšåˆå™¨] æˆåŠŸä» Alpha Vantage è·å– {len(av_news)} æ¡æ–°é—»ï¼Œè€—æ—¶: {av_time:.2f}ç§’")
        else:
            logger.info(f"[æ–°é—»èšåˆå™¨] Alpha Vantage æœªè¿”å›æ–°é—»ï¼Œè€—æ—¶: {av_time:.2f}ç§’")
            
        all_news.extend(av_news)
        
        # 3. NewsAPI (å¦‚æœé…ç½®äº†)
        if self.newsapi_key:
            logger.info(f"[æ–°é—»èšåˆå™¨] å°è¯•ä» NewsAPI è·å– {ticker} çš„æ–°é—»")
            newsapi_start = datetime.now()
            newsapi_news = self._get_newsapi_news(ticker, hours_back)
            newsapi_time = (datetime.now() - newsapi_start).total_seconds()
            
            if newsapi_news:
                logger.info(f"[æ–°é—»èšåˆå™¨] æˆåŠŸä» NewsAPI è·å– {len(newsapi_news)} æ¡æ–°é—»ï¼Œè€—æ—¶: {newsapi_time:.2f}ç§’")
            else:
                logger.info(f"[æ–°é—»èšåˆå™¨] NewsAPI æœªè¿”å›æ–°é—»ï¼Œè€—æ—¶: {newsapi_time:.2f}ç§’")
                
            all_news.extend(newsapi_news)
        else:
            logger.info(f"[æ–°é—»èšåˆå™¨] NewsAPI å¯†é’¥æœªé…ç½®ï¼Œè·³è¿‡æ­¤æ–°é—»æº")
        
        # 4. ä¸­æ–‡è´¢ç»æ–°é—»æº
        logger.info(f"[æ–°é—»èšåˆå™¨] å°è¯•è·å– {ticker} çš„ä¸­æ–‡è´¢ç»æ–°é—»")
        chinese_start = datetime.now()
        chinese_news = self._get_chinese_finance_news(ticker, hours_back)
        chinese_time = (datetime.now() - chinese_start).total_seconds()
        
        if chinese_news:
            logger.info(f"[æ–°é—»èšåˆå™¨] æˆåŠŸè·å– {len(chinese_news)} æ¡ä¸­æ–‡è´¢ç»æ–°é—»ï¼Œè€—æ—¶: {chinese_time:.2f}ç§’")
        else:
            logger.info(f"[æ–°é—»èšåˆå™¨] æœªè·å–åˆ°ä¸­æ–‡è´¢ç»æ–°é—»ï¼Œè€—æ—¶: {chinese_time:.2f}ç§’")
            
        all_news.extend(chinese_news)
        
        # å»é‡å’Œæ’åº
        logger.info(f"[æ–°é—»èšåˆå™¨] å¼€å§‹å¯¹ {len(all_news)} æ¡æ–°é—»è¿›è¡Œå»é‡å’Œæ’åº")
        dedup_start = datetime.now()
        unique_news = self._deduplicate_news(all_news)
        sorted_news = sorted(unique_news, key=lambda x: x.publish_time, reverse=True)
        dedup_time = (datetime.now() - dedup_start).total_seconds()
        
        # è®°å½•å»é‡ç»“æœ
        removed_count = len(all_news) - len(unique_news)
        logger.info(f"[æ–°é—»èšåˆå™¨] æ–°é—»å»é‡å®Œæˆï¼Œç§»é™¤äº† {removed_count} æ¡é‡å¤æ–°é—»ï¼Œå‰©ä½™ {len(sorted_news)} æ¡ï¼Œè€—æ—¶: {dedup_time:.2f}ç§’")
        
        # è®°å½•æ€»ä½“æƒ…å†µ
        total_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"[æ–°é—»èšåˆå™¨] {ticker} çš„æ–°é—»èšåˆå®Œæˆï¼Œæ€»å…±è·å– {len(sorted_news)} æ¡æ–°é—»ï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        # è®°å½•ä¸€äº›æ–°é—»æ ‡é¢˜ç¤ºä¾‹
        if sorted_news:
            sample_titles = [item.title for item in sorted_news[:3]]
            logger.info(f"[æ–°é—»èšåˆå™¨] æ–°é—»æ ‡é¢˜ç¤ºä¾‹: {', '.join(sample_titles)}")
        
        return sorted_news
    
    def _get_finnhub_realtime_news(self, ticker: str, hours_back: int) -> List[NewsItem]:
        """è·å–FinnHubå®æ—¶æ–°é—»"""
        if not self.finnhub_key:
            return []
        
        try:
            # è®¡ç®—æ—¶é—´èŒƒå›´
            end_time = datetime.now()
            start_time = end_time - timedelta(hours=hours_back)
            
            # FinnHub APIè°ƒç”¨
            url = "https://finnhub.io/api/v1/company-news"
            params = {
                'symbol': ticker,
                'from': start_time.strftime('%Y-%m-%d'),
                'to': end_time.strftime('%Y-%m-%d'),
                'token': self.finnhub_key
            }
            
            response = requests.get(url, params=params, headers=self.headers)
            response.raise_for_status()
            
            news_data = response.json()
            news_items = []
            
            for item in news_data:
                # æ£€æŸ¥æ–°é—»æ—¶æ•ˆæ€§
                publish_time = datetime.fromtimestamp(item.get('datetime', 0))
                if publish_time < start_time:
                    continue
                
                # è¯„ä¼°ç´§æ€¥ç¨‹åº¦
                urgency = self._assess_news_urgency(item.get('headline', ''), item.get('summary', ''))
                
                news_items.append(NewsItem(
                    title=item.get('headline', ''),
                    content=item.get('summary', ''),
                    source=item.get('source', 'FinnHub'),
                    publish_time=publish_time,
                    url=item.get('url', ''),
                    urgency=urgency,
                    relevance_score=self._calculate_relevance(item.get('headline', ''), ticker)
                ))
            
            return news_items
            
        except Exception as e:
            logger.error(f"FinnHubæ–°é—»è·å–å¤±è´¥: {e}")
            return []
    
    def _get_alpha_vantage_news(self, ticker: str, hours_back: int) -> List[NewsItem]:
        """è·å–Alpha Vantageæ–°é—»"""
        if not self.alpha_vantage_key:
            return []
        
        try:
            url = "https://www.alphavantage.co/query"
            params = {
                'function': 'NEWS_SENTIMENT',
                'tickers': ticker,
                'apikey': self.alpha_vantage_key,
                'limit': 50
            }
            
            response = requests.get(url, params=params, headers=self.headers)
            response.raise_for_status()
            
            data = response.json()
            news_items = []
            
            if 'feed' in data:
                for item in data['feed']:
                    # è§£ææ—¶é—´
                    time_str = item.get('time_published', '')
                    try:
                        publish_time = datetime.strptime(time_str, '%Y%m%dT%H%M%S')
                    except:
                        continue
                    
                    # æ£€æŸ¥æ—¶æ•ˆæ€§
                    if publish_time < datetime.now() - timedelta(hours=hours_back):
                        continue
                    
                    urgency = self._assess_news_urgency(item.get('title', ''), item.get('summary', ''))
                    
                    news_items.append(NewsItem(
                        title=item.get('title', ''),
                        content=item.get('summary', ''),
                        source=item.get('source', 'Alpha Vantage'),
                        publish_time=publish_time,
                        url=item.get('url', ''),
                        urgency=urgency,
                        relevance_score=self._calculate_relevance(item.get('title', ''), ticker)
                    ))
            
            return news_items
            
        except Exception as e:
            logger.error(f"Alpha Vantageæ–°é—»è·å–å¤±è´¥: {e}")
            return []
    
    def _get_newsapi_news(self, ticker: str, hours_back: int) -> List[NewsItem]:
        """è·å–NewsAPIæ–°é—»"""
        try:
            # æ„å»ºæœç´¢æŸ¥è¯¢
            company_names = {
                'AAPL': 'Apple',
                'TSLA': 'Tesla', 
                'NVDA': 'NVIDIA',
                'MSFT': 'Microsoft',
                'GOOGL': 'Google'
            }
            
            query = f"{ticker} OR {company_names.get(ticker, ticker)}"
            
            url = "https://newsapi.org/v2/everything"
            params = {
                'q': query,
                'language': 'en',
                'sortBy': 'publishedAt',
                'from': (datetime.now() - timedelta(hours=hours_back)).isoformat(),
                'apiKey': self.newsapi_key
            }
            
            response = requests.get(url, params=params, headers=self.headers)
            response.raise_for_status()
            
            data = response.json()
            news_items = []
            
            for item in data.get('articles', []):
                # è§£ææ—¶é—´
                time_str = item.get('publishedAt', '')
                try:
                    publish_time = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
                except:
                    continue
                
                urgency = self._assess_news_urgency(item.get('title', ''), item.get('description', ''))
                
                news_items.append(NewsItem(
                    title=item.get('title', ''),
                    content=item.get('description', ''),
                    source=item.get('source', {}).get('name', 'NewsAPI'),
                    publish_time=publish_time,
                    url=item.get('url', ''),
                    urgency=urgency,
                    relevance_score=self._calculate_relevance(item.get('title', ''), ticker)
                ))
            
            return news_items
            
        except Exception as e:
            logger.error(f"NewsAPIæ–°é—»è·å–å¤±è´¥: {e}")
            return []
    
    def _get_chinese_finance_news(self, ticker: str, hours_back: int) -> List[NewsItem]:
        """è·å–ä¸­æ–‡è´¢ç»æ–°é—»"""
        # é›†æˆä¸­æ–‡è´¢ç»æ–°é—»APIï¼šè´¢è”ç¤¾ã€ä¸œæ–¹è´¢å¯Œç­‰
        logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] å¼€å§‹è·å– {ticker} çš„ä¸­æ–‡è´¢ç»æ–°é—»ï¼Œå›æº¯æ—¶é—´: {hours_back}å°æ—¶")
        start_time = datetime.now()
        
        try:
            news_items = []
            
            # 1. å°è¯•ä½¿ç”¨AKShareè·å–ä¸œæ–¹è´¢å¯Œä¸ªè‚¡æ–°é—»
            try:
                logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] å°è¯•å¯¼å…¥ AKShare å·¥å…·")
                from .akshare_utils import get_stock_news_em
                
                # å¤„ç†è‚¡ç¥¨ä»£ç æ ¼å¼
                # å¦‚æœæ˜¯ç¾è‚¡ä»£ç ï¼Œä¸ä½¿ç”¨ä¸œæ–¹è´¢å¯Œæ–°é—»
                if '.' in ticker and any(suffix in ticker for suffix in ['.US', '.N', '.O', '.NYSE', '.NASDAQ']):
                    logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] æ£€æµ‹åˆ°ç¾è‚¡ä»£ç  {ticker}ï¼Œè·³è¿‡ä¸œæ–¹è´¢å¯Œæ–°é—»è·å–")
                else:
                    # å¤„ç†Aè‚¡å’Œæ¸¯è‚¡ä»£ç 
                    clean_ticker = ticker.replace('.SH', '').replace('.SZ', '').replace('.SS', '')\
                                    .replace('.HK', '').replace('.XSHE', '').replace('.XSHG', '')
                    
                    # è·å–ä¸œæ–¹è´¢å¯Œæ–°é—»
                    logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] å¼€å§‹è·å– {clean_ticker} çš„ä¸œæ–¹è´¢å¯Œæ–°é—»")
                    em_start_time = datetime.now()
                    news_df = get_stock_news_em(clean_ticker)
                    
                    if not news_df.empty:
                        logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] ä¸œæ–¹è´¢å¯Œè¿”å› {len(news_df)} æ¡æ–°é—»æ•°æ®ï¼Œå¼€å§‹å¤„ç†")
                        processed_count = 0
                        skipped_count = 0
                        error_count = 0
                        
                        # è½¬æ¢ä¸ºNewsItemæ ¼å¼
                        for _, row in news_df.iterrows():
                            try:
                                # è§£ææ—¶é—´
                                time_str = row.get('æ—¶é—´', '')
                                if time_str:
                                    # å°è¯•è§£ææ—¶é—´æ ¼å¼ï¼Œå¯èƒ½æ˜¯'2023-01-01 12:34:56'æ ¼å¼
                                    try:
                                        publish_time = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
                                    except:
                                        # å°è¯•å…¶ä»–å¯èƒ½çš„æ ¼å¼
                                        try:
                                            publish_time = datetime.strptime(time_str, '%Y-%m-%d')
                                        except:
                                            logger.warning(f"[ä¸­æ–‡è´¢ç»æ–°é—»] æ— æ³•è§£ææ—¶é—´æ ¼å¼: {time_str}ï¼Œä½¿ç”¨å½“å‰æ—¶é—´")
                                            publish_time = datetime.now()
                                else:
                                    logger.warning(f"[ä¸­æ–‡è´¢ç»æ–°é—»] æ–°é—»æ—¶é—´ä¸ºç©ºï¼Œä½¿ç”¨å½“å‰æ—¶é—´")
                                    publish_time = datetime.now()
                                
                                # æ£€æŸ¥æ—¶æ•ˆæ€§
                                if publish_time < datetime.now() - timedelta(hours=hours_back):
                                    skipped_count += 1
                                    continue
                                
                                # è¯„ä¼°ç´§æ€¥ç¨‹åº¦
                                title = row.get('æ ‡é¢˜', '')
                                content = row.get('å†…å®¹', '')
                                urgency = self._assess_news_urgency(title, content)
                                
                                news_items.append(NewsItem(
                                    title=title,
                                    content=content,
                                    source='ä¸œæ–¹è´¢å¯Œ',
                                    publish_time=publish_time,
                                    url=row.get('é“¾æ¥', ''),
                                    urgency=urgency,
                                    relevance_score=self._calculate_relevance(title, ticker)
                                ))
                                processed_count += 1
                            except Exception as item_e:
                                logger.error(f"[ä¸­æ–‡è´¢ç»æ–°é—»] å¤„ç†ä¸œæ–¹è´¢å¯Œæ–°é—»é¡¹ç›®å¤±è´¥: {item_e}")
                                error_count += 1
                                continue
                        
                        em_time = (datetime.now() - em_start_time).total_seconds()
                        logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] ä¸œæ–¹è´¢å¯Œæ–°é—»å¤„ç†å®Œæˆï¼ŒæˆåŠŸ: {processed_count}æ¡ï¼Œè·³è¿‡: {skipped_count}æ¡ï¼Œé”™è¯¯: {error_count}æ¡ï¼Œè€—æ—¶: {em_time:.2f}ç§’")
            except Exception as ak_e:
                logger.error(f"[ä¸­æ–‡è´¢ç»æ–°é—»] è·å–ä¸œæ–¹è´¢å¯Œæ–°é—»å¤±è´¥: {ak_e}")
            
            # 2. è´¢è”ç¤¾RSS (å¦‚æœå¯ç”¨)
            logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] å¼€å§‹è·å–è´¢è”ç¤¾RSSæ–°é—»")
            rss_start_time = datetime.now()
            rss_sources = [
                "https://www.cls.cn/api/sw?app=CailianpressWeb&os=web&sv=7.7.5",
                # å¯ä»¥æ·»åŠ æ›´å¤šRSSæº
            ]
            
            rss_success_count = 0
            rss_error_count = 0
            total_rss_items = 0
            
            for rss_url in rss_sources:
                try:
                    logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] å°è¯•è§£æRSSæº: {rss_url}")
                    rss_item_start = datetime.now()
                    items = self._parse_rss_feed(rss_url, ticker, hours_back)
                    rss_item_time = (datetime.now() - rss_item_start).total_seconds()
                    
                    if items:
                        logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] æˆåŠŸä»RSSæºè·å– {len(items)} æ¡æ–°é—»ï¼Œè€—æ—¶: {rss_item_time:.2f}ç§’")
                        news_items.extend(items)
                        total_rss_items += len(items)
                        rss_success_count += 1
                    else:
                        logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] RSSæºæœªè¿”å›ç›¸å…³æ–°é—»ï¼Œè€—æ—¶: {rss_item_time:.2f}ç§’")
                except Exception as rss_e:
                    logger.error(f"[ä¸­æ–‡è´¢ç»æ–°é—»] è§£æRSSæºå¤±è´¥: {rss_e}")
                    rss_error_count += 1
                    continue
            
            # è®°å½•RSSè·å–æ€»ç»“
            rss_total_time = (datetime.now() - rss_start_time).total_seconds()
            logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] RSSæ–°é—»è·å–å®Œæˆï¼ŒæˆåŠŸæº: {rss_success_count}ä¸ªï¼Œå¤±è´¥æº: {rss_error_count}ä¸ªï¼Œè·å–æ–°é—»: {total_rss_items}æ¡ï¼Œæ€»è€—æ—¶: {rss_total_time:.2f}ç§’")
            
            # è®°å½•ä¸­æ–‡è´¢ç»æ–°é—»è·å–æ€»ç»“
            total_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"[ä¸­æ–‡è´¢ç»æ–°é—»] {ticker} çš„ä¸­æ–‡è´¢ç»æ–°é—»è·å–å®Œæˆï¼Œæ€»å…±è·å– {len(news_items)} æ¡æ–°é—»ï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
            
            return news_items
            
        except Exception as e:
            logger.error(f"[ä¸­æ–‡è´¢ç»æ–°é—»] ä¸­æ–‡è´¢ç»æ–°é—»è·å–å¤±è´¥: {e}")
            return []
    
    def _parse_rss_feed(self, rss_url: str, ticker: str, hours_back: int) -> List[NewsItem]:
        """è§£æRSSæº"""
        logger.info(f"[RSSè§£æ] å¼€å§‹è§£æRSSæº: {rss_url}ï¼Œè‚¡ç¥¨: {ticker}ï¼Œå›æº¯æ—¶é—´: {hours_back}å°æ—¶")
        start_time = datetime.now()
        
        try:
            # å®é™…å®ç°éœ€è¦ä½¿ç”¨feedparseråº“
            # è¿™é‡Œæ˜¯ç®€åŒ–å®ç°ï¼Œå®é™…é¡¹ç›®ä¸­åº”è¯¥æ›¿æ¢ä¸ºçœŸå®çš„RSSè§£æé€»è¾‘
            import feedparser
            
            logger.info(f"[RSSè§£æ] å°è¯•è·å–RSSæºå†…å®¹")
            feed = feedparser.parse(rss_url)
            
            if not feed or not feed.entries:
                logger.warning(f"[RSSè§£æ] RSSæºæœªè¿”å›æœ‰æ•ˆå†…å®¹")
                return []
            
            logger.info(f"[RSSè§£æ] æˆåŠŸè·å–RSSæºï¼ŒåŒ…å« {len(feed.entries)} æ¡æ¡ç›®")
            news_items = []
            processed_count = 0
            skipped_count = 0
            
            for entry in feed.entries:
                try:
                    # è§£ææ—¶é—´
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        publish_time = datetime.fromtimestamp(time.mktime(entry.published_parsed))
                    else:
                        logger.warning(f"[RSSè§£æ] æ¡ç›®ç¼ºå°‘å‘å¸ƒæ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´")
                        publish_time = datetime.now()
                    
                    # æ£€æŸ¥æ—¶æ•ˆæ€§
                    if publish_time < datetime.now() - timedelta(hours=hours_back):
                        skipped_count += 1
                        continue
                    
                    title = entry.title if hasattr(entry, 'title') else ''
                    content = entry.description if hasattr(entry, 'description') else ''
                    
                    # æ£€æŸ¥ç›¸å…³æ€§
                    if ticker.lower() not in title.lower() and ticker.lower() not in content.lower():
                        skipped_count += 1
                        continue
                    
                    # è¯„ä¼°ç´§æ€¥ç¨‹åº¦
                    urgency = self._assess_news_urgency(title, content)
                    
                    news_items.append(NewsItem(
                        title=title,
                        content=content,
                        source='è´¢è”ç¤¾',
                        publish_time=publish_time,
                        url=entry.link if hasattr(entry, 'link') else '',
                        urgency=urgency,
                        relevance_score=self._calculate_relevance(title, ticker)
                    ))
                    processed_count += 1
                except Exception as e:
                    logger.error(f"[RSSè§£æ] å¤„ç†RSSæ¡ç›®å¤±è´¥: {e}")
                    continue
            
            total_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"[RSSè§£æ] RSSæºè§£æå®Œæˆï¼ŒæˆåŠŸ: {processed_count}æ¡ï¼Œè·³è¿‡: {skipped_count}æ¡ï¼Œè€—æ—¶: {total_time:.2f}ç§’")
            return news_items
        except ImportError:
            logger.error(f"[RSSè§£æ] feedparseråº“æœªå®‰è£…ï¼Œæ— æ³•è§£æRSSæº")
            return []
        except Exception as e:
            logger.error(f"[RSSè§£æ] è§£æRSSæºå¤±è´¥: {e}")
            return []
    
    def _assess_news_urgency(self, title: str, content: str) -> str:
        """è¯„ä¼°æ–°é—»ç´§æ€¥ç¨‹åº¦"""
        text = (title + ' ' + content).lower()
        
        # é«˜ç´§æ€¥åº¦å…³é”®è¯
        high_urgency_keywords = [
            'breaking', 'urgent', 'alert', 'emergency', 'halt', 'suspend',
            'çªå‘', 'ç´§æ€¥', 'æš‚åœ', 'åœç‰Œ', 'é‡å¤§'
        ]
        
        # ä¸­ç­‰ç´§æ€¥åº¦å…³é”®è¯
        medium_urgency_keywords = [
            'earnings', 'report', 'announce', 'launch', 'merger', 'acquisition',
            'è´¢æŠ¥', 'å‘å¸ƒ', 'å®£å¸ƒ', 'å¹¶è´­', 'æ”¶è´­'
        ]
        
        # æ£€æŸ¥é«˜ç´§æ€¥åº¦å…³é”®è¯
        for keyword in high_urgency_keywords:
            if keyword in text:
                logger.debug(f"[ç´§æ€¥åº¦è¯„ä¼°] æ£€æµ‹åˆ°é«˜ç´§æ€¥åº¦å…³é”®è¯ '{keyword}' åœ¨æ–°é—»ä¸­: {title[:50]}...")
                return 'high'
        
        # æ£€æŸ¥ä¸­ç­‰ç´§æ€¥åº¦å…³é”®è¯
        for keyword in medium_urgency_keywords:
            if keyword in text:
                logger.debug(f"[ç´§æ€¥åº¦è¯„ä¼°] æ£€æµ‹åˆ°ä¸­ç­‰ç´§æ€¥åº¦å…³é”®è¯ '{keyword}' åœ¨æ–°é—»ä¸­: {title[:50]}...")
                return 'medium'
        
        logger.debug(f"[ç´§æ€¥åº¦è¯„ä¼°] æœªæ£€æµ‹åˆ°ç´§æ€¥å…³é”®è¯ï¼Œè¯„ä¼°ä¸ºä½ç´§æ€¥åº¦: {title[:50]}...")
        return 'low'
    
    def _calculate_relevance(self, title: str, ticker: str) -> float:
        """è®¡ç®—æ–°é—»ç›¸å…³æ€§åˆ†æ•°"""
        text = title.lower()
        ticker_lower = ticker.lower()
        
        # åŸºç¡€ç›¸å…³æ€§ - è‚¡ç¥¨ä»£ç ç›´æ¥å‡ºç°åœ¨æ ‡é¢˜ä¸­
        if ticker_lower in text:
            logger.debug(f"[ç›¸å…³æ€§è®¡ç®—] è‚¡ç¥¨ä»£ç  {ticker} ç›´æ¥å‡ºç°åœ¨æ ‡é¢˜ä¸­ï¼Œç›¸å…³æ€§è¯„åˆ†: 1.0ï¼Œæ ‡é¢˜: {title[:50]}...")
            return 1.0
        
        # å…¬å¸åç§°åŒ¹é…
        company_names = {
            'aapl': ['apple', 'iphone', 'ipad', 'mac'],
            'tsla': ['tesla', 'elon musk', 'electric vehicle'],
            'nvda': ['nvidia', 'gpu', 'ai chip'],
            'msft': ['microsoft', 'windows', 'azure'],
            'googl': ['google', 'alphabet', 'search']
        }
        
        # æ£€æŸ¥å…¬å¸ç›¸å…³å…³é”®è¯
        if ticker_lower in company_names:
            for name in company_names[ticker_lower]:
                if name in text:
                    logger.debug(f"[ç›¸å…³æ€§è®¡ç®—] æ£€æµ‹åˆ°å…¬å¸ç›¸å…³å…³é”®è¯ '{name}' åœ¨æ ‡é¢˜ä¸­ï¼Œç›¸å…³æ€§è¯„åˆ†: 0.8ï¼Œæ ‡é¢˜: {title[:50]}...")
                    return 0.8
        
        # æå–è‚¡ç¥¨ä»£ç çš„çº¯æ•°å­—éƒ¨åˆ†ï¼ˆé€‚ç”¨äºä¸­å›½è‚¡ç¥¨ï¼‰
        pure_code = ''.join(filter(str.isdigit, ticker))
        if pure_code and pure_code in text:
            logger.debug(f"[ç›¸å…³æ€§è®¡ç®—] è‚¡ç¥¨ä»£ç æ•°å­—éƒ¨åˆ† {pure_code} å‡ºç°åœ¨æ ‡é¢˜ä¸­ï¼Œç›¸å…³æ€§è¯„åˆ†: 0.9ï¼Œæ ‡é¢˜: {title[:50]}...")
            return 0.9
        
        logger.debug(f"[ç›¸å…³æ€§è®¡ç®—] æœªæ£€æµ‹åˆ°æ˜ç¡®ç›¸å…³æ€§ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†: 0.3ï¼Œæ ‡é¢˜: {title[:50]}...")
        return 0.3  # é»˜è®¤ç›¸å…³æ€§
    
    def _deduplicate_news(self, news_items: List[NewsItem]) -> List[NewsItem]:
        """å»é‡æ–°é—»"""
        logger.info(f"[æ–°é—»å»é‡] å¼€å§‹å¯¹ {len(news_items)} æ¡æ–°é—»è¿›è¡Œå»é‡å¤„ç†")
        start_time = datetime.now()
        
        seen_titles = set()
        unique_news = []
        duplicate_count = 0
        short_title_count = 0
        
        for item in news_items:
            # ç®€å•çš„æ ‡é¢˜å»é‡
            title_key = item.title.lower().strip()
            
            # æ£€æŸ¥æ ‡é¢˜é•¿åº¦
            if len(title_key) <= 10:
                logger.debug(f"[æ–°é—»å»é‡] è·³è¿‡æ ‡é¢˜è¿‡çŸ­çš„æ–°é—»: '{item.title}'ï¼Œæ¥æº: {item.source}")
                short_title_count += 1
                continue
                
            # æ£€æŸ¥æ˜¯å¦é‡å¤
            if title_key in seen_titles:
                logger.debug(f"[æ–°é—»å»é‡] æ£€æµ‹åˆ°é‡å¤æ–°é—»: '{item.title[:50]}...'ï¼Œæ¥æº: {item.source}")
                duplicate_count += 1
                continue
                
            # æ·»åŠ åˆ°ç»“æœé›†
            seen_titles.add(title_key)
            unique_news.append(item)
        
        # è®°å½•å»é‡ç»“æœ
        time_taken = (datetime.now() - start_time).total_seconds()
        logger.info(f"[æ–°é—»å»é‡] å»é‡å®Œæˆï¼ŒåŸå§‹æ–°é—»: {len(news_items)}æ¡ï¼Œå»é‡å: {len(unique_news)}æ¡ï¼Œ")
        logger.info(f"[æ–°é—»å»é‡] å»é™¤é‡å¤: {duplicate_count}æ¡ï¼Œæ ‡é¢˜è¿‡çŸ­: {short_title_count}æ¡ï¼Œè€—æ—¶: {time_taken:.2f}ç§’")
        
        return unique_news
    
    def format_news_report(self, news_items: List[NewsItem], ticker: str) -> str:
        """æ ¼å¼åŒ–æ–°é—»æŠ¥å‘Š"""
        logger.info(f"[æ–°é—»æŠ¥å‘Š] å¼€å§‹ä¸º {ticker} ç”Ÿæˆæ–°é—»æŠ¥å‘Š")
        start_time = datetime.now()
        
        if not news_items:
            logger.warning(f"[æ–°é—»æŠ¥å‘Š] æœªè·å–åˆ° {ticker} çš„å®æ—¶æ–°é—»æ•°æ®")
            return f"æœªè·å–åˆ°{ticker}çš„å®æ—¶æ–°é—»æ•°æ®ã€‚"
        
        # æŒ‰ç´§æ€¥ç¨‹åº¦åˆ†ç»„
        high_urgency = [n for n in news_items if n.urgency == 'high']
        medium_urgency = [n for n in news_items if n.urgency == 'medium']
        low_urgency = [n for n in news_items if n.urgency == 'low']
        
        # è®°å½•æ–°é—»åˆ†ç±»æƒ…å†µ
        logger.info(f"[æ–°é—»æŠ¥å‘Š] {ticker} æ–°é—»åˆ†ç±»ç»Ÿè®¡: é«˜ç´§æ€¥åº¦ {len(high_urgency)}æ¡, ä¸­ç´§æ€¥åº¦ {len(medium_urgency)}æ¡, ä½ç´§æ€¥åº¦ {len(low_urgency)}æ¡")
        
        # è®°å½•æ–°é—»æ¥æºåˆ†å¸ƒ
        news_sources = {}
        for item in news_items:
            source = item.source
            if source in news_sources:
                news_sources[source] += 1
            else:
                news_sources[source] = 1
        
        sources_info = ", ".join([f"{source}: {count}æ¡" for source, count in news_sources.items()])
        logger.info(f"[æ–°é—»æŠ¥å‘Š] {ticker} æ–°é—»æ¥æºåˆ†å¸ƒ: {sources_info}")
        
        report = f"# {ticker} å®æ—¶æ–°é—»åˆ†ææŠ¥å‘Š\n\n"
        report += f"ğŸ“… ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        report += f"ğŸ“Š æ–°é—»æ€»æ•°: {len(news_items)}æ¡\n\n"
        
        if high_urgency:
            report += "## ğŸš¨ ç´§æ€¥æ–°é—»\n\n"
            for news in high_urgency[:3]:  # æœ€å¤šæ˜¾ç¤º3æ¡
                report += f"### {news.title}\n"
                report += f"**æ¥æº**: {news.source} | **æ—¶é—´**: {news.publish_time.strftime('%H:%M')}\n"
                report += f"{news.content}\n\n"
        
        if medium_urgency:
            report += "## ğŸ“¢ é‡è¦æ–°é—»\n\n"
            for news in medium_urgency[:5]:  # æœ€å¤šæ˜¾ç¤º5æ¡
                report += f"### {news.title}\n"
                report += f"**æ¥æº**: {news.source} | **æ—¶é—´**: {news.publish_time.strftime('%H:%M')}\n"
                report += f"{news.content}\n\n"
        
        # æ·»åŠ æ—¶æ•ˆæ€§è¯´æ˜
        latest_news = max(news_items, key=lambda x: x.publish_time)
        time_diff = datetime.now() - latest_news.publish_time
        
        report += f"\n## â° æ•°æ®æ—¶æ•ˆæ€§\n"
        report += f"æœ€æ–°æ–°é—»å‘å¸ƒäº: {time_diff.total_seconds() / 60:.0f}åˆ†é’Ÿå‰\n"
        
        if time_diff.total_seconds() < 1800:  # 30åˆ†é’Ÿå†…
            report += "ğŸŸ¢ æ•°æ®æ—¶æ•ˆæ€§: ä¼˜ç§€ (30åˆ†é’Ÿå†…)\n"
        elif time_diff.total_seconds() < 3600:  # 1å°æ—¶å†…
            report += "ğŸŸ¡ æ•°æ®æ—¶æ•ˆæ€§: è‰¯å¥½ (1å°æ—¶å†…)\n"
        else:
            report += "ğŸ”´ æ•°æ®æ—¶æ•ˆæ€§: ä¸€èˆ¬ (è¶…è¿‡1å°æ—¶)\n"
        
        # è®°å½•æŠ¥å‘Šç”Ÿæˆå®Œæˆä¿¡æ¯
        end_time = datetime.now()
        time_taken = (end_time - start_time).total_seconds()
        report_length = len(report)
        
        logger.info(f"[æ–°é—»æŠ¥å‘Š] {ticker} æ–°é—»æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {time_taken:.2f}ç§’ï¼ŒæŠ¥å‘Šé•¿åº¦: {report_length}å­—ç¬¦")
        
        # è®°å½•æ—¶æ•ˆæ€§ä¿¡æ¯
        time_diff_minutes = time_diff.total_seconds() / 60
        logger.info(f"[æ–°é—»æŠ¥å‘Š] {ticker} æ–°é—»æ—¶æ•ˆæ€§: æœ€æ–°æ–°é—»å‘å¸ƒäº {time_diff_minutes:.1f}åˆ†é’Ÿå‰")
        
        return report


def get_realtime_stock_news(ticker: str, curr_date: str, hours_back: int = 6) -> str:
    """
    è·å–å®æ—¶è‚¡ç¥¨æ–°é—»çš„ä¸»è¦æ¥å£å‡½æ•°
    """
    logger.info(f"[æ–°é—»åˆ†æ] ========== å‡½æ•°å…¥å£ ==========")
    logger.info(f"[æ–°é—»åˆ†æ] å‡½æ•°: get_realtime_stock_news")
    logger.info(f"[æ–°é—»åˆ†æ] å‚æ•°: ticker={ticker}, curr_date={curr_date}, hours_back={hours_back}")
    logger.info(f"[æ–°é—»åˆ†æ] å¼€å§‹è·å– {ticker} çš„å®æ—¶æ–°é—»ï¼Œæ—¥æœŸ: {curr_date}, å›æº¯æ—¶é—´: {hours_back}å°æ—¶")
    start_total_time = datetime.now()
    logger.info(f"[æ–°é—»åˆ†æ] å¼€å§‹æ—¶é—´: {start_total_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
    
    # åˆ¤æ–­è‚¡ç¥¨ç±»å‹
    logger.info(f"[æ–°é—»åˆ†æ] ========== æ­¥éª¤1: è‚¡ç¥¨ç±»å‹åˆ¤æ–­ ==========")
    stock_type = "æœªçŸ¥"
    is_china_stock = False
    logger.info(f"[æ–°é—»åˆ†æ] åŸå§‹ticker: {ticker}")
    
    if '.' in ticker:
        logger.info(f"[æ–°é—»åˆ†æ] æ£€æµ‹åˆ°tickeråŒ…å«ç‚¹å·ï¼Œè¿›è¡Œåç¼€åŒ¹é…")
        if any(suffix in ticker for suffix in ['.SH', '.SZ', '.SS', '.XSHE', '.XSHG']):
            stock_type = "Aè‚¡"
            is_china_stock = True
            logger.info(f"[æ–°é—»åˆ†æ] åŒ¹é…åˆ°Aè‚¡åç¼€ï¼Œè‚¡ç¥¨ç±»å‹: {stock_type}")
        elif '.HK' in ticker:
            stock_type = "æ¸¯è‚¡"
            logger.info(f"[æ–°é—»åˆ†æ] åŒ¹é…åˆ°æ¸¯è‚¡åç¼€ï¼Œè‚¡ç¥¨ç±»å‹: {stock_type}")
        elif any(suffix in ticker for suffix in ['.US', '.N', '.O', '.NYSE', '.NASDAQ']):
            stock_type = "ç¾è‚¡"
            logger.info(f"[æ–°é—»åˆ†æ] åŒ¹é…åˆ°ç¾è‚¡åç¼€ï¼Œè‚¡ç¥¨ç±»å‹: {stock_type}")
        else:
            logger.info(f"[æ–°é—»åˆ†æ] æœªåŒ¹é…åˆ°å·²çŸ¥åç¼€")
    else:
        logger.info(f"[æ–°é—»åˆ†æ] tickerä¸åŒ…å«ç‚¹å·ï¼Œå°è¯•ä½¿ç”¨StockUtilsåˆ¤æ–­")
        # å°è¯•ä½¿ç”¨StockUtilsåˆ¤æ–­è‚¡ç¥¨ç±»å‹
        try:
            from tradingagents.utils.stock_utils import StockUtils
            logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸå¯¼å…¥StockUtilsï¼Œå¼€å§‹åˆ¤æ–­è‚¡ç¥¨ç±»å‹")
            market_info = StockUtils.get_market_info(ticker)
            logger.info(f"[æ–°é—»åˆ†æ] StockUtilsè¿”å›å¸‚åœºä¿¡æ¯: {market_info}")
            if market_info['is_china']:
                stock_type = "Aè‚¡"
                is_china_stock = True
                logger.info(f"[æ–°é—»åˆ†æ] StockUtilsåˆ¤æ–­ä¸ºAè‚¡")
            elif market_info['is_hk']:
                stock_type = "æ¸¯è‚¡"
                logger.info(f"[æ–°é—»åˆ†æ] StockUtilsåˆ¤æ–­ä¸ºæ¸¯è‚¡")
            elif market_info['is_us']:
                stock_type = "ç¾è‚¡"
                logger.info(f"[æ–°é—»åˆ†æ] StockUtilsåˆ¤æ–­ä¸ºç¾è‚¡")
        except Exception as e:
            logger.warning(f"[æ–°é—»åˆ†æ] ä½¿ç”¨StockUtilsåˆ¤æ–­è‚¡ç¥¨ç±»å‹å¤±è´¥: {e}")
    
    logger.info(f"[æ–°é—»åˆ†æ] æœ€ç»ˆåˆ¤æ–­ç»“æœ - è‚¡ç¥¨ {ticker} ç±»å‹: {stock_type}, æ˜¯å¦Aè‚¡: {is_china_stock}")
    
    # å¯¹äºAè‚¡ï¼Œä¼˜å…ˆä½¿ç”¨ä¸œæ–¹è´¢å¯Œæ–°é—»æº
    if is_china_stock:
        logger.info(f"[æ–°é—»åˆ†æ] ========== æ­¥éª¤2: Aè‚¡ä¸œæ–¹è´¢å¯Œæ–°é—»è·å– ==========")
        logger.info(f"[æ–°é—»åˆ†æ] æ£€æµ‹åˆ°Aè‚¡è‚¡ç¥¨ {ticker}ï¼Œä¼˜å…ˆå°è¯•ä½¿ç”¨ä¸œæ–¹è´¢å¯Œæ–°é—»æº")
        try:
            logger.info(f"[æ–°é—»åˆ†æ] å°è¯•å¯¼å…¥ akshare_utils.get_stock_news_em")
            from .akshare_utils import get_stock_news_em
            logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸå¯¼å…¥ get_stock_news_em å‡½æ•°")
            
            # å¤„ç†Aè‚¡ä»£ç 
            clean_ticker = ticker.replace('.SH', '').replace('.SZ', '').replace('.SS', '')\
                            .replace('.XSHE', '').replace('.XSHG', '')
            logger.info(f"[æ–°é—»åˆ†æ] åŸå§‹ticker: {ticker} -> æ¸…ç†åticker: {clean_ticker}")
            
            logger.info(f"[æ–°é—»åˆ†æ] å‡†å¤‡è°ƒç”¨ get_stock_news_em({clean_ticker})")
            logger.info(f"[æ–°é—»åˆ†æ] å¼€å§‹ä»ä¸œæ–¹è´¢å¯Œè·å– {clean_ticker} çš„æ–°é—»æ•°æ®")
            start_time = datetime.now()
            logger.info(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯ŒAPIè°ƒç”¨å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
            
            news_df = get_stock_news_em(clean_ticker)
            
            end_time = datetime.now()
            time_taken = (end_time - start_time).total_seconds()
            logger.info(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯ŒAPIè°ƒç”¨ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
            logger.info(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯ŒAPIè°ƒç”¨è€—æ—¶: {time_taken:.2f}ç§’")
            logger.info(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯ŒAPIè¿”å›æ•°æ®ç±»å‹: {type(news_df)}")
            
            if hasattr(news_df, 'empty'):
                logger.info(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯ŒAPIè¿”å›DataFrameï¼Œæ˜¯å¦ä¸ºç©º: {news_df.empty}")
                if not news_df.empty:
                    logger.info(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯ŒAPIè¿”å›DataFrameå½¢çŠ¶: {news_df.shape}")
                    logger.info(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯ŒAPIè¿”å›DataFrameåˆ—å: {list(news_df.columns) if hasattr(news_df, 'columns') else 'æ— åˆ—å'}")
            else:
                logger.info(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯ŒAPIè¿”å›æ•°æ®: {news_df}")
            
            if not news_df.empty:
                # æ„å»ºç®€å•çš„æ–°é—»æŠ¥å‘Š
                news_count = len(news_df)
                logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸè·å– {news_count} æ¡ä¸œæ–¹è´¢å¯Œæ–°é—»ï¼Œè€—æ—¶ {time_taken:.2f} ç§’")
                
                report = f"# {ticker} ä¸œæ–¹è´¢å¯Œæ–°é—»æŠ¥å‘Š\n\n"
                report += f"ğŸ“… ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                report += f"ğŸ“Š æ–°é—»æ€»æ•°: {news_count}æ¡\n"
                report += f"ğŸ•’ è·å–è€—æ—¶: {time_taken:.2f}ç§’\n\n"
                
                # è®°å½•ä¸€äº›æ–°é—»æ ‡é¢˜ç¤ºä¾‹
                sample_titles = [row.get('æ–°é—»æ ‡é¢˜', 'æ— æ ‡é¢˜') for _, row in news_df.head(3).iterrows()]
                logger.info(f"[æ–°é—»åˆ†æ] æ–°é—»æ ‡é¢˜ç¤ºä¾‹: {', '.join(sample_titles)}")
                
                logger.info(f"[æ–°é—»åˆ†æ] å¼€å§‹æ„å»ºæ–°é—»æŠ¥å‘Š")
                for idx, (_, row) in enumerate(news_df.iterrows()):
                    if idx < 3:  # åªè®°å½•å‰3æ¡çš„è¯¦ç»†ä¿¡æ¯
                        logger.info(f"[æ–°é—»åˆ†æ] ç¬¬{idx+1}æ¡æ–°é—»: æ ‡é¢˜={row.get('æ–°é—»æ ‡é¢˜', 'æ— æ ‡é¢˜')}, æ—¶é—´={row.get('å‘å¸ƒæ—¶é—´', 'æ— æ—¶é—´')}")
                    report += f"### {row.get('æ–°é—»æ ‡é¢˜', '')}\n"
                    report += f"ğŸ“… {row.get('å‘å¸ƒæ—¶é—´', '')}\n"
                    report += f"ğŸ”— {row.get('æ–°é—»é“¾æ¥', '')}\n\n"
                    report += f"{row.get('æ–°é—»å†…å®¹', 'æ— å†…å®¹')}\n\n"
                
                total_time_taken = (datetime.now() - start_total_time).total_seconds()
                logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸç”Ÿæˆ {ticker} çš„æ–°é—»æŠ¥å‘Šï¼Œæ€»è€—æ—¶ {total_time_taken:.2f} ç§’ï¼Œæ–°é—»æ¥æº: ä¸œæ–¹è´¢å¯Œ")
                logger.info(f"[æ–°é—»åˆ†æ] æŠ¥å‘Šé•¿åº¦: {len(report)} å­—ç¬¦")
                logger.info(f"[æ–°é—»åˆ†æ] ========== ä¸œæ–¹è´¢å¯Œæ–°é—»è·å–æˆåŠŸï¼Œå‡½æ•°å³å°†è¿”å› ==========")
                return report
            else:
                logger.warning(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯Œæœªè·å–åˆ° {ticker} çš„æ–°é—»ï¼Œè€—æ—¶ {time_taken:.2f} ç§’ï¼Œå°è¯•ä½¿ç”¨å…¶ä»–æ–°é—»æº")
        except Exception as e:
            logger.error(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯Œæ–°é—»è·å–å¤±è´¥: {e}ï¼Œå°†å°è¯•å…¶ä»–æ–°é—»æº")
            logger.error(f"[æ–°é—»åˆ†æ] å¼‚å¸¸è¯¦æƒ…: {type(e).__name__}: {str(e)}")
            import traceback
            logger.error(f"[æ–°é—»åˆ†æ] å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
    else:
        logger.info(f"[æ–°é—»åˆ†æ] ========== è·³è¿‡Aè‚¡ä¸œæ–¹è´¢å¯Œæ–°é—»è·å– ==========")
        logger.info(f"[æ–°é—»åˆ†æ] è‚¡ç¥¨ç±»å‹ä¸º {stock_type}ï¼Œä¸æ˜¯Aè‚¡ï¼Œè·³è¿‡ä¸œæ–¹è´¢å¯Œæ–°é—»æº")
    
    # å¦‚æœä¸æ˜¯Aè‚¡æˆ–Aè‚¡æ–°é—»è·å–å¤±è´¥ï¼Œä½¿ç”¨å®æ—¶æ–°é—»èšåˆå™¨
    logger.info(f"[æ–°é—»åˆ†æ] ========== æ­¥éª¤3: å®æ—¶æ–°é—»èšåˆå™¨ ==========")
    aggregator = RealtimeNewsAggregator()
    logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸåˆ›å»ºå®æ—¶æ–°é—»èšåˆå™¨å®ä¾‹")
    try:
        logger.info(f"[æ–°é—»åˆ†æ] å°è¯•ä½¿ç”¨å®æ—¶æ–°é—»èšåˆå™¨è·å– {ticker} çš„æ–°é—»")
        start_time = datetime.now()
        logger.info(f"[æ–°é—»åˆ†æ] èšåˆå™¨è°ƒç”¨å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
        
        # è·å–å®æ—¶æ–°é—»
        news_items = aggregator.get_realtime_stock_news(ticker, hours_back)
        
        end_time = datetime.now()
        time_taken = (end_time - start_time).total_seconds()
        logger.info(f"[æ–°é—»åˆ†æ] èšåˆå™¨è°ƒç”¨ç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]}")
        logger.info(f"[æ–°é—»åˆ†æ] èšåˆå™¨è°ƒç”¨è€—æ—¶: {time_taken:.2f}ç§’")
        logger.info(f"[æ–°é—»åˆ†æ] èšåˆå™¨è¿”å›æ•°æ®ç±»å‹: {type(news_items)}")
        logger.info(f"[æ–°é—»åˆ†æ] èšåˆå™¨è¿”å›æ•°æ®: {news_items}")
        
        # å¦‚æœæˆåŠŸè·å–åˆ°æ–°é—»
        if news_items and len(news_items) > 0:
            news_count = len(news_items)
            logger.info(f"[æ–°é—»åˆ†æ] å®æ—¶æ–°é—»èšåˆå™¨æˆåŠŸè·å– {news_count} æ¡ {ticker} çš„æ–°é—»ï¼Œè€—æ—¶ {time_taken:.2f} ç§’")
            
            # è®°å½•ä¸€äº›æ–°é—»æ ‡é¢˜ç¤ºä¾‹
            sample_titles = [item.title for item in news_items[:3]]
            logger.info(f"[æ–°é—»åˆ†æ] æ–°é—»æ ‡é¢˜ç¤ºä¾‹: {', '.join(sample_titles)}")
            
            # æ ¼å¼åŒ–æŠ¥å‘Š
            logger.info(f"[æ–°é—»åˆ†æ] å¼€å§‹æ ¼å¼åŒ–æ–°é—»æŠ¥å‘Š")
            report = aggregator.format_news_report(news_items, ticker)
            logger.info(f"[æ–°é—»åˆ†æ] æŠ¥å‘Šæ ¼å¼åŒ–å®Œæˆï¼Œé•¿åº¦: {len(report)} å­—ç¬¦")
            
            total_time_taken = (datetime.now() - start_total_time).total_seconds()
            logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸç”Ÿæˆ {ticker} çš„æ–°é—»æŠ¥å‘Šï¼Œæ€»è€—æ—¶ {total_time_taken:.2f} ç§’ï¼Œæ–°é—»æ¥æº: å®æ—¶æ–°é—»èšåˆå™¨")
            logger.info(f"[æ–°é—»åˆ†æ] ========== å®æ—¶æ–°é—»èšåˆå™¨è·å–æˆåŠŸï¼Œå‡½æ•°å³å°†è¿”å› ==========")
            return report
        else:
            logger.warning(f"[æ–°é—»åˆ†æ] å®æ—¶æ–°é—»èšåˆå™¨æœªè·å–åˆ° {ticker} çš„æ–°é—»ï¼Œè€—æ—¶ {time_taken:.2f} ç§’ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨æ–°é—»æº")
            # å¦‚æœæ²¡æœ‰è·å–åˆ°æ–°é—»ï¼Œç»§ç»­å°è¯•å¤‡ç”¨æ–¹æ¡ˆ
    except Exception as e:
        logger.error(f"[æ–°é—»åˆ†æ] å®æ—¶æ–°é—»èšåˆå™¨è·å–å¤±è´¥: {e}ï¼Œå°†å°è¯•å¤‡ç”¨æ–°é—»æº")
        logger.error(f"[æ–°é—»åˆ†æ] å¼‚å¸¸è¯¦æƒ…: {type(e).__name__}: {str(e)}")
        import traceback
        logger.error(f"[æ–°é—»åˆ†æ] å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        # å‘ç”Ÿå¼‚å¸¸æ—¶ï¼Œç»§ç»­å°è¯•å¤‡ç”¨æ–¹æ¡ˆ
    
    # å¤‡ç”¨æ–¹æ¡ˆ1: å¯¹äºæ¸¯è‚¡ï¼Œä¼˜å…ˆå°è¯•ä½¿ç”¨ä¸œæ–¹è´¢å¯Œæ–°é—»ï¼ˆAè‚¡å·²åœ¨å‰é¢å¤„ç†ï¼‰
    if not is_china_stock and '.HK' in ticker:
        logger.info(f"[æ–°é—»åˆ†æ] æ£€æµ‹åˆ°æ¸¯è‚¡ä»£ç  {ticker}ï¼Œå°è¯•ä½¿ç”¨ä¸œæ–¹è´¢å¯Œæ–°é—»æº")
        try:
            from .akshare_utils import get_stock_news_em
            
            # å¤„ç†æ¸¯è‚¡ä»£ç 
            clean_ticker = ticker.replace('.HK', '')
            
            logger.info(f"[æ–°é—»åˆ†æ] å¼€å§‹ä»ä¸œæ–¹è´¢å¯Œè·å–æ¸¯è‚¡ {clean_ticker} çš„æ–°é—»æ•°æ®")
            start_time = datetime.now()
            news_df = get_stock_news_em(clean_ticker)
            end_time = datetime.now()
            time_taken = (end_time - start_time).total_seconds()
            
            if not news_df.empty:
                # æ„å»ºç®€å•çš„æ–°é—»æŠ¥å‘Š
                news_count = len(news_df)
                logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸè·å– {news_count} æ¡ä¸œæ–¹è´¢å¯Œæ¸¯è‚¡æ–°é—»ï¼Œè€—æ—¶ {time_taken:.2f} ç§’")
                
                report = f"# {ticker} ä¸œæ–¹è´¢å¯Œæ–°é—»æŠ¥å‘Š\n\n"
                report += f"ğŸ“… ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                report += f"ğŸ“Š æ–°é—»æ€»æ•°: {news_count}æ¡\n"
                report += f"ğŸ•’ è·å–è€—æ—¶: {time_taken:.2f}ç§’\n\n"
                
                # è®°å½•ä¸€äº›æ–°é—»æ ‡é¢˜ç¤ºä¾‹
                sample_titles = [row.get('æ–°é—»æ ‡é¢˜', 'æ— æ ‡é¢˜') for _, row in news_df.head(3).iterrows()]
                logger.info(f"[æ–°é—»åˆ†æ] æ–°é—»æ ‡é¢˜ç¤ºä¾‹: {', '.join(sample_titles)}")
                
                for _, row in news_df.iterrows():
                    report += f"### {row.get('æ–°é—»æ ‡é¢˜', '')}\n"
                    report += f"ğŸ“… {row.get('å‘å¸ƒæ—¶é—´', '')}\n"
                    report += f"ğŸ”— {row.get('æ–°é—»é“¾æ¥', '')}\n\n"
                    report += f"{row.get('æ–°é—»å†…å®¹', 'æ— å†…å®¹')}\n\n"
                
                logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸç”Ÿæˆä¸œæ–¹è´¢å¯Œæ–°é—»æŠ¥å‘Šï¼Œæ–°é—»æ¥æº: ä¸œæ–¹è´¢å¯Œ")
                return report
            else:
                logger.warning(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯Œæœªè·å–åˆ° {clean_ticker} çš„æ–°é—»æ•°æ®ï¼Œè€—æ—¶ {time_taken:.2f} ç§’ï¼Œå°è¯•ä¸‹ä¸€ä¸ªå¤‡ç”¨æ–¹æ¡ˆ")
        except Exception as e:
            logger.error(f"[æ–°é—»åˆ†æ] ä¸œæ–¹è´¢å¯Œæ–°é—»è·å–å¤±è´¥: {e}ï¼Œå°†å°è¯•ä¸‹ä¸€ä¸ªå¤‡ç”¨æ–¹æ¡ˆ")
    
    # å¤‡ç”¨æ–¹æ¡ˆ2: å°è¯•ä½¿ç”¨Googleæ–°é—»
    try:
        from tradingagents.dataflows.interface import get_google_news
        
        # æ ¹æ®è‚¡ç¥¨ç±»å‹æ„å»ºæœç´¢æŸ¥è¯¢
        if stock_type == "Aè‚¡":
            # Aè‚¡ä½¿ç”¨ä¸­æ–‡å…³é”®è¯
            clean_ticker = ticker.replace('.SH', '').replace('.SZ', '').replace('.SS', '')\
                           .replace('.XSHE', '').replace('.XSHG', '')
            search_query = f"{clean_ticker} è‚¡ç¥¨ å…¬å¸ è´¢æŠ¥ æ–°é—»"
            logger.info(f"[æ–°é—»åˆ†æ] å¼€å§‹ä»Googleè·å–Aè‚¡ {clean_ticker} çš„ä¸­æ–‡æ–°é—»æ•°æ®ï¼ŒæŸ¥è¯¢: {search_query}")
        elif stock_type == "æ¸¯è‚¡":
            # æ¸¯è‚¡ä½¿ç”¨ä¸­æ–‡å…³é”®è¯
            clean_ticker = ticker.replace('.HK', '')
            search_query = f"{clean_ticker} æ¸¯è‚¡ å…¬å¸"
            logger.info(f"[æ–°é—»åˆ†æ] å¼€å§‹ä»Googleè·å–æ¸¯è‚¡ {clean_ticker} çš„æ–°é—»æ•°æ®ï¼ŒæŸ¥è¯¢: {search_query}")
        else:
            # ç¾è‚¡ä½¿ç”¨è‹±æ–‡å…³é”®è¯
            search_query = f"{ticker} stock news"
            logger.info(f"[æ–°é—»åˆ†æ] å¼€å§‹ä»Googleè·å– {ticker} çš„æ–°é—»æ•°æ®ï¼ŒæŸ¥è¯¢: {search_query}")
        
        start_time = datetime.now()
        google_news = get_google_news(search_query, curr_date, 1)
        end_time = datetime.now()
        time_taken = (end_time - start_time).total_seconds()
        
        if google_news and len(google_news.strip()) > 0:
            # ä¼°ç®—è·å–çš„æ–°é—»æ•°é‡
            news_lines = google_news.strip().split('\n')
            news_count = sum(1 for line in news_lines if line.startswith('###'))
            
            logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸè·å– Google æ–°é—»ï¼Œä¼°è®¡ {news_count} æ¡æ–°é—»ï¼Œè€—æ—¶ {time_taken:.2f} ç§’")
            
            # è®°å½•ä¸€äº›æ–°é—»æ ‡é¢˜ç¤ºä¾‹
            sample_titles = [line.replace('### ', '') for line in news_lines if line.startswith('### ')][:3]
            if sample_titles:
                logger.info(f"[æ–°é—»åˆ†æ] æ–°é—»æ ‡é¢˜ç¤ºä¾‹: {', '.join(sample_titles)}")
                
            logger.info(f"[æ–°é—»åˆ†æ] æˆåŠŸç”Ÿæˆ Google æ–°é—»æŠ¥å‘Šï¼Œæ–°é—»æ¥æº: Google")
            return google_news
        else:
            logger.warning(f"[æ–°é—»åˆ†æ] Google æ–°é—»æœªè·å–åˆ° {ticker} çš„æ–°é—»æ•°æ®ï¼Œè€—æ—¶ {time_taken:.2f} ç§’")
    except Exception as e:
        logger.error(f"[æ–°é—»åˆ†æ] Google æ–°é—»è·å–å¤±è´¥: {e}ï¼Œæ‰€æœ‰å¤‡ç”¨æ–¹æ¡ˆå‡å·²å°è¯•")
    
    # æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
    total_time_taken = (datetime.now() - start_total_time).total_seconds()
    logger.error(f"[æ–°é—»åˆ†æ] {ticker} çš„æ‰€æœ‰æ–°é—»è·å–æ–¹æ³•å‡å·²å¤±è´¥ï¼Œæ€»è€—æ—¶ {total_time_taken:.2f} ç§’")
    
    # è®°å½•è¯¦ç»†çš„å¤±è´¥ä¿¡æ¯
    failure_details = {
        "è‚¡ç¥¨ä»£ç ": ticker,
        "è‚¡ç¥¨ç±»å‹": stock_type,
        "åˆ†ææ—¥æœŸ": curr_date,
        "å›æº¯æ—¶é—´": f"{hours_back}å°æ—¶",
        "æ€»è€—æ—¶": f"{total_time_taken:.2f}ç§’"
    }
    logger.error(f"[æ–°é—»åˆ†æ] æ–°é—»è·å–å¤±è´¥è¯¦æƒ…: {failure_details}")
    
    return f"""
å®æ—¶æ–°é—»è·å–å¤±è´¥ - {ticker}
åˆ†ææ—¥æœŸ: {curr_date}

âŒ é”™è¯¯ä¿¡æ¯: æ‰€æœ‰å¯ç”¨çš„æ–°é—»æºéƒ½æœªèƒ½è·å–åˆ°ç›¸å…³æ–°é—»

ğŸ’¡ å¤‡ç”¨å»ºè®®:
1. æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIå¯†é’¥é…ç½®
2. ä½¿ç”¨åŸºç¡€æ–°é—»åˆ†æä½œä¸ºå¤‡é€‰
3. å…³æ³¨å®˜æ–¹è´¢ç»åª’ä½“çš„æœ€æ–°æŠ¥é“
4. è€ƒè™‘ä½¿ç”¨ä¸“ä¸šé‡‘èç»ˆç«¯è·å–å®æ—¶æ–°é—»

æ³¨: å®æ—¶æ–°é—»è·å–ä¾èµ–å¤–éƒ¨APIæœåŠ¡çš„å¯ç”¨æ€§ã€‚
"""
