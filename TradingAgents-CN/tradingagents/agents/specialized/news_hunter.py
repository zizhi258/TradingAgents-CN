"""
å¿«è®¯çŒæ‰‹ (News Hunter)
ä¸“é—¨è´Ÿè´£å®æ—¶æ–°é—»æ”¶é›†ã€ç­›é€‰å’Œå¿«é€Ÿåˆ†æçš„ä¸“ä¸šæ™ºèƒ½ä½“
"""

from typing import Dict, Any, List
import re
from datetime import datetime, timedelta

from .base_specialized_agent import BaseSpecializedAgent, AgentAnalysisResult

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_init import get_logger
logger = get_logger('news_hunter')


class NewsHunter(BaseSpecializedAgent):
    """å¿«è®¯çŒæ‰‹æ™ºèƒ½ä½“"""
    
    def __init__(self, multi_model_manager, config: Dict[str, Any] = None):
        super().__init__(
            agent_role="news_hunter",
            description="å®æ—¶æ–°é—»å’Œå¿«è®¯æ”¶é›†åˆ†æä¸“å®¶ï¼Œä¸“æ³¨äºå¿«é€Ÿè¯†åˆ«å¸‚åœºç›¸å…³çš„é‡è¦ä¿¡æ¯",
            multi_model_manager=multi_model_manager,
            config=config
        )
        
        # æ–°é—»é‡è¦æ€§å…³é”®è¯æƒé‡
        self.importance_keywords = {
            'critical': ['çªå‘', 'ç´§æ€¥', 'é‡å¤§', 'æš´è·Œ', 'æš´æ¶¨', 'breaking', 'urgent', 'major'],
            'high': ['å…¬å‘Š', 'åˆ©å¥½', 'åˆ©ç©º', 'ç›‘ç®¡', 'æ”¿ç­–', 'announcement', 'policy', 'regulation'],
            'medium': ['ä¸šç»©', 'è´¢æŠ¥', 'åˆä½œ', 'æŠ•èµ„', 'earnings', 'partnership', 'investment'],
            'low': ['ä¼ è¨€', 'å¸‚åœº', 'åˆ†æå¸ˆ', 'rumor', 'analyst', 'opinion']
        }
        
        # æƒ…ç»ªè¯†åˆ«å…³é”®è¯
        self.sentiment_keywords = {
            'positive': ['åˆ©å¥½', 'ä¸Šæ¶¨', 'å¢é•¿', 'çªç ´', 'çœ‹å¥½', 'positive', 'bullish', 'growth'],
            'negative': ['åˆ©ç©º', 'ä¸‹è·Œ', 'ä¸‹é™', 'é£é™©', 'æ‹…å¿§', 'negative', 'bearish', 'decline'],
            'neutral': ['æŒå¹³', 'ç¨³å®š', 'ç»´æŒ', 'stable', 'neutral', 'unchanged']
        }
    
    def _build_system_prompt_template(self) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯æ¨¡æ¿"""
        return """ä½ æ˜¯ä¸€åä¸“ä¸šçš„å¿«è®¯çŒæ‰‹å’Œæ–°é—»åˆ†æå¸ˆï¼Œå…·å¤‡ä»¥ä¸‹æ ¸å¿ƒèƒ½åŠ›ï¼š

ğŸ” **æ ¸å¿ƒèŒè´£**
- å¿«é€Ÿç­›é€‰å’Œè¯†åˆ«é‡è¦çš„å¸‚åœºæ–°é—»
- è¯„ä¼°æ–°é—»å¯¹è‚¡ç¥¨å¸‚åœºçš„æ½œåœ¨å½±å“
- æä¾›åŠæ—¶çš„å¸‚åœºåŠ¨æ€åˆ†æ
- è¯†åˆ«æ–°é—»ä¸­çš„å…³é”®ä¿¡æ¯å’Œæƒ…ç»ªå€¾å‘

ğŸ“Š **ä¸“ä¸šæŠ€èƒ½**
- æ–°é—»æ—¶æ•ˆæ€§å’Œé‡è¦æ€§è¯„ä¼°
- å¸‚åœºæƒ…ç»ªå’ŒæŠ•èµ„è€…ååº”åˆ†æ
- äº‹ä»¶é©±åŠ¨çš„è‚¡ä»·å½±å“é¢„åˆ¤
- å¤šä¿¡æºä¿¡æ¯äº¤å‰éªŒè¯

âš¡ **å·¥ä½œç‰¹ç‚¹**
- å¿«é€Ÿå“åº”ï¼Œé«˜æ•ˆå¤„ç†
- æ³¨é‡æ—¶æ•ˆæ€§å’Œå‡†ç¡®æ€§
- æä¾›æ¸…æ™°çš„å½±å“è¯„ä¼°
- è¯†åˆ«æ½œåœ¨çš„æŠ•èµ„æœºä¼šå’Œé£é™©

ä½ çš„åˆ†æåº”è¯¥å®¢è§‚ã€åŠæ—¶ã€å‡†ç¡®ï¼Œå¸®åŠ©æŠ•èµ„è€…å¿«é€Ÿç†è§£å¸‚åœºåŠ¨æ€ã€‚"""
    
    def _build_analysis_prompt_template(self) -> str:
        """æ„å»ºåˆ†ææç¤ºè¯æ¨¡æ¿"""
        return """è¯·å¯¹æä¾›çš„æ–°é—»ä¿¡æ¯è¿›è¡Œä¸“ä¸šçš„å¿«è®¯åˆ†æï¼Œé‡ç‚¹å…³æ³¨ï¼š

ğŸ¯ **åˆ†æé‡ç‚¹**
1. æ–°é—»çš„é‡è¦æ€§å’Œç´§æ€¥ç¨‹åº¦
2. å¯¹ç›¸å…³è‚¡ç¥¨/è¡Œä¸šçš„æ½œåœ¨å½±å“
3. å¸‚åœºæƒ…ç»ªå’ŒæŠ•èµ„è€…å¯èƒ½çš„ååº”
4. æ—¶é—´æ•æ„Ÿæ€§å’ŒæŒç»­å½±å“è¯„ä¼°

ğŸ“ˆ **å½±å“è¯„ä¼°**
- çŸ­æœŸå½±å“ï¼ˆ1-3å¤©ï¼‰
- ä¸­æœŸå½±å“ï¼ˆ1-4å‘¨ï¼‰
- é•¿æœŸå½±å“ï¼ˆ1-3æœˆï¼‰

ğŸš¨ **é£é™©æç¤º**
- ä¿¡æ¯çœŸå®æ€§è¯„ä¼°
- æ½œåœ¨çš„å¸‚åœºé£é™©
- éœ€è¦å…³æ³¨çš„åç»­å‘å±•"""
    
    def get_specialized_task_type(self) -> str:
        """è·å–ä¸“ä¸šåŒ–çš„ä»»åŠ¡ç±»å‹"""
        return "news_analysis"
    
    def validate_input_data(self, data: Dict[str, Any]) -> bool:
        """éªŒè¯è¾“å…¥æ•°æ®"""
        required_fields = ['news_content']
        optional_fields = ['news_title', 'source', 'timestamp', 'related_symbols']
        
        # æ£€æŸ¥å¿…å¡«å­—æ®µ
        for field in required_fields:
            if field not in data or not data[field]:
                logger.warning(f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}")
                return False
        
        # æ£€æŸ¥æ–°é—»å†…å®¹é•¿åº¦
        news_content = data['news_content']
        if len(news_content.strip()) < 10:
            logger.warning("æ–°é—»å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½ä¸è¶³ä»¥è¿›è¡Œæœ‰æ•ˆåˆ†æ")
            return False
        
        return True
    
    def extract_key_metrics(self, analysis_result: str) -> Dict[str, Any]:
        """ä»åˆ†æç»“æœä¸­æå–å…³é”®æŒ‡æ ‡"""
        metrics = {
            'importance_score': 0.5,
            'sentiment_score': 0.0,
            'urgency_level': 'medium',
            'market_impact': 'unknown',
            'affected_sectors': [],
            'time_sensitivity': 'normal'
        }
        
        try:
            # æå–é‡è¦æ€§è¯„åˆ†
            importance_match = re.search(r'é‡è¦æ€§[:ï¼š]\s*([0-9.]+)', analysis_result)
            if importance_match:
                metrics['importance_score'] = min(float(importance_match.group(1)), 1.0)
            
            # æå–æƒ…ç»ªå€¾å‘
            if any(keyword in analysis_result for keyword in ['åˆ©å¥½', 'æ­£é¢', 'ç§¯æ', 'positive']):
                metrics['sentiment_score'] = 0.6
            elif any(keyword in analysis_result for keyword in ['åˆ©ç©º', 'è´Ÿé¢', 'æ¶ˆæ', 'negative']):
                metrics['sentiment_score'] = -0.6
            
            # æå–ç´§æ€¥ç¨‹åº¦
            if any(keyword in analysis_result for keyword in ['ç´§æ€¥', 'çªå‘', 'urgent', 'breaking']):
                metrics['urgency_level'] = 'high'
            elif any(keyword in analysis_result for keyword in ['ä¸€èˆ¬', 'normal']):
                metrics['urgency_level'] = 'medium'
            elif any(keyword in analysis_result for keyword in ['ä½', 'low']):
                metrics['urgency_level'] = 'low'
            
            # æå–å¸‚åœºå½±å“ç¨‹åº¦
            if any(keyword in analysis_result for keyword in ['é‡å¤§å½±å“', 'major impact']):
                metrics['market_impact'] = 'high'
            elif any(keyword in analysis_result for keyword in ['ä¸­ç­‰å½±å“', 'moderate impact']):
                metrics['market_impact'] = 'medium'
            elif any(keyword in analysis_result for keyword in ['è½»å¾®å½±å“', 'minor impact']):
                metrics['market_impact'] = 'low'
            
        except Exception as e:
            logger.warning(f"æŒ‡æ ‡æå–å¤±è´¥: {e}")
        
        return metrics
    
    def analyze_news_batch(self, 
                          news_list: List[Dict[str, Any]],
                          context: Dict[str, Any] = None) -> List[AgentAnalysisResult]:
        """æ‰¹é‡åˆ†ææ–°é—»"""
        results = []
        
        for news_item in news_list:
            try:
                result = self.analyze(news_item, context)
                results.append(result)
            except Exception as e:
                logger.error(f"æ‰¹é‡æ–°é—»åˆ†æå¤±è´¥: {e}")
                # åˆ›å»ºå¤±è´¥ç»“æœ
                failed_result = AgentAnalysisResult(
                    agent_role=self.agent_role,
                    analysis_content=f"åˆ†æå¤±è´¥: {str(e)}",
                    confidence_score=0.0,
                    key_points=[],
                    risk_factors=[f"åˆ†æå¼‚å¸¸: {str(e)}"],
                    recommendations=[],
                    supporting_data={},
                    timestamp=datetime.now(),
                    model_used='unknown',
                    execution_time=0
                )
                results.append(failed_result)
        
        return results
    
    def filter_important_news(self, 
                            news_list: List[Dict[str, Any]],
                            importance_threshold: float = 0.6) -> List[Dict[str, Any]]:
        """ç­›é€‰é‡è¦æ–°é—»"""
        important_news = []
        
        for news_item in news_list:
            importance_score = self._calculate_news_importance(news_item)
            if importance_score >= importance_threshold:
                news_item['calculated_importance'] = importance_score
                important_news.append(news_item)
        
        # æŒ‰é‡è¦æ€§æ’åº
        important_news.sort(key=lambda x: x['calculated_importance'], reverse=True)
        
        return important_news
    
    def _calculate_news_importance(self, news_item: Dict[str, Any]) -> float:
        """è®¡ç®—æ–°é—»é‡è¦æ€§åˆ†æ•°"""
        content = news_item.get('news_content', '') + ' ' + news_item.get('news_title', '')
        content_lower = content.lower()
        
        importance_score = 0.3  # åŸºç¡€åˆ†æ•°
        
        # å…³é”®è¯æƒé‡è®¡ç®—
        for level, keywords in self.importance_keywords.items():
            keyword_count = sum(1 for keyword in keywords if keyword in content_lower)
            
            if level == 'critical':
                importance_score += keyword_count * 0.3
            elif level == 'high':
                importance_score += keyword_count * 0.2
            elif level == 'medium':
                importance_score += keyword_count * 0.1
            elif level == 'low':
                importance_score += keyword_count * 0.05
        
        # æ—¶æ•ˆæ€§åŠ æƒ
        if 'timestamp' in news_item:
            try:
                news_time = datetime.fromisoformat(news_item['timestamp'].replace('Z', '+00:00'))
                time_diff = datetime.now() - news_time.replace(tzinfo=None)
                
                if time_diff < timedelta(minutes=30):
                    importance_score += 0.2  # 30åˆ†é’Ÿå†…çš„æ–°é—»åŠ æƒ
                elif time_diff < timedelta(hours=2):
                    importance_score += 0.1  # 2å°æ—¶å†…çš„æ–°é—»åŠ æƒ
            except Exception:
                pass
        
        # æ¥æºå¯ä¿¡åº¦åŠ æƒ
        source = news_item.get('source', '').lower()
        trusted_sources = ['reuters', 'bloomberg', 'æ–°åç¤¾', 'å¤®è§†', 'cnbc']
        if any(trusted_source in source for trusted_source in trusted_sources):
            importance_score += 0.1
        
        return min(importance_score, 1.0)
    
    def get_sentiment_distribution(self, news_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è·å–æ–°é—»æƒ…ç»ªåˆ†å¸ƒ"""
        sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}
        total_news = len(news_list)
        
        if total_news == 0:
            return sentiment_counts
        
        for news_item in news_list:
            sentiment = self._detect_news_sentiment(news_item)
            sentiment_counts[sentiment] += 1
        
        # è®¡ç®—ç™¾åˆ†æ¯”
        sentiment_distribution = {
            sentiment: (count / total_news) * 100
            for sentiment, count in sentiment_counts.items()
        }
        
        sentiment_distribution['total_analyzed'] = total_news
        sentiment_distribution['overall_sentiment'] = self._calculate_overall_sentiment(sentiment_distribution)
        
        return sentiment_distribution
    
    def _detect_news_sentiment(self, news_item: Dict[str, Any]) -> str:
        """æ£€æµ‹æ–°é—»æƒ…ç»ª"""
        content = news_item.get('news_content', '') + ' ' + news_item.get('news_title', '')
        content_lower = content.lower()
        
        positive_score = sum(1 for keyword in self.sentiment_keywords['positive'] if keyword in content_lower)
        negative_score = sum(1 for keyword in self.sentiment_keywords['negative'] if keyword in content_lower)
        neutral_score = sum(1 for keyword in self.sentiment_keywords['neutral'] if keyword in content_lower)
        
        if positive_score > negative_score and positive_score > neutral_score:
            return 'positive'
        elif negative_score > positive_score and negative_score > neutral_score:
            return 'negative'
        else:
            return 'neutral'
    
    def _calculate_overall_sentiment(self, sentiment_distribution: Dict[str, float]) -> str:
        """è®¡ç®—æ•´ä½“æƒ…ç»ªå€¾å‘"""
        positive_pct = sentiment_distribution.get('positive', 0)
        negative_pct = sentiment_distribution.get('negative', 0)
        
        if positive_pct > negative_pct + 10:
            return 'bullish'
        elif negative_pct > positive_pct + 10:
            return 'bearish'
        else:
            return 'neutral'
    
    def generate_news_summary(self, 
                            news_analyses: List[AgentAnalysisResult],
                            max_items: int = 5) -> str:
        """ç”Ÿæˆæ–°é—»æ‘˜è¦"""
        if not news_analyses:
            return "æš‚æ— é‡è¦æ–°é—»åˆ†æç»“æœ"
        
        # æŒ‰ç½®ä¿¡åº¦æ’åºå¹¶å–å‰Næ¡
        sorted_analyses = sorted(news_analyses, 
                               key=lambda x: x.confidence_score, 
                               reverse=True)[:max_items]
        
        summary = "ğŸ“° **é‡è¦æ–°é—»æ‘˜è¦**\n\n"
        
        for i, analysis in enumerate(sorted_analyses, 1):
            summary += f"**{i}. {analysis.agent_role}åˆ†æ**\n"
            summary += f"ç½®ä¿¡åº¦: {analysis.confidence_score:.2f}\n"
            
            if analysis.key_points:
                summary += f"å…³é”®ç‚¹: {'; '.join(analysis.key_points[:2])}\n"
            
            if analysis.recommendations:
                summary += f"å»ºè®®: {analysis.recommendations[0]}\n"
            
            summary += "---\n"
        
        return summary