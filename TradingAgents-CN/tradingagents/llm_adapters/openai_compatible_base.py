"""
OpenAIå…¼å®¹é€‚é…å™¨åŸºç±»
ä¸ºæ‰€æœ‰æ”¯æŒOpenAIæ¥å£çš„LLMæä¾›å•†æä¾›ç»Ÿä¸€çš„åŸºç¡€å®ç°
"""

import os
import time
from typing import Any, Dict, List, Optional, Union
from langchain_core.messages import BaseMessage
from langchain_core.outputs import ChatResult
from langchain_openai import ChatOpenAI
from langchain_core.callbacks import CallbackManagerForLLMRun

# å¯¼å…¥ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
from tradingagents.utils.logging_manager import get_logger, get_logger_manager
logger = get_logger('llm_adapters')

# å¯¼å…¥tokenè·Ÿè¸ªå™¨
try:
    from tradingagents.config.config_manager import token_tracker
    TOKEN_TRACKING_ENABLED = True
    logger.info("âœ… Tokenè·Ÿè¸ªåŠŸèƒ½å·²å¯ç”¨")
except ImportError:
    TOKEN_TRACKING_ENABLED = False
    logger.warning("âš ï¸ Tokenè·Ÿè¸ªåŠŸèƒ½æœªå¯ç”¨")


class OpenAICompatibleBase(ChatOpenAI):
    """
    OpenAIå…¼å®¹é€‚é…å™¨åŸºç±»
    ä¸ºæ‰€æœ‰æ”¯æŒOpenAIæ¥å£çš„LLMæä¾›å•†æä¾›ç»Ÿä¸€å®ç°
    """
    
    def __init__(
        self,
        provider_name: str,
        model: str,
        api_key_env_var: str,
        base_url: str,
        api_key: Optional[str] = None,
        temperature: float = 0.1,
        max_tokens: Optional[int] = None,
        **kwargs
    ):
        """
        åˆå§‹åŒ–OpenAIå…¼å®¹é€‚é…å™¨
        
        Args:
            provider_name: æä¾›å•†åç§° (å¦‚: "deepseek", "dashscope")
            model: æ¨¡å‹åç§°
            api_key_env_var: APIå¯†é’¥ç¯å¢ƒå˜é‡å
            base_url: APIåŸºç¡€URL
            api_key: APIå¯†é’¥ï¼Œå¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡è·å–
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°
        """
        
        self.provider_name = provider_name
        self.model_name = model
        
        # è·å–APIå¯†é’¥
        if api_key is None:
            api_key = os.getenv(api_key_env_var)
            if not api_key:
                raise ValueError(
                    f"{provider_name} APIå¯†é’¥æœªæ‰¾åˆ°ã€‚"
                    f"è¯·è®¾ç½®{api_key_env_var}ç¯å¢ƒå˜é‡æˆ–ä¼ å…¥api_keyå‚æ•°ã€‚"
                )
        
        # è®¾ç½®OpenAIå…¼å®¹å‚æ•°
        openai_kwargs = {
            "model": model,
            "temperature": temperature,
            "max_tokens": max_tokens,
            **kwargs
        }
        
        # æ ¹æ®LangChainç‰ˆæœ¬ä½¿ç”¨ä¸åŒçš„å‚æ•°å
        try:
            # æ–°ç‰ˆæœ¬LangChain
            openai_kwargs.update({
                "api_key": api_key,
                "base_url": base_url
            })
        except:
            # æ—§ç‰ˆæœ¬LangChain
            openai_kwargs.update({
                "openai_api_key": api_key,
                "openai_api_base": base_url
            })
        
        # åˆå§‹åŒ–çˆ¶ç±»
        super().__init__(**openai_kwargs)

        logger.info(f"âœ… {provider_name} OpenAIå…¼å®¹é€‚é…å™¨åˆå§‹åŒ–æˆåŠŸ")
        logger.info(f"   æ¨¡å‹: {model}")
        logger.info(f"   API Base: {base_url}")
    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """
        ç”ŸæˆèŠå¤©å“åº”ï¼Œå¹¶è®°å½•tokenä½¿ç”¨é‡
        """
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # è°ƒç”¨çˆ¶ç±»ç”Ÿæˆæ–¹æ³•
        result = super()._generate(messages, stop, run_manager, **kwargs)
        
        # è®°å½•tokenä½¿ç”¨é‡
        if TOKEN_TRACKING_ENABLED:
            try:
                self._track_token_usage(result, kwargs, start_time)
            except Exception as e:
                logger.error(f"âš ï¸ {self.provider_name} Tokenè¿½è¸ªå¤±è´¥: {e}", exc_info=True)
        
        return result
    
    def _track_token_usage(self, result: ChatResult, kwargs: Dict, start_time: float):
        """è¿½è¸ªtokenä½¿ç”¨é‡"""
        
        # æå–tokenä½¿ç”¨ä¿¡æ¯
        if hasattr(result, 'llm_output') and result.llm_output:
            token_usage = result.llm_output.get('token_usage', {})
            
            input_tokens = token_usage.get('prompt_tokens', 0)
            output_tokens = token_usage.get('completion_tokens', 0)
            
            if input_tokens > 0 or output_tokens > 0:
                # ç”Ÿæˆä¼šè¯ID
                session_id = kwargs.get('session_id', f"{self.provider_name}_{hash(str(kwargs))%10000}")
                analysis_type = kwargs.get('analysis_type', 'stock_analysis')
                
                # è®°å½•ä½¿ç”¨é‡
                token_tracker.track_usage(
                    provider=self.provider_name,
                    model_name=self.model_name,
                    input_tokens=input_tokens,
                    output_tokens=output_tokens,
                    session_id=session_id,
                    analysis_type=analysis_type
                )
                
                # è®¡ç®—æˆæœ¬
                cost = token_tracker.calculate_cost(
                    provider=self.provider_name,
                    model_name=self.model_name,
                    input_tokens=input_tokens,
                    output_tokens=output_tokens
                )
                
                # ä½¿ç”¨ç»Ÿä¸€æ—¥å¿—ç®¡ç†å™¨è®°å½•Tokenä½¿ç”¨
                logger_manager = get_logger_manager()
                logger_manager.log_token_usage(
                    logger, self.provider_name, self.model_name,
                    input_tokens, output_tokens, cost,
                    session_id
                )


class ChatDeepSeekOpenAI(OpenAICompatibleBase):
    """DeepSeek OpenAIå…¼å®¹é€‚é…å™¨"""
    
    def __init__(
        self,
        model: str = "deepseek-chat",
        api_key: Optional[str] = None,
        temperature: float = 0.1,
        max_tokens: Optional[int] = None,
        **kwargs
    ):
        super().__init__(
            provider_name="deepseek",
            model=model,
            api_key_env_var="DEEPSEEK_API_KEY",
            base_url="https://api.deepseek.com",
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )


# DashScope (é˜¿é‡Œç™¾ç‚¼) å·²åºŸå¼ƒï¼šç§»é™¤ç›¸å…³é€‚é…å™¨


# æ”¯æŒçš„OpenAIå…¼å®¹æ¨¡å‹é…ç½®
OPENAI_COMPATIBLE_PROVIDERS = {
    "deepseek": {
        "adapter_class": ChatDeepSeekOpenAI,
        "base_url": "https://api.deepseek.com",
        "api_key_env": "DEEPSEEK_API_KEY",
        "models": {
            "deepseek-chat": {"context_length": 65536, "supports_function_calling": True},  # DeepSeek V3: 64K context
            "deepseek-reasoner": {"context_length": 65536, "supports_function_calling": True}  # DeepSeek R1: 64K context
        }
    },
    "siliconflow": {
        "adapter_class": ChatOpenAI,
        "base_url": "https://api.siliconflow.cn/v1",
        "api_key_env": "SILICONFLOW_API_KEY",
        "models": {
            "deepseek-ai/DeepSeek-R1": {"context_length": 163840, "supports_function_calling": True},  # R1 æ¨ç†æ¨¡å‹
            "deepseek-ai/DeepSeek-V3": {"context_length": 131072, "supports_function_calling": True},  # V3 é€šç”¨æ¨¡å‹
            "zai-org/GLM-4.5": {"context_length": 131072, "supports_function_calling": True},  # GLM-4.5
            "Qwen/Qwen3-Coder-480B-A35B-Instruct": {"context_length": 262144, "supports_function_calling": True},  # Qwen3 Coder
            "moonshotai/Kimi-K2-Instruct": {"context_length": 131072, "supports_function_calling": True},  # Kimi K2
            "Qwen/Qwen3-235B-A22B-Thinking-2507": {"context_length": 262144, "supports_function_calling": True},  # Qwen3 Thinking
            "Qwen/Qwen3-235B-A22B-Instruct-2507": {"context_length": 262144, "supports_function_calling": True},  # Qwen3 Instruct
            "Qwen/Qwen3-Embedding-8B": {"context_length": 8192, "supports_function_calling": False},  # Embeddingæ¨¡å‹
            "Qwen/Qwen3-Reranker-8B": {"context_length": 8192, "supports_function_calling": False}  # Rerankeræ¨¡å‹
        }
    },
    # æ³¨æ„ï¼šä¸å†åŒ…å« dashscope
}


def create_openai_compatible_llm(
    provider: str,
    model: str,
    api_key: Optional[str] = None,
    temperature: float = 0.1,
    max_tokens: Optional[int] = None,
    **kwargs
) -> OpenAICompatibleBase:
    """
    åˆ›å»ºOpenAIå…¼å®¹LLMå®ä¾‹çš„ç»Ÿä¸€å·¥å‚å‡½æ•°
    
    Args:
        provider: æä¾›å•†åç§° ("deepseek", "dashscope")
        model: æ¨¡å‹åç§°
        api_key: APIå¯†é’¥
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§tokenæ•°
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        OpenAIå…¼å®¹çš„LLMå®ä¾‹
    """
    
    if provider not in OPENAI_COMPATIBLE_PROVIDERS:
        raise ValueError(f"ä¸æ”¯æŒçš„æä¾›å•†: {provider}ã€‚æ”¯æŒçš„æä¾›å•†: {list(OPENAI_COMPATIBLE_PROVIDERS.keys())}")
    
    provider_config = OPENAI_COMPATIBLE_PROVIDERS[provider]
    adapter_class = provider_config["adapter_class"]
    
    return adapter_class(
        model=model,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        **kwargs
    )


def test_openai_compatible_adapters():
    """æµ‹è¯•æ‰€æœ‰OpenAIå…¼å®¹é€‚é…å™¨"""

    logger.info("ğŸ§ª æµ‹è¯•OpenAIå…¼å®¹é€‚é…å™¨")
    logger.info("=" * 50)

    for provider_name, config in OPENAI_COMPATIBLE_PROVIDERS.items():
        logger.info(f"\nğŸ”§ æµ‹è¯• {provider_name}...")

        try:
            # è·å–ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
            first_model = list(config["models"].keys())[0]

            # åˆ›å»ºé€‚é…å™¨
            llm = create_openai_compatible_llm(
                provider=provider_name,
                model=first_model,
                max_tokens=100
            )

            logger.info(f"âœ… {provider_name} é€‚é…å™¨åˆ›å»ºæˆåŠŸ")

            # æµ‹è¯•å·¥å…·ç»‘å®š
            from langchain_core.tools import tool

            @tool
            def test_tool(text: str) -> str:
                """æµ‹è¯•å·¥å…·"""
                return f"å·¥å…·è¿”å›: {text}"

            llm_with_tools = llm.bind_tools([test_tool])
            logger.info(f"âœ… {provider_name} å·¥å…·ç»‘å®šæˆåŠŸ")

        except Exception as e:
            logger.error(f"âŒ {provider_name} æµ‹è¯•å¤±è´¥: {e}", exc_info=True)


if __name__ == "__main__":
    test_openai_compatible_adapters()
