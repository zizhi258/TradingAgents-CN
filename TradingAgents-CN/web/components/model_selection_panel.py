"""
å¯å¤ç”¨çš„AIæ¨¡å‹é€‰æ‹©é¢æ¿ç»„ä»¶
ä»ä¾§è¾¹æ æå–çš„æ¨¡å‹é…ç½®é€»è¾‘ï¼Œå¯åœ¨é¡µé¢ä¸»å†…å®¹åŒºåŸŸä½¿ç”¨
"""

import streamlit as st
import os
import logging
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from web.utils.persistence import load_model_selection, save_model_selection
from web.utils.model_catalog import (
    get_siliconflow_models,
    get_deepseek_models,
    get_google_models,
    get_openrouter_models,
    get_gemini_api_models,
)
from web.components.custom_model_helper import render_model_help, validate_custom_model_name

logger = logging.getLogger(__name__)

def render_routing_section(location: str = "main") -> tuple:
    """ä»…æ¸²æŸ“è·¯ç”±/å›é€€/é¢„ç®—ï¼Œè¿”å› (routing_strategy, fallbacks_structured, max_budget)"""
    st.markdown("### ğŸ§­ è·¯ç”±ç­–ç•¥ä¸å›é€€")

    routing_strategy = st.selectbox(
        "è·¯ç”±ç­–ç•¥",
        options=["è´¨é‡ä¼˜å…ˆ", "æ—¶å»¶ä¼˜å…ˆ", "æˆæœ¬ä¼˜å…ˆ", "å‡è¡¡"],
        key=f"{location}_routing_strategy_select"
    )

    enable_auto_fallback = st.checkbox(
        "å¤±è´¥è‡ªåŠ¨é™çº§",
        value=True,
        key=f"{location}_enable_auto_fallback"
    )

    # åŠ¨æ€ç”Ÿæˆï¼ˆProvider:Modelï¼‰
    ds = [f"deepseek:{m}" for m in get_deepseek_models()]
    gg = [f"google:{m}" for m in get_google_models()]
    sf = [f"siliconflow:{m}" for m in get_siliconflow_models()]
    # ä¸ºé¿å…è¿‡é•¿ï¼Œå¯æŒ‰éœ€è£å‰ªï¼›æ­¤å¤„å…¨é‡åˆ—å‡º
    fallback_choices_catalog = ds + gg + sf

    selected_fallbacks = st.multiselect(
        "å›é€€å€™é€‰ï¼ˆä»ä¸Šåˆ°ä¸‹ä¼˜å…ˆï¼‰",
        options=fallback_choices_catalog,
        default=([ds[0], (gg[1] if len(gg) > 1 else gg[0])] if (enable_auto_fallback and ds and gg) else []),
        key=f"{location}_selected_fallbacks"
    )

    # é¢„ç®—ä¸Šé™
    max_budget = st.number_input(
        "æ¯æ¬¡åˆ†ææˆæœ¬ä¸Šé™(Â¥)",
        min_value=0.0,
        value=0.0,
        step=0.1,
        key=f"{location}_max_budget"
    )

    # è§£æä¸ºç»“æ„åŒ–
    def parse_fb(item: str):
        try:
            p, m = item.split(":", 1)
            return {"provider": p, "model": m}
        except Exception:
            return None

    fallbacks_structured = [x for x in (parse_fb(i) for i in selected_fallbacks) if x]

    # å†™å…¥ä¼šè¯æ€ï¼Œä¾›å…¶ä»–åŒºåŸŸä½¿ç”¨
    st.session_state.routing_strategy_select = routing_strategy
    st.session_state.fallback_chain = fallbacks_structured
    st.session_state.max_budget = max_budget

    return routing_strategy, fallbacks_structured, max_budget


def render_advanced_overrides_section(location: str = "main") -> dict:
    """ä»…æ¸²æŸ“é«˜çº§ä¸è§’è‰²è¦†ç›–è®¾ç½®ï¼Œè¿”å›é…ç½®å­—å…¸"""
    with st.expander("âš™ï¸ é«˜çº§è®¾ç½®"):
        enable_memory = st.checkbox(
            "å¯ç”¨è®°å¿†åŠŸèƒ½",
            value=False,
            key=f"{location}_enable_memory"
        )

        enable_debug = st.checkbox(
            "è°ƒè¯•æ¨¡å¼",
            value=False,
            key=f"{location}_enable_debug"
        )

        max_tokens = st.slider(
            "æœ€å¤§è¾“å‡ºé•¿åº¦",
            min_value=1000,
            max_value=128000,
            value=32000,
            step=1000,
            key=f"{location}_max_tokens"
        )

        st.markdown("#### ğŸ‘¥ å›¢é˜Ÿç­–ç•¥ï¼ˆæŒ‰è§’è‰²æ§åˆ¶å¯ç”¨æ¨¡å‹ï¼‰")
        roles = [
            ("fundamental_expert", "åŸºæœ¬é¢ä¸“å®¶"),
            ("chief_decision_officer", "é¦–å¸­å†³ç­–å®˜"),
            ("technical_analyst", "æŠ€æœ¯åˆ†æå¸ˆ"),
            ("news_hunter", "å¿«è®¯çŒæ‰‹"),
            ("sentiment_analyst", "æƒ…ç»ªåˆ†æå¸ˆ"),
            ("tool_engineer", "å·¥å…·å·¥ç¨‹å¸ˆ"),
        ]

        # ç»Ÿä¸€æ¥æºï¼šä¸‰å®¶ä¾›åº”å•†æ±‡æ€»
        siliconflow_catalog = get_siliconflow_models()
        deepseek_catalog = get_deepseek_models()
        google_catalog = get_google_models()
        unified_catalog = siliconflow_catalog + google_catalog + deepseek_catalog

        allowed_models_by_role = {}
        model_overrides = {}
        for role_key, role_label in roles:
            with st.expander(f"{role_label} ({role_key})", expanded=False):
                allowed = st.multiselect(
                    f"å…è®¸çš„æ¨¡å‹ï¼ˆç•™ç©ºåˆ™ä¸é™åˆ¶ï¼‰ - {role_label}",
                    options=unified_catalog,
                    default=[],
                    key=f"{location}_allowed_models_{role_key}"
                )
                locked = st.selectbox(
                    f"é”å®šæ¨¡å‹ï¼ˆå¯é€‰ï¼‰ - {role_label}",
                    options=["(ä¸é”å®š)"] + unified_catalog,
                    index=0,
                    key=f"{location}_locked_model_{role_key}"
                )
                if allowed:
                    allowed_models_by_role[role_key] = allowed
                if locked and locked != "(ä¸é”å®š)":
                    model_overrides[role_key] = locked

        st.session_state.allowed_models_by_role = allowed_models_by_role
        st.session_state.model_overrides = model_overrides

    return {
        'enable_memory': enable_memory,
        'enable_debug': enable_debug,
        'max_tokens': max_tokens
    }


def render_basic_advanced_settings(location: str = "main") -> dict:
    """ä»…æ¸²æŸ“åŸºç¡€é«˜çº§é€‰é¡¹ï¼ˆè®°å¿†/è°ƒè¯•/æœ€å¤§è¾“å‡ºï¼‰ï¼Œä¸åŒ…å«è§’è‰²é…ç½®ï¼Œç•Œé¢æ›´ç´§å‡‘"""
    with st.expander("âš™ï¸ é«˜çº§è®¾ç½®", expanded=False):
        enable_memory = st.checkbox(
            "å¯ç”¨è®°å¿†åŠŸèƒ½",
            value=False,
            key=f"{location}_adv_only_enable_memory"
        )
        enable_debug = st.checkbox(
            "è°ƒè¯•æ¨¡å¼",
            value=False,
            key=f"{location}_adv_only_enable_debug"
        )
        max_tokens = st.slider(
            "æœ€å¤§è¾“å‡ºé•¿åº¦",
            min_value=1000,
            max_value=128000,
            value=32000,
            step=1000,
            key=f"{location}_adv_only_max_tokens"
        )
    return {
        'enable_memory': enable_memory,
        'enable_debug': enable_debug,
        'max_tokens': max_tokens,
    }


def render_role_overrides_compact(location: str = "main") -> dict:
    """
    ç´§å‡‘ç‰ˆè§’è‰²è¦†ç›–é…ç½®ï¼šå·¦ä¾§é€‰æ‹©è§’è‰²ï¼Œå³ä¾§é…ç½®å…è®¸/é”å®šæ¨¡å‹ï¼Œé¿å…çºµå‘æ— é™æ‰©å¼ ã€‚
    è¿”å› {'allowed_models_by_role': {...}, 'model_overrides': {...}}
    """
    # è§’è‰²æ¸…å•ä¸æ ‡ç­¾
    roles = [
        ("fundamental_expert", "åŸºæœ¬é¢ä¸“å®¶"),
        ("chief_decision_officer", "é¦–å¸­å†³ç­–å®˜"),
        ("technical_analyst", "æŠ€æœ¯åˆ†æå¸ˆ"),
        ("news_hunter", "å¿«è®¯çŒæ‰‹"),
        ("sentiment_analyst", "æƒ…ç»ªåˆ†æå¸ˆ"),
        ("tool_engineer", "å·¥å…·å·¥ç¨‹å¸ˆ"),
    ]
    role_labels = [label for _, label in roles]
    key_by_label = {label: key for key, label in roles}

    # æ¨¡å‹ç›®å½•ï¼ˆç»Ÿä¸€æ¥æºæ±‡æ€»ï¼‰
    siliconflow_catalog = get_siliconflow_models()
    deepseek_catalog = get_deepseek_models()
    google_catalog = get_google_models()
    unified_catalog = siliconflow_catalog + google_catalog + deepseek_catalog

    # ä»ä¼šè¯æ¢å¤
    allowed_models_by_role = dict(st.session_state.get('allowed_models_by_role', {}))
    model_overrides = dict(st.session_state.get('model_overrides', {}))

    # ä¸¤åˆ—å¸ƒå±€ï¼šå·¦é€‰æ‹©è§’è‰²ï¼Œå³é…ç½®è¯¦æƒ…
    col_left, col_right = st.columns([1, 2])
    with col_left:
        st.markdown("#### é€‰æ‹©è§’è‰²")
        selected_label = st.selectbox(
            "éœ€è¦é…ç½®çš„è§’è‰²",
            options=role_labels,
            key=f"{location}_role_select"
        )
        # å·²é…ç½®è®¡æ•°
        configured = sum(1 for k in allowed_models_by_role.keys() if k in [r[0] for r in roles]) + \
                     sum(1 for k in model_overrides.keys() if k in [r[0] for r in roles])
        st.caption(f"å·²é…ç½®è§’è‰²æ¡ç›®: {configured}")

    with col_right:
        st.markdown("#### è§’è‰²è¯¦æƒ…")
        role_key = key_by_label[selected_label]
        # å½“å‰å€¼
        current_allowed = allowed_models_by_role.get(role_key, [])
        current_locked = model_overrides.get(role_key, "(ä¸é”å®š)")

        allowed = st.multiselect(
            "å…è®¸çš„æ¨¡å‹ï¼ˆç•™ç©ºåˆ™ä¸é™åˆ¶ï¼‰",
            options=unified_catalog,
            default=current_allowed,
            key=f"{location}_allowed_models_compact_{role_key}"
        )
        locked = st.selectbox(
            "é”å®šæ¨¡å‹ï¼ˆå¯é€‰ï¼Œä¼˜å…ˆäºå…è®¸åˆ—è¡¨ï¼‰",
            options=["(ä¸é”å®š)"] + unified_catalog,
            index=(["(ä¸é”å®š)"] + unified_catalog).index(current_locked)
                   if current_locked in ["(ä¸é”å®š)"] + unified_catalog else 0,
            key=f"{location}_locked_model_compact_{role_key}"
        )

        # å†™å›ä¼šè¯
        if allowed:
            allowed_models_by_role[role_key] = allowed
        elif role_key in allowed_models_by_role:
            # æ¸…ç©ºåˆ™ç§»é™¤é”®ï¼Œé¿å…å™ªéŸ³
            allowed_models_by_role.pop(role_key, None)

        if locked and locked != "(ä¸é”å®š)":
            model_overrides[role_key] = locked
        else:
            model_overrides.pop(role_key, None)

    # æŒä¹…åˆ° session_stateï¼Œä¾›åç»­åˆ†æä½¿ç”¨
    st.session_state.allowed_models_by_role = allowed_models_by_role
    st.session_state.model_overrides = model_overrides

    return {
        'allowed_models_by_role': allowed_models_by_role,
        'model_overrides': model_overrides,
    }


def render_model_selection_panel(location: str = "main", *, show_routing: bool = True, show_advanced: bool = True) -> dict:
    """
    æ¸²æŸ“provider/model/routing/costæ§ä»¶å¹¶è¿”å›é€‰æ‹©ç»“æœçš„å­—å…¸
    
    Args:
        location: "sidebar" æˆ– "main" (ç”¨äºé—´è·/æ ·å¼å·®å¼‚)
        
    Returns:
        dict: åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸:
            - llm_provider: LLMæä¾›å•†
            - model_category: æ¨¡å‹ç±»åˆ«
            - llm_quick_model: å¿«é€Ÿæ¨¡å‹
            - llm_deep_model: æ·±åº¦æ¨¡å‹  
            - llm_model: å…¼å®¹å­—æ®µ(ç­‰äºllm_deep_model)
            - routing_strategy: è·¯ç”±ç­–ç•¥
            - fallbacks: å›é€€å€™é€‰åˆ—è¡¨
            - max_budget: æˆæœ¬ä¸Šé™
    """
    
    # ä»æŒä¹…åŒ–å­˜å‚¨åŠ è½½é…ç½®
    saved_config = load_model_selection()
    
    # åˆå§‹åŒ–session stateï¼Œä¼˜å…ˆä½¿ç”¨ä¿å­˜çš„é…ç½®
    if 'llm_provider' not in st.session_state:
        st.session_state.llm_provider = saved_config['provider']
        logger.debug(f"ğŸ”§ [Persistence] æ¢å¤ llm_provider: {st.session_state.llm_provider}")
    if 'model_category' not in st.session_state:
        st.session_state.model_category = saved_config['category']
        logger.debug(f"ğŸ”§ [Persistence] æ¢å¤ model_category: {st.session_state.model_category}")
    if 'llm_model' not in st.session_state:
        st.session_state.llm_model = saved_config['model']
        logger.debug(f"ğŸ”§ [Persistence] æ¢å¤ llm_model: {st.session_state.llm_model}")
    
    # åˆå§‹åŒ–åŒæ¨¡å‹ä½çš„ session_state
    if 'llm_quick_model' not in st.session_state:
        st.session_state.llm_quick_model = st.session_state.get('llm_model', "") or ""
    if 'llm_deep_model' not in st.session_state:
        st.session_state.llm_deep_model = st.session_state.get('llm_model', "") or ""
    
    # æ˜¾ç¤ºå½“å‰session stateçŠ¶æ€ï¼ˆè°ƒè¯•ç”¨ï¼‰
    logger.debug(f"ğŸ” [Session State] å½“å‰çŠ¶æ€ - provider: {st.session_state.llm_provider}, category: {st.session_state.model_category}, model: {st.session_state.llm_model}")

    # åªåœ¨ä¸»å†…å®¹åŒºåŸŸæ·»åŠ åˆ†éš”çº¿
    if location == "main":
        st.markdown("---")

    # AIæ¨¡å‹é…ç½®æ ‡é¢˜
    st.markdown("### ğŸ§  AIæ¨¡å‹é…ç½®")
    
    # LLMæä¾›å•†é€‰æ‹©
    provider_options = ["deepseek", "google", "gemini_api", "openrouter", "siliconflow"]
    llm_provider = st.selectbox(
        "LLMæä¾›å•†",
        options=provider_options,
        index=(provider_options.index(st.session_state.llm_provider)
               if st.session_state.llm_provider in provider_options else 0),
        format_func=lambda x: {
            "deepseek": "ğŸš€ DeepSeek å®˜æ–¹",
            "google": "ğŸŒŸ Google AI (Gemini)", 
            "gemini_api": "ğŸŒŸ Gemini-API (OpenAIå…¼å®¹åä»£)",
            "openrouter": "ğŸ§­ OpenRouter (èšåˆ/å«Gemini)",
            "siliconflow": "ğŸŒ SiliconFlow (èšåˆ)"
        }.get(x, x),
        key=f"{location}_llm_provider_select"
    )
    
    # æ›´æ–°session stateå’ŒæŒä¹…åŒ–å­˜å‚¨
    if st.session_state.llm_provider != llm_provider:
        logger.info(f"ğŸ”„ [Persistence] æä¾›å•†å˜æ›´: {st.session_state.llm_provider} â†’ {llm_provider}")
        st.session_state.llm_provider = llm_provider
        # æä¾›å•†å˜æ›´æ—¶æ¸…ç©ºæ¨¡å‹é€‰æ‹©
        st.session_state.llm_model = ""
        st.session_state.model_category = "google"  # é‡ç½®ä¸ºé»˜è®¤ç±»åˆ«
        logger.info(f"ğŸ”„ [Persistence] æ¸…ç©ºæ¨¡å‹é€‰æ‹©")
        
        # ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
        save_model_selection(llm_provider, st.session_state.model_category, "")
    else:
        st.session_state.llm_provider = llm_provider
    
    # æ¨¡å‹é¢„è®¾é€‰æ‹©ï¼ˆä¸å¤šæ¨¡å‹é¡µé¢çš„â€œåˆ†æå¥—é¤â€è”åŠ¨ï¼Œä»…ç”¨äºåˆå§‹åŒ–é»˜è®¤å€¼ï¼‰
    preset_options = ["ä½æˆæœ¬", "å‡è¡¡", "é«˜è´¨é‡"]
    pkg = st.session_state.get('analysis_package')
    preset_default = 1  # å‡è¡¡
    if pkg == "ä½æˆæœ¬å¥—é¤":
        preset_default = 0
    elif pkg == "é«˜è´¨é‡å¥—é¤":
        preset_default = 2
    # ä»…åœ¨é¦–æ¬¡æ¸²æŸ“ä½¿ç”¨ indexï¼Œåç»­ç”±æ§ä»¶è‡ªèº«çŠ¶æ€ç®¡ç†
    preset = st.selectbox(
        "æ¨¡å‹é¢„è®¾",
        options=preset_options,
        index=preset_default,
        help="ä¸å¤šæ¨¡å‹â€˜åˆ†æå¥—é¤â€™ä¿æŒä¸€è‡´ï¼šä½æˆæœ¬/å‡è¡¡/é«˜è´¨é‡",
        key=f"{location}_model_preset_select"
    )
    
    if llm_provider == "deepseek":
        llm_quick_model, llm_deep_model = _render_deepseek_models(location, preset)
        
    elif llm_provider == "google":
        llm_quick_model, llm_deep_model = _render_google_models(location, preset)
        
    elif llm_provider == "openrouter":
        llm_quick_model, llm_deep_model = _render_openrouter_models(location, preset)

    elif llm_provider == "siliconflow":
        llm_quick_model, llm_deep_model = _render_siliconflow_models(location, preset)
    elif llm_provider == "gemini_api":
        llm_quick_model, llm_deep_model = _render_gemini_api_models(location, preset)
    
    # æ›´æ–°session state
    st.session_state.llm_model = llm_deep_model  # å…¼å®¹æ—§å­—æ®µ
    st.session_state.llm_quick_model = llm_quick_model
    st.session_state.llm_deep_model = llm_deep_model
    
    # ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
    save_model_selection(st.session_state.llm_provider, st.session_state.model_category, llm_deep_model)
    
    routing_strategy = None
    selected_fallbacks = []
    max_budget = 0.0
    if show_routing:
        if location == "main":
            st.markdown("---")
        # è·¯ç”±ç­–ç•¥ä¸å›é€€é…ç½®
        st.markdown("### ğŸ§­ è·¯ç”±ç­–ç•¥ä¸å›é€€")

        routing_strategy = st.selectbox(
            "è·¯ç”±ç­–ç•¥",
            options=["è´¨é‡ä¼˜å…ˆ", "æ—¶å»¶ä¼˜å…ˆ", "æˆæœ¬ä¼˜å…ˆ", "å‡è¡¡"],
            key=f"{location}_routing_strategy_select"
        )

        enable_auto_fallback = st.checkbox(
            "å¤±è´¥è‡ªåŠ¨é™çº§",
            value=True,
            key=f"{location}_enable_auto_fallback"
        )

        # åŠ¨æ€ç”Ÿæˆå›é€€å€™é€‰ï¼ˆProvider:Modelï¼‰
        ds = [f"deepseek:{m}" for m in get_deepseek_models()]
        gg = [f"google:{m}" for m in get_google_models()]
        ga = [f"gemini_api:{m}" for m in get_gemini_api_models()]
        orc = [f"openrouter:{m}" for m in get_openrouter_models()]
        sf = [f"siliconflow:{m}" for m in get_siliconflow_models()]
        fallback_choices_catalog = ds + gg + ga + orc + sf

        selected_fallbacks = st.multiselect(
            "å›é€€å€™é€‰ï¼ˆä»ä¸Šåˆ°ä¸‹ä¼˜å…ˆï¼‰",
            options=fallback_choices_catalog,
            default=([ds[0], (gg[1] if len(gg) > 1 else gg[0])] if (enable_auto_fallback and ds and gg) else []),
            key=f"{location}_selected_fallbacks"
        )

        # é¢„ç®—ä¸Šé™
        max_budget = st.number_input(
            "æ¯æ¬¡åˆ†ææˆæœ¬ä¸Šé™(Â¥)",
            min_value=0.0,
            value=0.0,
            step=0.1,
            key=f"{location}_max_budget"
        )
    
    # è§£æå›é€€å€™é€‰ä¸ºç»“æ„åŒ–
    def parse_fb(item: str):
        try:
            p, m = item.split(":", 1)
            return {"provider": p, "model": m}
        except Exception:
            return None
    
    fallbacks_structured = [x for x in (parse_fb(i) for i in selected_fallbacks) if x]
    
    # é«˜çº§è®¾ç½®
    enable_memory = False
    enable_debug = False
    max_tokens = 32000
    if show_advanced:
        with st.expander("âš™ï¸ é«˜çº§è®¾ç½®"):
            enable_memory = st.checkbox(
                "å¯ç”¨è®°å¿†åŠŸèƒ½",
                value=False,
                key=f"{location}_enable_memory"
            )

            enable_debug = st.checkbox(
                "è°ƒè¯•æ¨¡å¼",
                value=False,
                key=f"{location}_enable_debug"
            )

            max_tokens = st.slider(
                "æœ€å¤§è¾“å‡ºé•¿åº¦",
                min_value=1000,
                max_value=128000,
                value=32000,
                step=1000,
                key=f"{location}_max_tokens"
            )

            st.markdown("#### ğŸ‘¥ å›¢é˜Ÿç­–ç•¥ï¼ˆæŒ‰è§’è‰²æ§åˆ¶å¯ç”¨æ¨¡å‹ï¼‰")
            # å…è®¸ç”¨æˆ·ä¸ºå…³é”®è§’è‰²æŒ‡å®šå…è®¸/é”å®šæ¨¡å‹ï¼Œä¾›æœ¬æ¬¡ä¼šè¯å³æ—¶è¦†å†™
            roles = [
                ("fundamental_expert", "åŸºæœ¬é¢ä¸“å®¶"),
                ("chief_decision_officer", "é¦–å¸­å†³ç­–å®˜"),
                ("technical_analyst", "æŠ€æœ¯åˆ†æå¸ˆ"),
                ("news_hunter", "å¿«è®¯çŒæ‰‹"),
                ("sentiment_analyst", "æƒ…ç»ªåˆ†æå¸ˆ"),
                ("tool_engineer", "å·¥å…·å·¥ç¨‹å¸ˆ"),
            ]

            # ç»Ÿä¸€æ¥æºï¼šä»é…ç½®/åç«¯è½½å…¥ç¡…åŸºæµåŠ¨æ¨¡å‹ç›®å½•
            siliconflow_catalog = get_siliconflow_models()

            allowed_models_by_role = {}
            model_overrides = {}
            for role_key, role_label in roles:
                with st.expander(f"{role_label} ({role_key})", expanded=False):
                    allowed = st.multiselect(
                        f"å…è®¸çš„æ¨¡å‹ï¼ˆç•™ç©ºåˆ™ä¸é™åˆ¶ï¼‰ - {role_label}",
                        options=siliconflow_catalog + ["gemini-2.5-pro", "gemini-2.5-flash"],
                        default=[],
                        key=f"{location}_allowed_models_{role_key}"
                    )
                    locked = st.selectbox(
                        f"é”å®šæ¨¡å‹ï¼ˆå¯é€‰ï¼‰ - {role_label}",
                        options=["(ä¸é”å®š)"] + siliconflow_catalog + ["gemini-2.5-pro", "gemini-2.5-flash"],
                        index=0,
                        key=f"{location}_locked_model_{role_key}"
                    )
                    if allowed:
                        allowed_models_by_role[role_key] = allowed
                    if locked and locked != "(ä¸é”å®š)":
                        model_overrides[role_key] = locked

            # å°†å³æ—¶ç­–ç•¥å†™å…¥ session_stateï¼Œä¾› web/utils/analysis_runner æˆ– web/app.py ä¼ å…¥ context
            st.session_state.allowed_models_by_role = allowed_models_by_role
            st.session_state.model_overrides = model_overrides
    
    # ä¿å­˜åˆ°session stateä»¥ä¾›è·¯ç”±ç­–ç•¥ç­‰ä½¿ç”¨
    if show_routing:
        st.session_state.routing_strategy_select = routing_strategy
        st.session_state.fallback_chain = fallbacks_structured
        st.session_state.max_budget = max_budget
    
    # è¿”å›é…ç½®å­—å…¸
    return {
        'llm_provider': st.session_state.llm_provider,
        'model_category': st.session_state.get('model_category'),
        'llm_quick_model': st.session_state.get('llm_quick_model'),
        'llm_deep_model': st.session_state.get('llm_deep_model'),
        'llm_model': st.session_state.get('llm_deep_model') or st.session_state.get('llm_model'),
        'routing_strategy': routing_strategy,
        'fallbacks': fallbacks_structured,
        'max_budget': max_budget,
        'enable_memory': enable_memory,
        'enable_debug': enable_debug,
        'max_tokens': max_tokens
    }


def _render_deepseek_models(location: str, preset: str) -> tuple:
    """æ¸²æŸ“DeepSeekæ¨¡å‹é€‰æ‹©ï¼ˆç»Ÿä¸€æ¥æºï¼‰"""
    deepseek_options = get_deepseek_models() + ["ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹"]
    
    # é¢„è®¾é»˜è®¤ç»„åˆ
    if preset == "ä½æˆæœ¬":
        default_quick, default_deep = "deepseek-chat", "deepseek-chat"
    elif preset == "é«˜è´¨é‡":
        default_quick, default_deep = "deepseek-chat", "deepseek-reasoner"
    else:  # å‡è¡¡
        default_quick, default_deep = "deepseek-chat", "deepseek-chat"
    
    # å¿«é€Ÿæ¨¡å‹
    model_choice_quick = st.selectbox(
        "å¿«é€Ÿæ¨¡å‹ (Quick)",
        options=deepseek_options,
        index=(deepseek_options.index(st.session_state.llm_quick_model)
               if st.session_state.llm_quick_model in deepseek_options else
               deepseek_options.index(default_quick)),
        format_func=lambda x: {
            "deepseek-chat": "DeepSeek Chat (V3) - å¿«é€Ÿ/é«˜æ€§ä»·æ¯”",
            "deepseek-reasoner": "DeepSeek Reasoner (R1) - æ¨ç†æ›´å¼º(æˆæœ¬é«˜)",
            "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹": "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹ - è¾“å…¥ä»»æ„æ¨¡å‹åç§°"
        }.get(x, x),
        key=f"{location}_deepseek_quick_model_select"
    )
    
    # æ·±åº¦æ¨¡å‹
    model_choice_deep = st.selectbox(
        "æ·±åº¦æ¨¡å‹ (Deep)",
        options=deepseek_options,
        index=(deepseek_options.index(st.session_state.llm_deep_model)
               if st.session_state.llm_deep_model in deepseek_options else
               deepseek_options.index(default_deep)),
        format_func=lambda x: {
            "deepseek-chat": "DeepSeek Chat (V3) - é€šç”¨/æ€§ä»·æ¯”",
            "deepseek-reasoner": "DeepSeek Reasoner (R1) - æ·±åº¦æ¨ç†ä¼˜é€‰",
            "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹": "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹ - è¾“å…¥ä»»æ„æ¨¡å‹åç§°"
        }.get(x, x),
        key=f"{location}_deepseek_deep_model_select"
    )
    
    # å¤„ç†è‡ªå®šä¹‰è¾“å…¥ï¼ˆå¿«é€Ÿï¼‰
    if model_choice_quick == "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹":
        custom_model = st.text_input(
            "ğŸ”§ è‡ªå®šä¹‰DeepSeekå¿«é€Ÿæ¨¡å‹",
            value=st.session_state.llm_quick_model if st.session_state.llm_quick_model not in deepseek_options else "",
            placeholder="ä¾‹å¦‚: deepseek-chat, deepseek-reasonerç­‰",
            key=f"{location}_deepseek_custom_input_quick"
        )
        
        if custom_model:
            is_valid, message = validate_custom_model_name(custom_model, "deepseek")
            if is_valid:
                if message.startswith("âœ…"):
                    st.success(message)
                else:
                    st.info(message)
                llm_quick_model = custom_model
            else:
                st.error(message)
                llm_quick_model = "deepseek-chat"
        else:
            st.warning("âš ï¸ è¯·è¾“å…¥æ¨¡å‹åç§°ï¼Œæˆ–é€‰æ‹©é¢„è®¾æ¨¡å‹")
            llm_quick_model = "deepseek-chat"
        
        render_model_help("deepseek")
    else:
        llm_quick_model = model_choice_quick
    
    # å¤„ç†è‡ªå®šä¹‰è¾“å…¥ï¼ˆæ·±åº¦ï¼‰
    if model_choice_deep == "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹":
        custom_model_deep = st.text_input(
            "ğŸ”§ è‡ªå®šä¹‰DeepSeekæ·±åº¦æ¨¡å‹",
            value=st.session_state.llm_deep_model if st.session_state.llm_deep_model not in deepseek_options else "",
            placeholder="ä¾‹å¦‚: deepseek-reasoner",
            key=f"{location}_deepseek_custom_input_deep"
        )
        if custom_model_deep:
            is_valid, message = validate_custom_model_name(custom_model_deep, "deepseek")
            if is_valid:
                st.success(message) if message.startswith("âœ…") else st.info(message)
                llm_deep_model = custom_model_deep
            else:
                st.error(message)
                llm_deep_model = default_deep
        else:
            st.warning("âš ï¸ è¯·è¾“å…¥æ·±åº¦æ¨¡å‹åç§°ï¼Œæˆ–é€‰æ‹©é¢„è®¾æ¨¡å‹")
            llm_deep_model = default_deep
    else:
        llm_deep_model = model_choice_deep
    
    logger.debug(f"ğŸ’¾ [Persistence] DeepSeekæ¨¡å‹å·²ä¿å­˜: quick={llm_quick_model}, deep={llm_deep_model}")
    
    return llm_quick_model, llm_deep_model


def _render_openrouter_models(location: str, preset: str) -> tuple:
    """æ¸²æŸ“OpenRouteræ¨¡å‹é€‰æ‹©ï¼ˆå«Gemini/Claude/OpenAI/Llamaç­‰ï¼‰"""
    catalog = get_openrouter_models()
    options = catalog + ["ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹"]

    # é¢„è®¾ï¼šä¼˜å…ˆGeminiç³»åˆ—
    if preset == "ä½æˆæœ¬":
        default_quick, default_deep = "google/gemini-2.0-flash", "google/gemini-2.0-flash"
    elif preset == "é«˜è´¨é‡":
        default_quick, default_deep = "google/gemini-2.0-flash", "google/gemini-2.5-pro"
    else:
        default_quick, default_deep = "google/gemini-2.0-flash", "google/gemini-2.5-pro"

    def _fmt(name: str) -> str:
        mapping = {
            "google/gemini-2.5-pro": "Gemini 2.5 Pro (OpenRouter)",
            "google/gemini-2.0-flash": "Gemini 2.0 Flash (OpenRouter)",
            "google/gemini-1.5-pro": "Gemini 1.5 Pro (OpenRouter)",
            "anthropic/claude-3.5-sonnet": "Claude 3.5 Sonnet (OpenRouter)",
            "anthropic/claude-3.5-haiku": "Claude 3.5 Haiku (OpenRouter)",
            "openai/o4-mini-high": "OpenAI o4-mini-high (OpenRouter)",
            "openai/o3-pro": "OpenAI o3-pro (OpenRouter)",
            "meta-llama/llama-3.2-90b-instruct": "Llama 3.2 90B Instruct (OpenRouter)",
            "mistralai/mistral-large-latest": "Mistral Large (OpenRouter)",
            "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹": "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹",
        }
        return mapping.get(name, name)

    model_choice_quick = st.selectbox(
        "å¿«é€Ÿæ¨¡å‹ (Quick)",
        options=options,
        index=(options.index(st.session_state.llm_quick_model)
               if st.session_state.llm_quick_model in options else
               (options.index(default_quick) if default_quick in options else 0)),
        format_func=_fmt,
        key=f"{location}_openrouter_quick_model_select"
    )

    model_choice_deep = st.selectbox(
        "æ·±åº¦æ¨¡å‹ (Deep)",
        options=options,
        index=(options.index(st.session_state.llm_deep_model)
               if st.session_state.llm_deep_model in options else
               (options.index(default_deep) if default_deep in options else 0)),
        format_func=_fmt,
        key=f"{location}_openrouter_deep_model_select"
    )

    # è‡ªå®šä¹‰æ¨¡å‹ï¼ˆprovider/modelï¼‰
    if model_choice_quick == "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹":
        custom_model = st.text_input(
            "ğŸ”§ è‡ªå®šä¹‰OpenRouterå¿«é€Ÿæ¨¡å‹",
            value=st.session_state.llm_quick_model if st.session_state.llm_quick_model not in options else "",
            placeholder="ä¾‹å¦‚: google/gemini-2.0-flash æˆ– anthropic/claude-3.5-sonnet",
            key=f"{location}_openrouter_custom_input_quick"
        )
        if custom_model:
            is_valid, message = validate_custom_model_name(custom_model, "openrouter")
            if is_valid:
                st.success(message) if message.startswith("âœ…") else st.info(message)
                llm_quick_model = custom_model
            else:
                st.error(message)
                llm_quick_model = default_quick
        else:
            st.warning("âš ï¸ è¯·è¾“å…¥æ¨¡å‹åç§°ï¼Œæˆ–é€‰æ‹©é¢„è®¾æ¨¡å‹")
            llm_quick_model = default_quick
        render_model_help("openrouter")
    else:
        llm_quick_model = model_choice_quick

    if model_choice_deep == "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹":
        custom_model_deep = st.text_input(
            "ğŸ”§ è‡ªå®šä¹‰OpenRouteræ·±åº¦æ¨¡å‹",
            value=st.session_state.llm_deep_model if st.session_state.llm_deep_model not in options else "",
            placeholder="ä¾‹å¦‚: google/gemini-2.5-pro",
            key=f"{location}_openrouter_custom_input_deep"
        )
        if custom_model_deep:
            is_valid, message = validate_custom_model_name(custom_model_deep, "openrouter")
            if is_valid:
                st.success(message) if message.startswith("âœ…") else st.info(message)
                llm_deep_model = custom_model_deep
            else:
                st.error(message)
                llm_deep_model = default_deep
        else:
            st.warning("âš ï¸ è¯·è¾“å…¥æ·±åº¦æ¨¡å‹åç§°ï¼Œæˆ–é€‰æ‹©é¢„è®¾æ¨¡å‹")
            llm_deep_model = default_deep
    else:
        llm_deep_model = model_choice_deep

    logger.debug(f"ğŸ’¾ [Persistence] OpenRouteræ¨¡å‹å·²ä¿å­˜: quick={llm_quick_model}, deep={llm_deep_model}")
    return llm_quick_model, llm_deep_model


def _render_gemini_api_models(location: str, preset: str) -> tuple:
    """æ¸²æŸ“ Gemini-API(å…¼å®¹) æ¸ é“æ¨¡å‹é€‰æ‹©ã€‚"""
    from web.utils.model_catalog import get_gemini_api_models

    catalog = get_gemini_api_models()
    options = catalog + ["ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹"]

    # é¢„è®¾ï¼šä¸ Google å®¶æ—ä¿æŒä¸€è‡´çš„é»˜è®¤
    if preset == "ä½æˆæœ¬":
        default_quick, default_deep = "gemini-2.0-flash", "gemini-2.0-flash"
    elif preset == "é«˜è´¨é‡":
        default_quick, default_deep = "gemini-2.0-flash", "gemini-2.5-pro"
    else:
        default_quick, default_deep = "gemini-2.0-flash", "gemini-2.5-pro"

    def _fmt(name: str) -> str:
        mapping = {
            "gemini-2.5-pro": "Gemini 2.5 Pro (Gemini-API)",
            "gemini-2.0-flash": "Gemini 2.0 Flash (Gemini-API)",
            "gemini-1.5-pro": "Gemini 1.5 Pro (Gemini-API)",
            "gemini-1.5-flash": "Gemini 1.5 Flash (Gemini-API)",
            "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹": "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹",
        }
        return mapping.get(name, name)

    model_choice_quick = st.selectbox(
        "å¿«é€Ÿæ¨¡å‹ (Quick)",
        options=options,
        index=(options.index(st.session_state.llm_quick_model)
               if st.session_state.llm_quick_model in options else
               (options.index(default_quick) if default_quick in options else 0)),
        format_func=_fmt,
        key=f"{location}_gemini_api_quick_model_select"
    )

    model_choice_deep = st.selectbox(
        "æ·±åº¦æ¨¡å‹ (Deep)",
        options=options,
        index=(options.index(st.session_state.llm_deep_model)
               if st.session_state.llm_deep_model in options else
               (options.index(default_deep) if default_deep in options else 0)),
        format_func=_fmt,
        key=f"{location}_gemini_api_deep_model_select"
    )

    # è‡ªå®šä¹‰æ¨¡å‹ï¼ˆä¸ Google å‘½åä¿æŒä¸€è‡´ï¼Œä¾¿äºè¿ç§»ï¼‰
    if model_choice_quick == "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹":
        custom_model = st.text_input(
            "ğŸ”§ è‡ªå®šä¹‰Gemini-APIå¿«é€Ÿæ¨¡å‹",
            value=st.session_state.llm_quick_model if st.session_state.llm_quick_model not in options else "",
            placeholder="ä¾‹å¦‚: gemini-2.0-flash, gemini-2.5-pro",
            key=f"{location}_gemini_api_custom_input_quick"
        )
        llm_quick_model = custom_model or default_quick
    else:
        llm_quick_model = model_choice_quick

    if model_choice_deep == "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹":
        custom_model_deep = st.text_input(
            "ğŸ”§ è‡ªå®šä¹‰Gemini-APIæ·±åº¦æ¨¡å‹",
            value=st.session_state.llm_deep_model if st.session_state.llm_deep_model not in options else "",
            placeholder="ä¾‹å¦‚: gemini-2.5-pro",
            key=f"{location}_gemini_api_custom_input_deep"
        )
        llm_deep_model = custom_model_deep or default_deep
    else:
        llm_deep_model = model_choice_deep

    return llm_quick_model, llm_deep_model

def _render_google_models(location: str, preset: str) -> tuple:
    """æ¸²æŸ“Google AIæ¨¡å‹é€‰æ‹©ï¼ˆç»Ÿä¸€æ¥æºï¼‰"""
    google_options = get_google_models() + ["ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹"]
    
    if preset == "ä½æˆæœ¬":
        default_quick, default_deep = "gemini-2.5-flash", "gemini-2.5-flash"
    elif preset == "é«˜è´¨é‡":
        default_quick, default_deep = "gemini-2.5-flash", "gemini-2.5-pro"
    else:
        default_quick, default_deep = "gemini-2.0-flash", "gemini-2.5-pro"
    
    model_choice_quick = st.selectbox(
        "å¿«é€Ÿæ¨¡å‹ (Quick)",
        options=google_options,
        index=(google_options.index(st.session_state.llm_quick_model)
               if st.session_state.llm_quick_model in google_options else
               google_options.index(default_quick)),
        format_func=lambda x: {
            "gemini-2.5-pro": "Gemini 2.5 Pro - æ·±åº¦æ¨ç†",
            "gemini-2.5-flash": "Gemini 2.5 Flash - å¿«é€Ÿ",
            "gemini-2.0-flash": "Gemini 2.0 Flash - å‡è¡¡",
            "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹": "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹"
        }.get(x, x),
        key=f"{location}_google_quick_model_select"
    )
    
    model_choice_deep = st.selectbox(
        "æ·±åº¦æ¨¡å‹ (Deep)",
        options=google_options,
        index=(google_options.index(st.session_state.llm_deep_model)
               if st.session_state.llm_deep_model in google_options else
               google_options.index(default_deep)),
        format_func=lambda x: {
            "gemini-2.5-pro": "Gemini 2.5 Pro - æ·±åº¦æ¨ç†ä¼˜é€‰",
            "gemini-2.5-flash": "Gemini 2.5 Flash - å¿«é€Ÿ",
            "gemini-2.0-flash": "Gemini 2.0 Flash - å‡è¡¡",
            "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹": "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹"
        }.get(x, x),
        key=f"{location}_google_deep_model_select"
    )
    
    # å¤„ç†è‡ªå®šä¹‰è¾“å…¥
    if model_choice_quick == "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹":
        custom_model = st.text_input(
            "ğŸ”§ è‡ªå®šä¹‰Googleå¿«é€Ÿæ¨¡å‹",
            value=st.session_state.llm_quick_model if st.session_state.llm_quick_model not in google_options else "",
            placeholder="ä¾‹å¦‚: gemini-2.5-flash, gemini-2.5-proç­‰",
            key=f"{location}_google_custom_input_quick"
        )
        
        if custom_model:
            is_valid, message = validate_custom_model_name(custom_model, "google")
            if is_valid:
                if message.startswith("âœ…"):
                    st.success(message)
                else:
                    st.info(message)
                llm_quick_model = custom_model
            else:
                st.error(message)
                llm_quick_model = default_quick
        else:
            st.warning("âš ï¸ è¯·è¾“å…¥æ¨¡å‹åç§°ï¼Œæˆ–é€‰æ‹©é¢„è®¾æ¨¡å‹")
            llm_quick_model = default_quick
        
        render_model_help("google")
    else:
        llm_quick_model = model_choice_quick
    
    if model_choice_deep == "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹":
        custom_model_deep = st.text_input(
            "ğŸ”§ è‡ªå®šä¹‰Googleæ·±åº¦æ¨¡å‹",
            value=st.session_state.llm_deep_model if st.session_state.llm_deep_model not in google_options else "",
            placeholder="ä¾‹å¦‚: gemini-2.5-pro",
            key=f"{location}_google_custom_input_deep"
        )
        if custom_model_deep:
            is_valid, message = validate_custom_model_name(custom_model_deep, "google")
            if is_valid:
                st.success(message) if message.startswith("âœ…") else st.info(message)
                llm_deep_model = custom_model_deep
            else:
                st.error(message)
                llm_deep_model = default_deep
        else:
            st.warning("âš ï¸ è¯·è¾“å…¥æ·±åº¦æ¨¡å‹åç§°ï¼Œæˆ–é€‰æ‹©é¢„è®¾æ¨¡å‹")
            llm_deep_model = default_deep
    else:
        llm_deep_model = model_choice_deep
    
    logger.debug(f"ğŸ’¾ [Persistence] Googleæ¨¡å‹å·²ä¿å­˜: quick={llm_quick_model}, deep={llm_deep_model}")
    
    return llm_quick_model, llm_deep_model


def _render_siliconflow_models(location: str, preset: str) -> tuple:
    """æ¸²æŸ“SiliconFlowæ¨¡å‹é€‰æ‹©ï¼ˆç»Ÿä¸€æ¥æºï¼‰"""
    catalog = get_siliconflow_models()
    # è¡¥å……â€˜è‡ªå®šä¹‰æ¨¡å‹â€™å…¥å£
    siliconflow_options = catalog + ["ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹"]
    
    if preset == "ä½æˆæœ¬":
        default_quick, default_deep = "deepseek-ai/DeepSeek-V3", "deepseek-ai/DeepSeek-V3"
    elif preset == "é«˜è´¨é‡":
        default_quick, default_deep = "deepseek-ai/DeepSeek-V3", "deepseek-ai/DeepSeek-R1"
    else:
        default_quick, default_deep = "deepseek-ai/DeepSeek-V3", "deepseek-ai/DeepSeek-R1"
    
    def _fmt_model(name: str) -> str:
        mapping = {
            "deepseek-ai/DeepSeek-R1": "DeepSeek R1 - æ¨ç†å¼º",
            "deepseek-ai/DeepSeek-V3": "DeepSeek V3 - é€šç”¨/æ¨è",
            "zai-org/GLM-4.5": "GLM-4.5 - ä¸­æ–‡ä¼˜ç§€",
            "moonshotai/Kimi-K2-Instruct": "Kimi K2 - é•¿æ–‡æœ¬",
            "Qwen/Qwen3-Coder-480B-A35B-Instruct": "Qwen3 Coder 480B - ä»£ç /å·¥å…·",
            "Qwen/Qwen3-235B-A22B-Instruct-2507": "Qwen3 235B Instruct - é€šç”¨",
            "Qwen/Qwen3-235B-A22B-Thinking-2507": "Qwen3 235B Thinking - æ¨ç†",
            "stepfun-ai/step3": "Step-3 - å¤šæ¨¡æ€æ¨ç†",
            "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹": "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹",
        }
        return mapping.get(name, name)

    model_choice_quick = st.selectbox(
        "å¿«é€Ÿæ¨¡å‹ (Quick)",
        options=siliconflow_options,
        index=(siliconflow_options.index(st.session_state.llm_quick_model)
               if st.session_state.llm_quick_model in siliconflow_options else
               (siliconflow_options.index(default_quick) if default_quick in siliconflow_options else 0)),
        format_func=_fmt_model,
        key=f"{location}_siliconflow_quick_model_select"
    )
    
    model_choice_deep = st.selectbox(
        "æ·±åº¦æ¨¡å‹ (Deep)",
        options=siliconflow_options,
        index=(siliconflow_options.index(st.session_state.llm_deep_model)
               if st.session_state.llm_deep_model in siliconflow_options else
               (siliconflow_options.index(default_deep) if default_deep in siliconflow_options else 0)),
        format_func=_fmt_model,
        key=f"{location}_siliconflow_deep_model_select"
    )
    
    # å¤„ç†è‡ªå®šä¹‰è¾“å…¥
    if model_choice_quick == "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹":
        custom_model = st.text_input(
            "ğŸ”§ è‡ªå®šä¹‰SiliconFlowå¿«é€Ÿæ¨¡å‹",
            value=st.session_state.llm_quick_model if st.session_state.llm_quick_model not in siliconflow_options else "",
            placeholder="ä¾‹å¦‚: deepseek-ai/DeepSeek-R1, gemini-1.5-pro ç­‰",
            key=f"{location}_siliconflow_custom_input_quick"
        )
        
        if custom_model:
            is_valid, message = validate_custom_model_name(custom_model, "siliconflow")
            if is_valid:
                if message.startswith("âœ…"):
                    st.success(message)
                else:
                    st.info(message)
                llm_quick_model = custom_model
            else:
                st.error(message)
                llm_quick_model = default_quick
        else:
            st.warning("âš ï¸ è¯·è¾“å…¥æ¨¡å‹åç§°ï¼Œæˆ–é€‰æ‹©é¢„è®¾æ¨¡å‹")
            llm_quick_model = default_quick
        
        render_model_help("siliconflow")
    else:
        llm_quick_model = model_choice_quick
    
    if model_choice_deep == "ğŸ’¡ è‡ªå®šä¹‰æ¨¡å‹":
        custom_model_deep = st.text_input(
            "ğŸ”§ è‡ªå®šä¹‰SiliconFlowæ·±åº¦æ¨¡å‹",
            value=st.session_state.llm_deep_model if st.session_state.llm_deep_model not in siliconflow_options else "",
            placeholder="ä¾‹å¦‚: deepseek-ai/DeepSeek-R1",
            key=f"{location}_siliconflow_custom_input_deep"
        )
        if custom_model_deep:
            is_valid, message = validate_custom_model_name(custom_model_deep, "siliconflow")
            if is_valid:
                st.success(message) if message.startswith("âœ…") else st.info(message)
                llm_deep_model = custom_model_deep
            else:
                st.error(message)
                llm_deep_model = default_deep
        else:
            st.warning("âš ï¸ è¯·è¾“å…¥æ·±åº¦æ¨¡å‹åç§°ï¼Œæˆ–é€‰æ‹©é¢„è®¾æ¨¡å‹")
            llm_deep_model = default_deep
    else:
        llm_deep_model = model_choice_deep
    
    logger.debug(f"ğŸ’¾ [Persistence] SiliconFlowæ¨¡å‹å·²ä¿å­˜: quick={llm_quick_model}, deep={llm_deep_model}")
    
    return llm_quick_model, llm_deep_model
